"""Chat with local and remote LLMs in the context of the wordslab-notebooks environment"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02_chat.ipynb.

# %% auto 0
__all__ = ['ChatTurn', 'ChatTurns', 'get_tools_schemas_and_functions', 'ToolExecutionError', 'Tools', 'ModelClient',
           'OllamaModelClient', 'OpenRouterModelClient']

# %% ../nbs/02_chat.ipynb 2
from abc import ABC, abstractmethod
from collections.abc import Sequence as SequenceType
from typing import Callable, Optional, Union, Sequence, Mapping, Any, Literal
from datetime import datetime
import inspect, json, re, time, traceback

from toolslm.funccall import get_schema, call_func
from IPython.display import display, Markdown, clear_output

from ollama import Client
from ollama._types import Options
from openai import OpenAI

from .env import WordslabEnv

# %% ../nbs/02_chat.ipynb 4
class ChatTurn:
    def __init__(self, refresh_display: Callable[None, None] = None):
        self.thinking_chunks = []
        self.content_chunks = []
        self.tool_calls = {}
        self.refresh_display = refresh_display

    def append_thinking(self, chunk:str):
        self.thinking_chunks.append(chunk)
        if self.refresh_display:
            self.refresh_display()

    @property
    def thinking(self):
        return "".join(self.thinking_chunks)
    
    def append_content(self, chunk:str):
        self.content_chunks.append(chunk)
        if self.refresh_display:
            self.refresh_display()

    @property
    def content(self):
        return "".join(self.content_chunks)
    
    def append_tool_call(self, tool_name:str, params:dict):
        self.tool_calls[tool_name] = { "params": params }
        if self.refresh_display:
            self.refresh_display()

    def start_tool_call(self, tool_name:str):
        self.tool_calls[tool_name]["start_time"] = time.time()
        if self.refresh_display:
            self.refresh_display()

    def end_tool_call(self, tool_name:str, result: object):
        self.tool_calls[tool_name]["end_time"] = time.time()
        self.tool_calls[tool_name]["result"] = str(result)
        if self.refresh_display:
            self.refresh_display()
    
    def to_markdown(self, hide_thinking:bool=True, hide_tool_calls:bool=True):
        output = ""
        if len(self.thinking_chunks) > 0:
            if not hide_thinking:
                output += "> [Thinking]\n\n"
                output += "\n".join(f"> {line}" for line in "".join(self.thinking_chunks).splitlines())
            else:
                output += f"> [Thinking] ... thought in {sum(s.count(' ') + s.count('\n') for s in self.thinking_chunks)} words\n\n"
        if len(self.content_chunks) > 0:
            output += "".join(self.content_chunks) + "\n\n"
        if len(self.tool_calls.keys()) > 0:
            if not hide_tool_calls:
                for tool_name in self.tool_calls.keys():
                    output += "> [Tool call]\n"
                    tool_call = self.tool_calls[tool_name]
                    if "params" in tool_call:
                        output += f"> - model wants to call `{tool_name}` with parameters `{tool_call["params"]}`\n"
                    if "start_time" in tool_call:
                        output += f"> - agent called {tool_name} at {datetime.fromtimestamp(tool_call["start_time"]).strftime("%H:%M:%S")}\n"
                    if "end_time" in tool_call:
                        result = tool_call["result"]
                        output += f"> - {tool_name} returned `{result if len(result)<=100 else result[:97]+'...'}` in {(tool_call["end_time"]-tool_call["start_time"]):.3f} sec\n"
                    output += "\n"
            else:
                for tool_name in self.tool_calls.keys():
                    tool_call = self.tool_calls[tool_name]
                    if "end_time" in tool_call:
                        result = self.tool_calls[tool_name]["result"]
                        output += f"> [Tool call] ... `{tool_name}` returned `{result if len(result)<=50 else result[:47]+'...'}`\n\n"
                    elif "start_time" in tool_call:                        
                        output += f"> [Tool call] ... agent is calling `{tool_name}`\n\n"
                    elif "params" in tool_call:                        
                        output += f"> [Tool call] ... model wants to call `{tool_name}`\n\n"
        return output

class ChatTurns:
    def __init__(self, notebook_display:bool=True, hide_thinking:bool=True, hide_tool_calls:bool=True):
        self.chat_turns = []
        self.notebook_display = notebook_display
        self.hide_thinking = hide_thinking
        self.hide_tool_calls = hide_tool_calls

    def new_turn(self):
        new_turn = ChatTurn(self.refresh_notebook_display)
        self.chat_turns.append(new_turn)
        return new_turn
    
    def refresh_notebook_display(self):
        clear_output(wait=True)
        output = ""
        for turn in self.chat_turns:
            output += turn.to_markdown(self.hide_thinking, self.hide_tool_calls)
        display(Markdown(output))

# %% ../nbs/02_chat.ipynb 14
def _get_function_schema(func: Callable, responsesAPIFormat: bool = False):
    "Get a json schema for a python function defined with comments for all parameters"
    if responsesAPIFormat:        
        return {'type': 'function', **get_schema(func, pname='parameters')}
    else:
        return {'type': 'function', 'function': get_schema(func, pname='parameters')}

def get_tools_schemas_and_functions(funcs: Sequence[Callable], responsesAPIFormat: bool = False):
    """Get a dictionary of json schemas and callable functions which can be used for native tool calling."""
    return {func.__name__: (_get_function_schema(func, responsesAPIFormat), func) for func in funcs}

# %% ../nbs/02_chat.ipynb 17
class ToolExecutionError(Exception):
    """Raised when a tool cannot be executed safely."""

class Tools:
    """"Execute tools implemented as python functions with Large Language Models.
    The python functions must be fully documented:
    - type annotations are mandatory on all parameters and on the return type
    - a docstring after the function definition is mandatory
    - a descriptive comment after each parameter and the return type is also mandatory
    - the expected format is: one parameter by line, a traditional python comment at the end of the line
    """   
    def __init__(self, python_functions:Sequence[Callable], responsesAPIFormat:bool=False):
        self.schemas_and_functions = get_tools_schemas_and_functions(python_functions, responsesAPIFormat=responsesAPIFormat)

    def has_tool(self, tool_name:str):
        return tool_name in self.schemas_and_functions
    
    def get_schemas(self):
        return [t[0] for t in self.schemas_and_functions.values()]

    def get_schema(self, tool_name:str):
        return self.schemas_and_functions[tool_name][0]

    def get_functions(self):
        return [t[1] for t in self.schemas_and_functions.values()]

    def get_function(self, tool_name:str):
        return self.schemas_and_functions[tool_name][1]

    def call(self, tool_name:str, tool_arguments_dict:Mapping[str,Any]):
        # 1. Resolve the tool safely
        try:
            self.get_function(tool_name)
        except Exception as e:
            raise ToolExecutionError(f"Tool '{tool_name}' does not exist or could not be resolved.")
            
        # 2. Execute the tool with runtime protection
        try:
            return call_func(tool_name, tool_arguments_dict, self.get_functions(), raise_on_err=True)
        except Exception as e:
            tb = traceback.format_exc()
            raise ToolExecutionError(f"Tool '{tool_name}' raised an exception: {e}")

# %% ../nbs/02_chat.ipynb 26
class ModelClient(ABC):
    def __init__(
        self,
        model: str,
        base_url: Optional[str] = None,
        api_key: Optional[str] = None,
        context_size: Optional[int] = None,
    ):
        self.model = model
        self.base_url = base_url
        self.api_key = api_key
        self.context_size = context_size

    @abstractmethod
    def __call__(
        self,
        messages: Sequence[Mapping[str, Any]],
        chat_turns: ChatTurns,
        tools: Tools = None,
        think: Union[bool, Literal["low", "medium", "high"], None] = None,
        max_new_tokens: Optional[int] = None,
        seed: Optional[int] = None,
        temperature: Optional[float] = None,
        top_k: Optional[int] = None,
        top_p: Optional[float] = None,
        min_p: Optional[float] = None,
    ) -> bool:
        """
        Execute a model call and return the model response.
        """
        raise NotImplementedError

# %% ../nbs/02_chat.ipynb 28
_whitespace_pattern = re.compile(r"\s+")

def _messages_words(messages):
    return sum([len(_whitespace_pattern.findall(message["content"])) for message in messages if message["role"] in {"user", "assitant"}])

class OllamaModelClient(ModelClient):
    def __init__(
        self,
        model: str,
        context_size: int = 32768, # This is the default value for the ollama server in wordslab-notebooks
        base_url: str = "http://localhost:11434",
        api_key: Optional[str] = None,  # If not provided, the optional key will be pulled from WordslabEnv
    ):
        super().__init__(model, base_url, api_key, context_size)

        # Initialize API client
        if not api_key:
            env = WordslabEnv()
            api_key = env.cloud_ollama_api_key
        if api_key:
            headers = {'Authorization': 'Bearer ' + api_key}
        else:            
            headers = {}
        self.client = Client(host=self.base_url, headers=headers)
        
        # Load model in memory with the right context length
        print(f"ollama: loading model {self.model} with context size {self.context_size} ... ", end="");
        self.client.chat(model=self.model, messages=[{'role': 'user', 'content': 'say yes'}], options=Options(num_ctx=self.context_size, num_predict=1))
        print(f"ok");

    def __call__(
        self,
        messages: Sequence[Mapping[str, Any]],
        chat_turns: ChatTurns,
        tools: Tools = None,
        think: Union[bool, Literal["low", "medium", "high"], None] = None,
        max_new_tokens: Optional[int] = None,
        seed: Optional[int] = None,
        temperature: Optional[float] = None,
        top_k: Optional[int] = None,
        top_p: Optional[float] = None,
        min_p: Optional[float] = None,      
    ) -> bool:
        # Check tools parameter type
        if tools and not isinstance(tools, Tools):
            raise TypeError("Argument tools must be of type wordslab_notebooks_lib.chat.Tools. Create a tools object with the syntax: Tools([func1, func2, func3]), where the parameters are documented python functions.")
        
        # Immediate user feedback
        print(f"ollama: processing {_messages_words(messages)} words with `{self.model}` ...")
        
        # Observable conversation turn
        chat_turn = chat_turns.new_turn()
        
        stream = self.client.chat(
            model = self.model,
            messages = messages,
            tools = tools.get_schemas() if tools else None,
            stream = True,
            think = think,
            options = Options(num_ctx = self.context_size, num_predict = max_new_tokens, seed = seed,
                              temperature = temperature, top_k=top_k, top_p=top_p, min_p=min_p)
        )
    
        # Streaming: accumulate the partial fields
        tool_calls = []        
        for chunk in stream:
            if chunk.message.thinking:
                chat_turn.append_thinking(chunk.message.thinking)                
            if chunk.message.content:
                chat_turn.append_content(chunk.message.content)
            if chunk.message.tool_calls:
                tool_calls.extend(chunk.message.tool_calls)
                for tc in chunk.message.tool_calls:
                    chat_turn.append_tool_call(tc.function.name, tc.function.arguments)
        
        # append accumulated fields to the messages
        if chat_turn.thinking or chat_turn.content or tool_calls:
            messages.append({'role': 'assistant', 'thinking': chat_turn.thinking, 'content': chat_turn.content, 'tool_calls': tool_calls})
    
        # end the loop if there is no more tool calls
        if not tool_calls: 
            return False      
            
        # execute tool calls  
        else:    
            for tc in tool_calls:
                if tools.has_tool(tc.function.name):
                    chat_turn.start_tool_call(tc.function.name)
                    result = tools.call(tc.function.name, tc.function.arguments)
                    chat_turn.end_tool_call(tc.function.name, result)
                else:
                    result = 'Unknown tool'
        
                # append tool call result to the messages 
                messages.append({'role': 'tool', 'tool_name': tc.function.name, 'content': str(result)})

        # continue the loop after tool calls
        return True

# %% ../nbs/02_chat.ipynb 43
class OpenRouterModelClient(ModelClient):
    def __init__(
        self,
        model: str,
        context_size: Optional[int] = None, # For OpenRouter this parameter is ignored, we inherit the remote model config
        base_url: str = "https://openrouter.ai/api/v1",
        api_key: Optional[str] = None, # If not provided, the mandatory key will be pulled from WordslabEnv
    ):
        super().__init__(model, base_url, api_key, context_size)

        # Initialize API client
        if not api_key:
            env = WordslabEnv()
            api_key = env.cloud_openrouter_api_key
        self.client = OpenAI(base_url=base_url, api_key=api_key)
        
        # Check connection
        print(f"openrouter: testing model {self.model} ... ", end="");
        self.client.chat.completions.create(model=self.model, messages=[{'role': 'user', 'content': 'say yes'}], max_tokens=1)
        print(f"ok");

    def __call__(
        self,
        messages: Sequence[Mapping[str, Any]],
        chat_turns: ChatTurns,
        tools: Tools = None,
        think: Union[bool, Literal["low", "medium", "high"], None] = None,
        max_new_tokens: Optional[int] = None,
        seed: Optional[int] = None,
        temperature: Optional[float] = None,
        top_k: Optional[int] = None,  # Ignored, not supported by the openai chat completions API
        top_p: Optional[float] = None,
        min_p: Optional[float] = None,  # Ignored, not supported by the openai chat completions API
    ) -> bool:
        # Check tools parameter type
        if tools and not isinstance(tools, Tools):
            raise TypeError("Argument tools must be of type wordslab_notebooks_lib.chat.Tools. Create a tools object with the syntax: Tools([func1, func2, func3]), where the parameters are documented python functions.")
        
        # Immediate user feedback
        print(f"openrouter: processing {_messages_words(messages)} words with `{self.model}` ...")
        
        # Observable conversation turn
        chat_turn = chat_turns.new_turn()
        
        # Map "think" â†’ reasoning_effort
        reasoning = None
        if think is True:
            reasoning = {"reasoning": {"enabled": True}} 
        elif think in ("low", "medium", "high"):
            reasoning = {"reasoning": {"effort": think}} # Can be "xhigh", "high", "medium", "low", "minimal" or "none" (OpenAI-style)

        stream = self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            tools = tools.get_schemas() if tools else None,
            stream=True,
            extra_body= reasoning,
            max_tokens=max_new_tokens,
            seed=seed,
            temperature=temperature,
            top_p=top_p,
        )
    
        # Streaming: accumulate the partial fields
        tool_calls = {}       
        for chunk in stream:
            delta = chunk.choices[0].delta
            if hasattr(delta, "reasoning") and delta.reasoning:
                chat_turn.append_thinking(delta.reasoning)                
            if hasattr(delta, "content") and delta.content:
                chat_turn.append_content(delta.content)
            if hasattr(delta, "tool_calls") and delta.tool_calls:
                for tool_call in delta.tool_calls:
                    call_id = tool_call.id        
                    if call_id not in tool_calls:
                        tool_calls[call_id] = {
                            "name": tool_call.function.name,
                            "arguments": ""
                        }
                    # arguments arrive as a JSON string fragment
                    tool_calls[call_id]["arguments"] += (tool_call.function.arguments or "")
        # We need to wait the end of the stream to make sure the tool calls are complete
        for tc in tool_calls.values():
            chat_turn.append_tool_call(tc["name"], tc["arguments"])
        
        # append accumulated fields to the messages
        if chat_turn.thinking or chat_turn.content or tool_calls:
            messages.append({
                "role": "assistant",
                "content": chat_turn.content,
                "tool_calls": [
                    {
                        "id": call_id,
                        "type": "function",
                        "function": {
                            "name": call["name"],
                            "arguments": call["arguments"]
                        }
                    }
                    for call_id, call in tool_calls.items()
                ]
            })
    
        # end the loop if there is no more tool calls
        if not tool_calls: 
            return False      
            
        # execute tool calls  
        else:    
            for tc_id, tc in tool_calls.items():
                if tools.has_tool(tc["name"]):
                    chat_turn.start_tool_call(tc["name"])
                    result = tools.call(tc["name"], json.loads(tc["arguments"]))
                    chat_turn.end_tool_call(tc["name"], result)
                else:
                    result = 'Unknown tool'
        
                # append tool call result to the messages 
                messages.append({"role": "tool", "tool_call_id": call_id, "content": str(result)})

        # continue the loop after tool calls
        return True
