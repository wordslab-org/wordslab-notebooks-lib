{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0bdbfa3-6397-46f5-9cbf-ce6d574bbd11",
   "metadata": {},
   "source": [
    "# wordslab-notebooks-lib.chat\n",
    "\n",
    "> Chat with local and remote LLMs in the context of the wordslab-notebooks environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c915d8d-bf1a-41e4-a262-409595268e1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T14:58:39.738472Z",
     "iopub.status.busy": "2026-01-04T14:58:39.738374Z",
     "iopub.status.idle": "2026-01-04T14:58:39.740440Z",
     "shell.execute_reply": "2026-01-04T14:58:39.740065Z",
     "shell.execute_reply.started": "2026-01-04T14:58:39.738464Z"
    }
   },
   "outputs": [],
   "source": [
    "#| default_exp chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "16eebe41-aa01-45fa-8d37-22dcb74ba621",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T22:01:23.026235Z",
     "iopub.status.busy": "2026-01-05T22:01:23.026154Z",
     "iopub.status.idle": "2026-01-05T22:01:23.029401Z",
     "shell.execute_reply": "2026-01-05T22:01:23.028950Z",
     "shell.execute_reply.started": "2026-01-05T22:01:23.026228Z"
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "from abc import ABC, abstractmethod\n",
    "from collections.abc import Sequence as SequenceType\n",
    "from typing import Callable, Optional, Union, Sequence, Mapping, Any, Literal\n",
    "from datetime import datetime\n",
    "import inspect, json, re, time, traceback\n",
    "\n",
    "from toolslm.funccall import get_schema, call_func\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "\n",
    "from ollama import Client\n",
    "from ollama._types import Options\n",
    "from openai import OpenAI\n",
    "\n",
    "from wordslab_notebooks_lib.env import WordslabEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86096ba-efc1-44e4-bb28-904005d36407",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T18:06:54.991860Z",
     "iopub.status.busy": "2026-01-03T18:06:54.991772Z",
     "iopub.status.idle": "2026-01-03T18:06:54.995220Z",
     "shell.execute_reply": "2026-01-03T18:06:54.993556Z",
     "shell.execute_reply.started": "2026-01-03T18:06:54.991853Z"
    }
   },
   "source": [
    "## Observable conversation turns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f46f24e0-a93a-47c0-8b8c-302b97649839",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T21:42:07.316715Z",
     "iopub.status.busy": "2026-01-05T21:42:07.316604Z",
     "iopub.status.idle": "2026-01-05T21:42:07.322046Z",
     "shell.execute_reply": "2026-01-05T21:42:07.321475Z",
     "shell.execute_reply.started": "2026-01-05T21:42:07.316706Z"
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class ChatTurn:\n",
    "    def __init__(self, refresh_display: Callable[None, None] = None):\n",
    "        self.thinking_chunks = []\n",
    "        self.content_chunks = []\n",
    "        self.tool_calls = {}\n",
    "        self.refresh_display = refresh_display\n",
    "\n",
    "    def append_thinking(self, chunk:str):\n",
    "        self.thinking_chunks.append(chunk)\n",
    "        if self.refresh_display:\n",
    "            self.refresh_display()\n",
    "\n",
    "    @property\n",
    "    def thinking(self):\n",
    "        return \"\".join(self.thinking_chunks)\n",
    "    \n",
    "    def append_content(self, chunk:str):\n",
    "        self.content_chunks.append(chunk)\n",
    "        if self.refresh_display:\n",
    "            self.refresh_display()\n",
    "\n",
    "    @property\n",
    "    def content(self):\n",
    "        return \"\".join(self.content_chunks)\n",
    "    \n",
    "    def append_tool_call(self, tool_name:str, params:dict):\n",
    "        self.tool_calls[tool_name] = { \"params\": params }\n",
    "        if self.refresh_display:\n",
    "            self.refresh_display()\n",
    "\n",
    "    def start_tool_call(self, tool_name:str):\n",
    "        self.tool_calls[tool_name][\"start_time\"] = time.time()\n",
    "        if self.refresh_display:\n",
    "            self.refresh_display()\n",
    "\n",
    "    def end_tool_call(self, tool_name:str, result: object):\n",
    "        self.tool_calls[tool_name][\"end_time\"] = time.time()\n",
    "        self.tool_calls[tool_name][\"result\"] = str(result)\n",
    "        if self.refresh_display:\n",
    "            self.refresh_display()\n",
    "    \n",
    "    def to_markdown(self, hide_thinking:bool=True, hide_tool_calls:bool=True):\n",
    "        output = \"\"\n",
    "        if len(self.thinking_chunks) > 0:\n",
    "            if not hide_thinking:\n",
    "                output += \"> [Thinking]\\n\\n\"\n",
    "                output += \"\\n\".join(f\"> {line}\" for line in \"\".join(self.thinking_chunks).splitlines())\n",
    "            else:\n",
    "                output += f\"> [Thinking] ... thought in {sum(s.count(' ') + s.count('\\n') for s in self.thinking_chunks)} words\\n\\n\"\n",
    "        if len(self.content_chunks) > 0:\n",
    "            output += \"\".join(self.content_chunks) + \"\\n\\n\"\n",
    "        if len(self.tool_calls.keys()) > 0:\n",
    "            if not hide_tool_calls:\n",
    "                for tool_name in self.tool_calls.keys():\n",
    "                    output += \"> [Tool call]\\n\"\n",
    "                    tool_call = self.tool_calls[tool_name]\n",
    "                    if \"params\" in tool_call:\n",
    "                        output += f\"> - model wants to call `{tool_name}` with parameters `{tool_call[\"params\"]}`\\n\"\n",
    "                    if \"start_time\" in tool_call:\n",
    "                        output += f\"> - agent called {tool_name} at {datetime.fromtimestamp(tool_call[\"start_time\"]).strftime(\"%H:%M:%S\")}\\n\"\n",
    "                    if \"end_time\" in tool_call:\n",
    "                        result = tool_call[\"result\"]\n",
    "                        output += f\"> - {tool_name} returned `{result if len(result)<=100 else result[:97]+'...'}` in {(tool_call[\"end_time\"]-tool_call[\"start_time\"]):.3f} sec\\n\"\n",
    "                    output += \"\\n\"\n",
    "            else:\n",
    "                for tool_name in self.tool_calls.keys():\n",
    "                    tool_call = self.tool_calls[tool_name]\n",
    "                    if \"end_time\" in tool_call:\n",
    "                        result = self.tool_calls[tool_name][\"result\"]\n",
    "                        output += f\"> [Tool call] ... `{tool_name}` returned `{result if len(result)<=50 else result[:47]+'...'}`\\n\\n\"\n",
    "                    elif \"start_time\" in tool_call:                        \n",
    "                        output += f\"> [Tool call] ... agent is calling `{tool_name}`\\n\\n\"\n",
    "                    elif \"params\" in tool_call:                        \n",
    "                        output += f\"> [Tool call] ... model wants to call `{tool_name}`\\n\\n\"\n",
    "        return output\n",
    "\n",
    "class ChatTurns:\n",
    "    def __init__(self, notebook_display:bool=True, hide_thinking:bool=True, hide_tool_calls:bool=True):\n",
    "        self.chat_turns = []\n",
    "        self.notebook_display = notebook_display\n",
    "        self.hide_thinking = hide_thinking\n",
    "        self.hide_tool_calls = hide_tool_calls\n",
    "\n",
    "    def new_turn(self):\n",
    "        new_turn = ChatTurn(self.refresh_notebook_display)\n",
    "        self.chat_turns.append(new_turn)\n",
    "        return new_turn\n",
    "    \n",
    "    def refresh_notebook_display(self):\n",
    "        clear_output(wait=True)\n",
    "        output = \"\"\n",
    "        for turn in self.chat_turns:\n",
    "            output += turn.to_markdown(self.hide_thinking, self.hide_tool_calls)\n",
    "        display(Markdown(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d1362b05-cffa-4d19-83c5-49c010c1750d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T21:34:47.548707Z",
     "iopub.status.busy": "2026-01-05T21:34:47.548605Z",
     "iopub.status.idle": "2026-01-05T21:34:50.589051Z",
     "shell.execute_reply": "2026-01-05T21:34:50.588635Z",
     "shell.execute_reply.started": "2026-01-05T21:34:47.548699Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> [Thinking] ... thought in 10 words\n",
       "\n",
       "I need to call 2 tools.\n",
       "\n",
       "> [Tool call] ... `myfunc` returned `17.43`\n",
       "\n",
       "> [Thinking] ... thought in 10 words\n",
       "\n",
       "> [Tool call] ... `myfunc2` returned `The weather is nice today but clouds an wind ar...`\n",
       "\n",
       "> [Thinking] ... thought in 11 words\n",
       "\n",
       "This is the incredible result.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "turns = ChatTurns()\n",
    "\n",
    "turn = turns.new_turn()\n",
    "turn.append_thinking(\"I think a lot longer.\\nIn sentences.\\n\\nWith line breaks.\")\n",
    "time.sleep(0.5)\n",
    "turn.append_content(\"I need to call 2 tools.\")\n",
    "time.sleep(0.5)\n",
    "turn.append_tool_call(\"myfunc\", {\"param1\": \"value1\", \"param2\": \"value2\"})\n",
    "turn.start_tool_call(\"myfunc\")\n",
    "time.sleep(0.5)\n",
    "turn.end_tool_call(\"myfunc\", 17.43)\n",
    "turn1 = turn\n",
    "\n",
    "turn = turns.new_turn()\n",
    "turn.append_thinking(\"Ok, I got the first result, now call the second tool.\")\n",
    "time.sleep(0.5)\n",
    "turn.append_tool_call(\"myfunc2\", {})\n",
    "turn.start_tool_call(\"myfunc2\")\n",
    "time.sleep(0.5)\n",
    "turn.end_tool_call(\"myfunc2\", \"The weather is nice today but clouds an wind are coming for tommorow and the rest of the week will be awful\")\n",
    "\n",
    "turn = turns.new_turn()\n",
    "turn.append_thinking(\"Ok, I got the second result, now I can answer the question.\")\n",
    "time.sleep(0.5)\n",
    "turn.append_content(\"This is the incredible result.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da052dd6-95d7-46fa-a90e-cca3ab599d13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T14:58:45.440265Z",
     "iopub.status.busy": "2026-01-04T14:58:45.440188Z",
     "iopub.status.idle": "2026-01-04T14:58:45.442618Z",
     "shell.execute_reply": "2026-01-04T14:58:45.442168Z",
     "shell.execute_reply.started": "2026-01-04T14:58:45.440258Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I think a lot longer.\\nIn sentences.\\n\\nWith line breaks.',\n",
       " 'I need to call 2 tools.')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "turn1.thinking, turn1.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac35c7a8-c1cc-49b5-bb87-ef5cbf8d6509",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T14:58:45.442868Z",
     "iopub.status.busy": "2026-01-04T14:58:45.442781Z",
     "iopub.status.idle": "2026-01-04T14:58:45.465973Z",
     "shell.execute_reply": "2026-01-04T14:58:45.465502Z",
     "shell.execute_reply.started": "2026-01-04T14:58:45.442862Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> [Thinking] ... thought in 10 words\n",
       "\n",
       "I need to call 2 tools.\n",
       "\n",
       "> [Tool call] ... `myfunc` returned `17.43`\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(turn1.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9fd10db-4630-436e-92ad-70f301858105",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T14:58:45.466223Z",
     "iopub.status.busy": "2026-01-04T14:58:45.466154Z",
     "iopub.status.idle": "2026-01-04T14:58:45.479290Z",
     "shell.execute_reply": "2026-01-04T14:58:45.478843Z",
     "shell.execute_reply.started": "2026-01-04T14:58:45.466216Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> [Thinking]\n",
       ">\n",
       "> I think a lot longer.\n",
       "In sentences.\n",
       ">\n",
       "> With line breaks.\n",
       "\n",
       "I need to call 2 tools.\n",
       "\n",
       "> [Tool call]\n",
       "> - model wants to call `myfunc` with parameters `{'param1': 'value1', 'param2': 'value2'}`\n",
       "> - agent called myfunc at 15:58:43\n",
       "> - myfunc returned `17.43` in 0.501 sec\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(turn1.to_markdown(hide_thinking=False, hide_tool_calls=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c60d47-692f-4c8d-b80b-648bd511eedd",
   "metadata": {},
   "source": [
    "## Native tool calling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404c7d35-7125-4d99-8c91-06b011317f21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T22:43:19.547741Z",
     "iopub.status.busy": "2026-01-03T22:43:19.547579Z",
     "iopub.status.idle": "2026-01-03T22:43:19.550806Z",
     "shell.execute_reply": "2026-01-03T22:43:19.550334Z",
     "shell.execute_reply.started": "2026-01-03T22:43:19.547729Z"
    }
   },
   "source": [
    "Use python functions as tools callable by Large Language Models.\n",
    "\n",
    "The python functions must be fully documented:\n",
    "- type annotations are mandatory on all parameters and on the return type\n",
    "- a docstring after the function definition is mandatory, it should explain the return value\n",
    "- a descriptive comment after each parameter is also mandatory\n",
    "- the expected format is: one parameter by line, a traditional python comment at the end of the line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9131fc4a-84c0-4904-80ea-4a26246fe6a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T14:58:47.193128Z",
     "iopub.status.busy": "2026-01-04T14:58:47.193037Z",
     "iopub.status.idle": "2026-01-04T14:58:47.194983Z",
     "shell.execute_reply": "2026-01-04T14:58:47.194599Z",
     "shell.execute_reply.started": "2026-01-04T14:58:47.193121Z"
    }
   },
   "outputs": [],
   "source": [
    "def add(a: int,  # The first number\n",
    "        b: int   # The second number\n",
    "       ) -> int: # The sum of the two numbers\n",
    "  \"\"\"Add two numbers\"\"\"\n",
    "  return a + b\n",
    "\n",
    "\n",
    "def multiply(a: int,  # The first number \n",
    "             b: int   # The second number\n",
    "            ) -> int: # The product of the two numbers\n",
    "  \"\"\"Multiply two numbers\"\"\"\n",
    "  return a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb332c0-5e76-4021-acf7-7c949b5955b7",
   "metadata": {},
   "source": [
    "**Tool description format for ollama API**\n",
    "\n",
    "Here is the code used to process the tools parameter:\n",
    "\n",
    "```python\n",
    "for unprocessed_tool in tools or []:\n",
    "    yield convert_function_to_tool(unprocessed_tool) if callable(unprocessed_tool) else Tool.model_validate(unprocessed_tool)\n",
    "```\n",
    "\n",
    "So we can pass either a list of pyhton functions or a list of dictionaries conforming to a specific tool schema.\n",
    "\n",
    "Here are the expectations for the python functions documentation:\n",
    "\n",
    "```python\n",
    "def convert_function_to_tool(func: Callable) -> Tool:\n",
    " \n",
    "  -> def _parse_docstring(doc_string: Union[str, None]) -> dict[str, str]:\n",
    "  ...\n",
    "  for line in doc_string.splitlines():\n",
    "    ...\n",
    "    if lowered_line.startswith('args:'):\n",
    "      key = 'args'\n",
    "    elif lowered_line.startswith(('returns:', 'yields:', 'raises:')):\n",
    "      key = '_'\n",
    "  ...\n",
    "  for line in parsed_docstring['args'].splitlines():\n",
    "    ...\n",
    "    if ':' in line:\n",
    "      # Split the line on either:\n",
    "      # 1. A parenthetical expression like (integer) - captured in group 1\n",
    "      # 2. A colon :\n",
    "      # Followed by optional whitespace. Only split on first occurrence.\n",
    "      ...\n",
    "```\n",
    "\n",
    "This is much less robust and readable than what `toolslm.funccall.get_schema` does, so we will preprocess the list of python functions ourselves.\n",
    "\n",
    "Now let's see what tool description schema is expected by ollama.\n",
    "\n",
    "pydantic Tool.model_validate() accepts:\n",
    "- dict\n",
    "- Pydantic model instances\n",
    "- Objects with attributes (ORM-style, if configured)\n",
    "\n",
    "Here is the ollama schema:\n",
    "\n",
    "```python\n",
    "class Tool(SubscriptableBaseModel):\n",
    "  type: Optional[str] = 'function'\n",
    "\n",
    "  class Function(SubscriptableBaseModel):\n",
    "    name: Optional[str] = None\n",
    "    description: Optional[str] = None\n",
    "\n",
    "    class Parameters(SubscriptableBaseModel):\n",
    "      model_config = ConfigDict(populate_by_name=True)\n",
    "      type: Optional[Literal['object']] = 'object'\n",
    "      defs: Optional[Any] = Field(None, alias='$defs')\n",
    "      items: Optional[Any] = None\n",
    "      required: Optional[Sequence[str]] = None\n",
    "```\n",
    "\n",
    "So this is the schema expected by the openai completions API, as we will see below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba8552e-7c2f-4745-9806-1035bee73125",
   "metadata": {},
   "source": [
    "**Tool description formats for the openai API**\n",
    "\n",
    "The legacy openai completions API: `client.chat.completions.create(...)` expects tools to be described in a json format that uses the “wrapped function” schema:\n",
    "\n",
    "```\n",
    "tool = {\n",
    "  type: \"function\",\n",
    "  function: {\n",
    "    name,\n",
    "    description,\n",
    "    parameters\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "This is the canonical format for Chat Completions and is what OpenAI examples historically used.\n",
    "\n",
    "This format does not work for the new API: `client.responses.create(...)`\n",
    "\n",
    "The Responses API uses a flattened tool schema.\n",
    "\n",
    "```\n",
    "{\n",
    "  \"type\": \"function\",\n",
    "  \"name\": \"...\",\n",
    "  \"description\": \"..\",\n",
    "  \"parameters\": {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "      \"a\": {\"type\": \"integer\"},\n",
    "      \"b\": {\"type\": \"integer\"}\n",
    "    },\n",
    "    \"required\": [\"a\", \"b\"]\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "If you pass your wrapped version (function: {...}) to responses.create, you’ll get a schema validation error.\n",
    "\n",
    "- Chat Completions treats tools as message-level actions → nested function\n",
    "- Responses API treats tools as first-class model capabilities → flattened schema\n",
    "- The Responses API also supports non-function tools (web search, file search, computer use), which drove the redesign\n",
    "\n",
    "If you want maximum forward compatibility:\n",
    "- Use the flattened format\n",
    "- Even when working with Chat Completions, it’s easy to convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb46e7d6-baf8-45a3-9f73-7728dc0646d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T14:58:48.716651Z",
     "iopub.status.busy": "2026-01-04T14:58:48.716543Z",
     "iopub.status.idle": "2026-01-04T14:58:48.720849Z",
     "shell.execute_reply": "2026-01-04T14:58:48.719024Z",
     "shell.execute_reply.started": "2026-01-04T14:58:48.716643Z"
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def _get_function_schema(func: Callable, responsesAPIFormat: bool = False):\n",
    "    \"Get a json schema for a python function defined with comments for all parameters\"\n",
    "    if responsesAPIFormat:        \n",
    "        return {'type': 'function', **get_schema(func, pname='parameters')}\n",
    "    else:\n",
    "        return {'type': 'function', 'function': get_schema(func, pname='parameters')}\n",
    "\n",
    "def get_tools_schemas_and_functions(funcs: Sequence[Callable], responsesAPIFormat: bool = False):\n",
    "    \"\"\"Get a dictionary of json schemas and callable functions which can be used for native tool calling.\"\"\"\n",
    "    return {func.__name__: (_get_function_schema(func, responsesAPIFormat), func) for func in funcs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a726307-f8c0-4297-9e83-d902e463ab9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T14:58:49.406560Z",
     "iopub.status.busy": "2026-01-04T14:58:49.406457Z",
     "iopub.status.idle": "2026-01-04T14:58:49.412091Z",
     "shell.execute_reply": "2026-01-04T14:58:49.411769Z",
     "shell.execute_reply.started": "2026-01-04T14:58:49.406552Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'add': ({'type': 'function',\n",
       "   'function': {'name': 'add',\n",
       "    'description': 'Add two numbers\\n\\nReturns:\\n- type: integer',\n",
       "    'parameters': {'type': 'object',\n",
       "     'properties': {'a': {'type': 'integer',\n",
       "       'description': 'The first number'},\n",
       "      'b': {'type': 'integer', 'description': 'The second number'}},\n",
       "     'required': ['a', 'b']}}},\n",
       "  <function __main__.add(a: int, b: int) -> int>),\n",
       " 'multiply': ({'type': 'function',\n",
       "   'function': {'name': 'multiply',\n",
       "    'description': 'Multiply two numbers\\n\\nReturns:\\n- type: integer',\n",
       "    'parameters': {'type': 'object',\n",
       "     'properties': {'a': {'type': 'integer',\n",
       "       'description': 'The first number'},\n",
       "      'b': {'type': 'integer', 'description': 'The second number'}},\n",
       "     'required': ['a', 'b']}}},\n",
       "  <function __main__.multiply(a: int, b: int) -> int>)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tools_schemas_and_functions([add, multiply])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf2e99c9-0e59-484e-934a-8a1029656fba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T14:58:49.985599Z",
     "iopub.status.busy": "2026-01-04T14:58:49.985512Z",
     "iopub.status.idle": "2026-01-04T14:58:49.991179Z",
     "shell.execute_reply": "2026-01-04T14:58:49.989557Z",
     "shell.execute_reply.started": "2026-01-04T14:58:49.985591Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'add': ({'type': 'function',\n",
       "   'name': 'add',\n",
       "   'description': 'Add two numbers\\n\\nReturns:\\n- type: integer',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'a': {'type': 'integer', 'description': 'The first number'},\n",
       "     'b': {'type': 'integer', 'description': 'The second number'}},\n",
       "    'required': ['a', 'b']}},\n",
       "  <function __main__.add(a: int, b: int) -> int>),\n",
       " 'multiply': ({'type': 'function',\n",
       "   'name': 'multiply',\n",
       "   'description': 'Multiply two numbers\\n\\nReturns:\\n- type: integer',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'a': {'type': 'integer', 'description': 'The first number'},\n",
       "     'b': {'type': 'integer', 'description': 'The second number'}},\n",
       "    'required': ['a', 'b']}},\n",
       "  <function __main__.multiply(a: int, b: int) -> int>)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tools_schemas_and_functions([add, multiply], responsesAPIFormat=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "045b8921-3eba-414c-944a-6d441b182b2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T14:58:50.606615Z",
     "iopub.status.busy": "2026-01-04T14:58:50.606519Z",
     "iopub.status.idle": "2026-01-04T14:58:50.609953Z",
     "shell.execute_reply": "2026-01-04T14:58:50.609196Z",
     "shell.execute_reply.started": "2026-01-04T14:58:50.606607Z"
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class ToolExecutionError(Exception):\n",
    "    \"\"\"Raised when a tool cannot be executed safely.\"\"\"\n",
    "\n",
    "class Tools:\n",
    "    \"\"\"\"Execute tools implemented as python functions with Large Language Models.\n",
    "    The python functions must be fully documented:\n",
    "    - type annotations are mandatory on all parameters and on the return type\n",
    "    - a docstring after the function definition is mandatory\n",
    "    - a descriptive comment after each parameter and the return type is also mandatory\n",
    "    - the expected format is: one parameter by line, a traditional python comment at the end of the line\n",
    "    \"\"\"   \n",
    "    def __init__(self, python_functions:Sequence[Callable], responsesAPIFormat:bool=False):\n",
    "        self.schemas_and_functions = get_tools_schemas_and_functions(python_functions, responsesAPIFormat=responsesAPIFormat)\n",
    "\n",
    "    def has_tool(self, tool_name:str):\n",
    "        return tool_name in self.schemas_and_functions\n",
    "    \n",
    "    def get_schemas(self):\n",
    "        return [t[0] for t in self.schemas_and_functions.values()]\n",
    "\n",
    "    def get_schema(self, tool_name:str):\n",
    "        return self.schemas_and_functions[tool_name][0]\n",
    "\n",
    "    def get_functions(self):\n",
    "        return [t[1] for t in self.schemas_and_functions.values()]\n",
    "\n",
    "    def get_function(self, tool_name:str):\n",
    "        return self.schemas_and_functions[tool_name][1]\n",
    "\n",
    "    def call(self, tool_name:str, tool_arguments_dict:Mapping[str,Any]):\n",
    "        # 1. Resolve the tool safely\n",
    "        try:\n",
    "            self.get_function(tool_name)\n",
    "        except Exception as e:\n",
    "            raise ToolExecutionError(f\"Tool '{tool_name}' does not exist or could not be resolved.\")\n",
    "            \n",
    "        # 2. Execute the tool with runtime protection\n",
    "        try:\n",
    "            return call_func(tool_name, tool_arguments_dict, self.get_functions(), raise_on_err=True)\n",
    "        except Exception as e:\n",
    "            tb = traceback.format_exc()\n",
    "            raise ToolExecutionError(f\"Tool '{tool_name}' raised an exception: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90404ad5-ebac-40e1-ae45-7ba79a99a595",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T14:58:51.628380Z",
     "iopub.status.busy": "2026-01-04T14:58:51.628271Z",
     "iopub.status.idle": "2026-01-04T14:58:51.631440Z",
     "shell.execute_reply": "2026-01-04T14:58:51.630940Z",
     "shell.execute_reply.started": "2026-01-04T14:58:51.628372Z"
    }
   },
   "outputs": [],
   "source": [
    "tools = Tools([add,multiply])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd3ae44d-e38f-4f2d-91a2-6759c220394c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T14:58:53.842180Z",
     "iopub.status.busy": "2026-01-04T14:58:53.842084Z",
     "iopub.status.idle": "2026-01-04T14:58:53.845158Z",
     "shell.execute_reply": "2026-01-04T14:58:53.844685Z",
     "shell.execute_reply.started": "2026-01-04T14:58:53.842172Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools.has_tool(\"add\"), tools.has_tool(\"toto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da93dfce-b521-4cf4-91d5-36c489fed386",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T14:58:54.271324Z",
     "iopub.status.busy": "2026-01-04T14:58:54.271234Z",
     "iopub.status.idle": "2026-01-04T14:58:54.274055Z",
     "shell.execute_reply": "2026-01-04T14:58:54.273641Z",
     "shell.execute_reply.started": "2026-01-04T14:58:54.271317Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'function': {'name': 'add',\n",
       "   'description': 'Add two numbers\\n\\nReturns:\\n- type: integer',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'a': {'type': 'integer', 'description': 'The first number'},\n",
       "     'b': {'type': 'integer', 'description': 'The second number'}},\n",
       "    'required': ['a', 'b']}}},\n",
       " {'type': 'function',\n",
       "  'function': {'name': 'multiply',\n",
       "   'description': 'Multiply two numbers\\n\\nReturns:\\n- type: integer',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'a': {'type': 'integer', 'description': 'The first number'},\n",
       "     'b': {'type': 'integer', 'description': 'The second number'}},\n",
       "    'required': ['a', 'b']}}}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools.get_schemas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12d29f37-9138-46f6-946f-64f0a9e2a87f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T14:58:54.510936Z",
     "iopub.status.busy": "2026-01-04T14:58:54.510849Z",
     "iopub.status.idle": "2026-01-04T14:58:54.515045Z",
     "shell.execute_reply": "2026-01-04T14:58:54.513234Z",
     "shell.execute_reply.started": "2026-01-04T14:58:54.510929Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'function',\n",
       " 'function': {'name': 'add',\n",
       "  'description': 'Add two numbers\\n\\nReturns:\\n- type: integer',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'a': {'type': 'integer', 'description': 'The first number'},\n",
       "    'b': {'type': 'integer', 'description': 'The second number'}},\n",
       "   'required': ['a', 'b']}}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools.get_schema(\"add\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac1bcb66-7762-4869-8911-3f8dbac92e85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T14:58:55.063721Z",
     "iopub.status.busy": "2026-01-04T14:58:55.063618Z",
     "iopub.status.idle": "2026-01-04T14:58:55.068278Z",
     "shell.execute_reply": "2026-01-04T14:58:55.067767Z",
     "shell.execute_reply.started": "2026-01-04T14:58:55.063713Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<function __main__.add(a: int, b: int) -> int>,\n",
       " <function __main__.multiply(a: int, b: int) -> int>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools.get_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b98d00a-8c88-4dc3-a290-1b19739c59aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T14:58:55.276741Z",
     "iopub.status.busy": "2026-01-04T14:58:55.276643Z",
     "iopub.status.idle": "2026-01-04T14:58:55.279432Z",
     "shell.execute_reply": "2026-01-04T14:58:55.278992Z",
     "shell.execute_reply.started": "2026-01-04T14:58:55.276733Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.add(a: int, b: int) -> int>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools.get_function(\"add\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04a8b645-d905-46a8-a522-f9517bc4859f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T14:58:55.729458Z",
     "iopub.status.busy": "2026-01-04T14:58:55.729374Z",
     "iopub.status.idle": "2026-01-04T14:58:55.731703Z",
     "shell.execute_reply": "2026-01-04T14:58:55.731321Z",
     "shell.execute_reply.started": "2026-01-04T14:58:55.729451Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools.call(\"add\", {\"a\": 1, \"b\": 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f61e20-a863-4571-80a7-dbe61202722e",
   "metadata": {},
   "source": [
    "## Model client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85dd131a-07e4-440a-b817-7dac90e2dafc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T14:58:57.706027Z",
     "iopub.status.busy": "2026-01-04T14:58:57.705939Z",
     "iopub.status.idle": "2026-01-04T14:58:57.708588Z",
     "shell.execute_reply": "2026-01-04T14:58:57.708279Z",
     "shell.execute_reply.started": "2026-01-04T14:58:57.706018Z"
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class ModelClient(ABC):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str,\n",
    "        base_url: Optional[str] = None,\n",
    "        api_key: Optional[str] = None,\n",
    "        context_size: Optional[int] = None,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.base_url = base_url\n",
    "        self.api_key = api_key\n",
    "        self.context_size = context_size\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(\n",
    "        self,\n",
    "        messages: Sequence[Mapping[str, Any]],\n",
    "        chat_turns: ChatTurns,\n",
    "        tools: Tools = None,\n",
    "        think: Union[bool, Literal[\"low\", \"medium\", \"high\"], None] = None,\n",
    "        max_new_tokens: Optional[int] = None,\n",
    "        seed: Optional[int] = None,\n",
    "        temperature: Optional[float] = None,\n",
    "        top_k: Optional[int] = None,\n",
    "        top_p: Optional[float] = None,\n",
    "        min_p: Optional[float] = None,\n",
    "    ) -> bool:\n",
    "        \"\"\"\n",
    "        Execute a model call and return the model response.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8d8979-6a3f-47b6-8654-e3a1e9254011",
   "metadata": {},
   "source": [
    "### ollama model client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3160e75-f63f-4c46-81b6-0bd67c1f31fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T15:03:56.772452Z",
     "iopub.status.busy": "2026-01-04T15:03:56.772351Z",
     "iopub.status.idle": "2026-01-04T15:03:56.777920Z",
     "shell.execute_reply": "2026-01-04T15:03:56.777395Z",
     "shell.execute_reply.started": "2026-01-04T15:03:56.772444Z"
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "_whitespace_pattern = re.compile(r\"\\s+\")\n",
    "\n",
    "def _messages_words(messages):\n",
    "    return sum([len(_whitespace_pattern.findall(message[\"content\"])) for message in messages if message[\"role\"] in {\"user\", \"assitant\"}])\n",
    "\n",
    "class OllamaModelClient(ModelClient):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str,\n",
    "        context_size: int = 32768, # This is the default value for the ollama server in wordslab-notebooks\n",
    "        base_url: str = \"http://localhost:11434\",\n",
    "        api_key: Optional[str] = None,  # If not provided, the optional key will be pulled from WordslabEnv\n",
    "    ):\n",
    "        super().__init__(model, base_url, api_key, context_size)\n",
    "\n",
    "        # Initialize API client\n",
    "        if not api_key:\n",
    "            env = WordslabEnv()\n",
    "            api_key = env.cloud_ollama_api_key\n",
    "        if api_key:\n",
    "            headers = {'Authorization': 'Bearer ' + api_key}\n",
    "        else:            \n",
    "            headers = {}\n",
    "        self.client = Client(host=self.base_url, headers=headers)\n",
    "        \n",
    "        # Load model in memory with the right context length\n",
    "        print(f\"ollama: loading model {self.model} with context size {self.context_size} ... \", end=\"\");\n",
    "        self.client.chat(model=self.model, messages=[{'role': 'user', 'content': 'say yes'}], options=Options(num_ctx=self.context_size, num_predict=1))\n",
    "        print(f\"ok\");\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        messages: Sequence[Mapping[str, Any]],\n",
    "        chat_turns: ChatTurns,\n",
    "        tools: Tools = None,\n",
    "        think: Union[bool, Literal[\"low\", \"medium\", \"high\"], None] = None,\n",
    "        max_new_tokens: Optional[int] = None,\n",
    "        seed: Optional[int] = None,\n",
    "        temperature: Optional[float] = None,\n",
    "        top_k: Optional[int] = None,\n",
    "        top_p: Optional[float] = None,\n",
    "        min_p: Optional[float] = None,      \n",
    "    ) -> bool:\n",
    "        # Check tools parameter type\n",
    "        if tools and not isinstance(tools, Tools):\n",
    "            raise TypeError(\"Argument tools must be of type wordslab_notebooks_lib.chat.Tools. Create a tools object with the syntax: Tools([func1, func2, func3]), where the parameters are documented python functions.\")\n",
    "        \n",
    "        # Immediate user feedback\n",
    "        print(f\"ollama: processing {_messages_words(messages)} words with `{self.model}` ...\")\n",
    "        \n",
    "        # Observable conversation turn\n",
    "        chat_turn = chat_turns.new_turn()\n",
    "        \n",
    "        stream = self.client.chat(\n",
    "            model = self.model,\n",
    "            messages = messages,\n",
    "            tools = tools.get_schemas() if tools else None,\n",
    "            stream = True,\n",
    "            think = think,\n",
    "            options = Options(num_ctx = self.context_size, num_predict = max_new_tokens, seed = seed,\n",
    "                              temperature = temperature, top_k=top_k, top_p=top_p, min_p=min_p)\n",
    "        )\n",
    "    \n",
    "        # Streaming: accumulate the partial fields\n",
    "        tool_calls = []        \n",
    "        for chunk in stream:\n",
    "            if chunk.message.thinking:\n",
    "                chat_turn.append_thinking(chunk.message.thinking)                \n",
    "            if chunk.message.content:\n",
    "                chat_turn.append_content(chunk.message.content)\n",
    "            if chunk.message.tool_calls:\n",
    "                tool_calls.extend(chunk.message.tool_calls)\n",
    "                for tc in chunk.message.tool_calls:\n",
    "                    chat_turn.append_tool_call(tc.function.name, tc.function.arguments)\n",
    "        \n",
    "        # append accumulated fields to the messages\n",
    "        if chat_turn.thinking or chat_turn.content or tool_calls:\n",
    "            messages.append({'role': 'assistant', 'thinking': chat_turn.thinking, 'content': chat_turn.content, 'tool_calls': tool_calls})\n",
    "    \n",
    "        # end the loop if there is no more tool calls\n",
    "        if not tool_calls: \n",
    "            return False      \n",
    "            \n",
    "        # execute tool calls  \n",
    "        else:    \n",
    "            for tc in tool_calls:\n",
    "                if tools.has_tool(tc.function.name):\n",
    "                    chat_turn.start_tool_call(tc.function.name)\n",
    "                    result = tools.call(tc.function.name, tc.function.arguments)\n",
    "                    chat_turn.end_tool_call(tc.function.name, result)\n",
    "                else:\n",
    "                    result = 'Unknown tool'\n",
    "        \n",
    "                # append tool call result to the messages \n",
    "                messages.append({'role': 'tool', 'tool_name': tc.function.name, 'content': str(result)})\n",
    "\n",
    "        # continue the loop after tool calls\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d77f8e6-8322-4da8-830e-b2b26ccec705",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T15:04:14.750285Z",
     "iopub.status.busy": "2026-01-04T15:04:14.750207Z",
     "iopub.status.idle": "2026-01-04T15:04:14.752850Z",
     "shell.execute_reply": "2026-01-04T15:04:14.752374Z",
     "shell.execute_reply.started": "2026-01-04T15:04:14.750278Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'qwen3:30b'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = env.default_model_code\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "05e3a5f1-6fe0-4dff-9a5a-8ac348f7d012",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T15:04:15.223479Z",
     "iopub.status.busy": "2026-01-04T15:04:15.223376Z",
     "iopub.status.idle": "2026-01-04T15:04:21.978608Z",
     "shell.execute_reply": "2026-01-04T15:04:21.977593Z",
     "shell.execute_reply.started": "2026-01-04T15:04:15.223471Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ollama: loading model qwen3:30b with context size 65000 ... ok\n"
     ]
    }
   ],
   "source": [
    "oclient = OllamaModelClient(model, context_size=65000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0795c383-f0a7-4d3a-ac1c-c7a19799ffc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T15:06:31.037447Z",
     "iopub.status.busy": "2026-01-04T15:06:31.037357Z",
     "iopub.status.idle": "2026-01-04T15:06:32.835331Z",
     "shell.execute_reply": "2026-01-04T15:06:32.834208Z",
     "shell.execute_reply.started": "2026-01-04T15:06:31.037440Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> [Thinking] ... thought in 205 words\n",
       "\n",
       "Sunlight scatters in Earth's atmosphere, with shorter blue wavelengths scattering more effectively than other colors, causing the sky to appear blue.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = [{'role': 'user', 'content': 'In one sentence: why is the sky blue?'}]\n",
    "turns = ChatTurns()\n",
    "tool_calls_to_process = oclient(messages, turns, think=True, max_new_tokens=1000, seed=42, temperature=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b0f45c04-ef53-4755-b846-e59a1a8bceeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T15:06:36.596014Z",
     "iopub.status.busy": "2026-01-04T15:06:36.595889Z",
     "iopub.status.idle": "2026-01-04T15:06:36.598437Z",
     "shell.execute_reply": "2026-01-04T15:06:36.598058Z",
     "shell.execute_reply.started": "2026-01-04T15:06:36.596005Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'role': 'user', 'content': 'In one sentence: why is the sky blue?'},\n",
       "  {'role': 'assistant',\n",
       "   'thinking': 'Okay, the user is asking why the sky is blue in one sentence. Hmm, they probably want a quick, straightforward explanation without any extra fluff. Maybe they\\'re in a hurry or just need a simple fact for a conversation.  \\n\\nI remember the main science behind it: Rayleigh scattering. Sunlight is white light, made of all colors, but blue light scatters more because its wavelengths are shorter. That scattered blue light reaches our eyes from all directions, making the sky look blue.  \\n\\nGotta make sure it\\'s concise though—just one sentence. Let me draft: \"Sunlight scatters in Earth\\'s atmosphere, with shorter blue wavelengths scattering more than other colors, making the sky appear blue.\" That covers it without technical jargon.  \\n\\nWait, is the user a kid? A curious adult? Either way, keep it simple. No need for \"molecules\" or \"wavelengths\" unless necessary. The key is \"blue scatters more,\" which is the core idea.  \\n\\nAlso, no need to mention why sunsets are red—it\\'s tempting but irrelevant here. User asked for one sentence only, so stick to the point.  \\n\\nDouble-checking sources: Yep, Rayleigh scattering is the accepted reason. *Nods* Ready to go with that sentence.\\n',\n",
       "   'content': \"Sunlight scatters in Earth's atmosphere, with shorter blue wavelengths scattering more effectively than other colors, causing the sky to appear blue.\",\n",
       "   'tool_calls': []}],\n",
       " False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages, tool_calls_to_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3848807c-8298-4db2-ad92-9a8737fe7c8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T15:06:37.098540Z",
     "iopub.status.busy": "2026-01-04T15:06:37.098436Z",
     "iopub.status.idle": "2026-01-04T15:06:59.414138Z",
     "shell.execute_reply": "2026-01-04T15:06:59.413374Z",
     "shell.execute_reply.started": "2026-01-04T15:06:37.098532Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> [Thinking] ... thought in 2910 words\n",
       "\n",
       "> [Tool call] ... `add` returned `90327899`\n",
       "\n",
       "> [Tool call] ... `multiply` returned `37824085083058`\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = [{'role': 'user', 'content': \"Using only the provided tools to make no mistake, what is (11545468+78782431)*418742?\"}]\n",
    "turns = ChatTurns()\n",
    "tools = Tools([add, multiply])\n",
    "tool_calls_to_process = oclient(messages, turns, tools=tools, think=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f07a2cf7-72c3-4e9a-9f0b-83e75d4c6501",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T15:07:04.069398Z",
     "iopub.status.busy": "2026-01-04T15:07:04.069324Z",
     "iopub.status.idle": "2026-01-04T15:07:04.073271Z",
     "shell.execute_reply": "2026-01-04T15:07:04.072887Z",
     "shell.execute_reply.started": "2026-01-04T15:07:04.069391Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_calls_to_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a43506f4-3a67-4659-b073-681565b16be3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T15:07:04.758754Z",
     "iopub.status.busy": "2026-01-04T15:07:04.758665Z",
     "iopub.status.idle": "2026-01-04T15:07:07.738106Z",
     "shell.execute_reply": "2026-01-04T15:07:07.737607Z",
     "shell.execute_reply.started": "2026-01-04T15:07:04.758748Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> [Thinking] ... thought in 2910 words\n",
       "\n",
       "> [Tool call] ... `add` returned `90327899`\n",
       "\n",
       "> [Tool call] ... `multiply` returned `37824085083058`\n",
       "\n",
       "> [Thinking] ... thought in 155 words\n",
       "\n",
       "The result of (11545468 + 78782431) * 418742 is **37824085083058**.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tool_calls_to_process = oclient(messages, turns, tools=tools, think=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cc9eaed4-8afd-4aec-af36-9dc85fbfb918",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T15:07:12.353218Z",
     "iopub.status.busy": "2026-01-04T15:07:12.353134Z",
     "iopub.status.idle": "2026-01-04T15:07:12.355640Z",
     "shell.execute_reply": "2026-01-04T15:07:12.355309Z",
     "shell.execute_reply.started": "2026-01-04T15:07:12.353211Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_calls_to_process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a589734-14f6-476d-8445-6bc0fc1a65f4",
   "metadata": {},
   "source": [
    "## openrouter chat client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7c4efe62-a7e2-426d-9777-85e7584eaa6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T15:07:33.369142Z",
     "iopub.status.busy": "2026-01-04T15:07:33.369044Z",
     "iopub.status.idle": "2026-01-04T15:07:33.371383Z",
     "shell.execute_reply": "2026-01-04T15:07:33.370817Z",
     "shell.execute_reply.started": "2026-01-04T15:07:33.369134Z"
    }
   },
   "outputs": [],
   "source": [
    "env = WordslabEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c8c0a058-d4df-4063-9870-52f47f4fd691",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T15:07:56.640535Z",
     "iopub.status.busy": "2026-01-04T15:07:56.640446Z",
     "iopub.status.idle": "2026-01-04T15:07:56.647159Z",
     "shell.execute_reply": "2026-01-04T15:07:56.646719Z",
     "shell.execute_reply.started": "2026-01-04T15:07:56.640528Z"
    }
   },
   "outputs": [],
   "source": [
    "client = OpenAI(base_url=\"https://openrouter.ai/api/v1\", api_key=env.cloud_openrouter_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "27a0c9eb-6c47-4a23-b8be-eba2fb8923c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T20:48:27.347083Z",
     "iopub.status.busy": "2026-01-05T20:48:27.347003Z",
     "iopub.status.idle": "2026-01-05T20:48:30.977131Z",
     "shell.execute_reply": "2026-01-05T20:48:30.976335Z",
     "shell.execute_reply.started": "2026-01-05T20:48:27.347075Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChoiceDelta(content='', function_call=None, refusal=None, role='assistant', tool_calls=None, reasoning=\"**Identifying a Solution**\\n\\nI've homed in on the core challenge: pinpointing the smallest palindrome exceeding 130. The constraints are clear, and I'm strategizing how to efficiently generate and validate candidate numbers. I am starting by looking at the numbers directly after 130.\\n\\n\\n\", reasoning_details=[{'index': 0, 'type': 'reasoning.text', 'text': \"**Identifying a Solution**\\n\\nI've homed in on the core challenge: pinpointing the smallest palindrome exceeding 130. The constraints are clear, and I'm strategizing how to efficiently generate and validate candidate numbers. I am starting by looking at the numbers directly after 130.\\n\\n\\n\", 'format': 'google-gemini-v1'}], annotations=[])\n",
      "ChoiceDelta(content='', function_call=None, refusal=None, role='assistant', tool_calls=None, reasoning=\"**Determining the Answer**\\n\\nI've directly confirmed that 131 fulfills all criteria. No need to look further: it's the smallest palindrome that's larger than 130. I have confirmed that this number satisfies the relevant constraints, and I consider this the final result.\\n\\n\\n\", reasoning_details=[{'index': 0, 'type': 'reasoning.text', 'text': \"**Determining the Answer**\\n\\nI've directly confirmed that 131 fulfills all criteria. No need to look further: it's the smallest palindrome that's larger than 130. I have confirmed that this number satisfies the relevant constraints, and I consider this the final result.\\n\\n\\n\", 'format': 'google-gemini-v1'}], annotations=[])\n",
      "ChoiceDelta(content='The smallest palindrome number greater than 130 is **131**.\\n\\nA palindrome is a number that reads the same forwards and backwards. Since 131 reads as \"1-3-1\" in both directions and is the very next integer after 130 that follows this rule, it is the', function_call=None, refusal=None, role='assistant', tool_calls=None, reasoning=None, reasoning_details=[], annotations=[])\n",
      "ChoiceDelta(content=' correct answer.', function_call=None, refusal=None, role='assistant', tool_calls=None, reasoning=None, reasoning_details=[{'index': 0, 'type': 'reasoning.encrypted', 'data': 'CiIBjz1rXx1wDxrqK4iEiYLEfo4BC0TbRcSKhvVO48q1Ge9HCmgBjz1rX93sCp42z//JmKxDWvv3kYw3fkxVXSsbAkJ9/OsR5GxzvX3NRxq1GmWU6PzNix6QQ9aFfZmZqEwc3u/anTQiQwCLH+N77Rh7vLxAvr0VCSVLJ83rqljqFi93fMyjr0pjX9OE5gpZAY89a1/PpcHwJxd6EjqwLvSFlebK6nVnQDn90G86P3/tRXy2sZxYogVEh44KsQZ4r+0B/2ClLCNJf+EvIBCEs6zXHTMLx6AJIDU6SumITgIppnPdYWwoUuoKeAGPPWtfeJ7lICOOm3dtb2YzhnJiSGPI5m96K1G/2lwo04wSze+2vbS0FGKQnAYkOR21tRqDV+JGOKon/MdNhKTfjVd+TNPUfcQCNQh+dmlVajIJRyql4sYeYSg6Wz+AA0A0dNv6GWIKaXmKCBS087b+FUT1ZMBuSgpgAY89a1+ZR6BSJsZTg5YUGg11YzeDIsaqhs+FvHy/1XB4uPp3SW+mqIfysxta4e4y7oeOuTNButgOKnDeaJfPG8VSjSh0lT0R1dHEifHAIFbiF2IKQJXW07ns6L1E9syYCn8Bjz1rXwn3gb5Q8D+qPic5yZTziFidiTxpKH3uhjyYqTrZR8hRGnmVVk05a8E/5J81UkxOevZ5yAxLiFCdimXI5yr7LriL4bDiHajdSXVxdOiKHzb6Pqx3MiLySYUt1ToD3XUAIZQBmfDVw+Qd6SQdagvsi86NKMv793xrUMlECroBAY89a18BugGYHx3fQYckccMCOS91fxOFH7hBj24O746sJhbrBMLmQSPk0du421Zmd8Lx1Ns21/SwpHCfnQ3AEA9BZ9XfahR51tE96d9DjXO7kMsgGDPoLsDApRTnRenQbPJnpXqoQYZbsnE/H1B6Tb4wcE5xJpcBKLRvXhg+c14T5NYpFEcE3dHaNUH6xXj68ZLBl2Q5FtzqOd6tyxSLRhQJfa00yDFnpn+Lbo0zLSRqbP+ovAWIq79BCpMBAY89a1/ENqGNTYpM7VJzpDZus8SoUClqlwaOwrOOHYfz6NaB3CP3coADgPnltU0hZrGfE0w4xHdM85pJaT/2HHv9GQH2o2R4nAGwT3KInxBeAVo0TtE2M9aN78wDhveiyqPUsHO1uybcRorF3uUwEIAT8NX93ykz3FwcVZvO/aYtE9A7mVN4qYgXfjvIFwIBjFJs', 'format': 'google-gemini-v1'}], annotations=[])\n",
      "ChoiceDelta(content='', function_call=None, refusal=None, role='assistant', tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "model = \"google/gemini-3-flash-preview\"\n",
    "messages = [{'role': 'user', 'content': 'What is the smallest number palindrome greater than 130?'}]\n",
    "stream = client.chat.completions.create(model=model, messages=messages, stream=True, extra_body={\"reasoning\": {\"enabled\": True}})\n",
    "for chunk in stream:\n",
    "    delta = chunk.choices[0].delta\n",
    "    print(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ca7830b3-686f-4152-a957-483cd26428c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T21:01:55.278484Z",
     "iopub.status.busy": "2026-01-05T21:01:55.278361Z",
     "iopub.status.idle": "2026-01-05T21:01:58.366839Z",
     "shell.execute_reply": "2026-01-05T21:01:58.366028Z",
     "shell.execute_reply.started": "2026-01-05T21:01:55.278476Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChoiceDelta(content='', function_call=None, refusal=None, role='assistant', tool_calls=None, reasoning=\"**Calculating the Total Sum**\\n\\nI'm currently focused on the first step: summing the initial numbers. I've successfully employed the `add` function, and the intermediate result is now readily available. It's a significant figure, and I'm ready to proceed to the next stage after a brief review.\\n\\n\\n\", reasoning_details=[{'index': 0, 'type': 'reasoning.text', 'text': \"**Calculating the Total Sum**\\n\\nI'm currently focused on the first step: summing the initial numbers. I've successfully employed the `add` function, and the intermediate result is now readily available. It's a significant figure, and I'm ready to proceed to the next stage after a brief review.\\n\\n\\n\", 'format': 'google-gemini-v1'}], annotations=[])\n",
      "ChoiceDelta(content='', function_call=None, refusal=None, role='assistant', tool_calls=None, reasoning=\"**Initiating Multiplication Operations**\\n\\nI've got the total sum from the previous stage, a substantial number. Now, the plan is to multiply this sum by 418,742. I'm preparing to invoke the `multiply` function, and I'm ready to observe the final result soon.\\n\\n\\n\", reasoning_details=[{'index': 0, 'type': 'reasoning.text', 'text': \"**Initiating Multiplication Operations**\\n\\nI've got the total sum from the previous stage, a substantial number. Now, the plan is to multiply this sum by 418,742. I'm preparing to invoke the `multiply` function, and I'm ready to observe the final result soon.\\n\\n\\n\", 'format': 'google-gemini-v1'}], annotations=[])\n",
      "ChoiceDelta(content=None, function_call=None, refusal=None, role='assistant', tool_calls=[ChoiceDeltaToolCall(index=0, id='tool_add_bxj7qiqYR0YykpEmfzoC', function=ChoiceDeltaToolCallFunction(arguments='{\"b\":78782431,\"a\":11545468}', name='add'), type='function')], reasoning=None, reasoning_details=[{'index': 0, 'id': 'tool_add_bxj7qiqYR0YykpEmfzoC', 'type': 'reasoning.encrypted', 'data': 'CiQBjz1rX1bExybTM6VtK0kASX6FTxfBBwyi932MRRrEuYtvkdsKUQGPPWtfr55kYc/lU0ZhVkHjN6wZ74W1LVMPXkSF3tFEoDwQtfzpuzYnV7xzD1qz5CnzNvZm5i9eY0qQKJhzhXAC3FTmjZ6D2OdODRvDX7uUGwo0AY89a1/zpxr22ksclwFiLTfaAWYr57nJPyB5mHc8h6PIE3uu5ggyUE4VsZoaqVxv2q03jQp6AY89a187jC4w08IR8/fDOrJwQEOLQO5yNEUlQphL8lckJvAltj2ULGXY06WBV8yi1n3YGUZEAgLU4ihc/o8+/nVuk+OnKvicbju9XY82ZeP35D8yFVPEFMyHifVptjjPry8rCpr4N84UipqVfc0rmtFPBbE6+6TTFIUKaQGPPWtf6XkgaitG1kvzHAeW+I55A2dWbhyG/8Z15RjCkArCtKMEEf1r9v7G0ke7kNuIW8jhHF5H/wm/20m69zrsCp1obV+jbZR8T6lb3Km4EXJOldkm/U1lsHZRI8Lp2mkzreV1QbkqHwo2AY89a1/PJ5AClPojS2Y4jbD3oN8tblRCmUNTysi69fLd/4tqapYHz4CLAo+LMhFvzLN9N4qZ', 'format': 'google-gemini-v1'}], annotations=[])\n",
      "ChoiceDelta(content='', function_call=None, refusal=None, role='assistant', tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "messages = [{'role': 'user', 'content': \"Using only the provided tools to make no mistake, what is (11545468+78782431)*418742?\"}]\n",
    "tools = Tools([add, multiply])\n",
    "stream = client.chat.completions.create(model=model, messages=messages, tools = tools.get_schemas(), stream=True, extra_body={\"reasoning\": {\"enabled\": True}})\n",
    "for chunk in stream:\n",
    "    delta = chunk.choices[0].delta\n",
    "    print(delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2e9517-1fc5-421a-b409-b7d87b2e6138",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T16:30:04.908030Z",
     "iopub.status.busy": "2026-01-04T16:30:04.907935Z",
     "iopub.status.idle": "2026-01-04T16:30:04.912082Z",
     "shell.execute_reply": "2026-01-04T16:30:04.911400Z",
     "shell.execute_reply.started": "2026-01-04T16:30:04.908022Z"
    }
   },
   "source": [
    "**Openrouter API reference**\n",
    "\n",
    "Note: as of January 2026 - the OpenAI-compatible Responses API (Beta) is in beta stage and may have breaking changes. Use with caution in production environments.\n",
    "\n",
    "=> we will use the competions API for now\n",
    "\n",
    "https://openrouter.ai/docs/api/reference/overview\n",
    "\n",
    "REQUEST SCHEMA\n",
    "\n",
    "```typescript\n",
    "// Definitions of subtypes are below\n",
    "type Request = {\n",
    "  // Either \"messages\" or \"prompt\" is required\n",
    "  messages?: Message[];\n",
    "  prompt?: string;\n",
    "  // If \"model\" is unspecified, uses the user's default\n",
    "  model?: string; // See \"Supported Models\" section\n",
    "  // Allows to force the model to produce specific output format.\n",
    "  // See models page and note on this docs page for which models support it.\n",
    "  response_format?: { type: 'json_object' };\n",
    "  stop?: string | string[];\n",
    "  stream?: boolean; // Enable streaming\n",
    "  // See LLM Parameters (openrouter.ai/docs/api/reference/parameters)\n",
    "  max_tokens?: number; // Range: [1, context_length)\n",
    "  temperature?: number; // Range: [0, 2]\n",
    "  // Tool calling\n",
    "  // Will be passed down as-is for providers implementing OpenAI's interface.\n",
    "  // For providers with custom interfaces, we transform and map the properties.\n",
    "  // Otherwise, we transform the tools into a YAML template. The model responds with an assistant message.\n",
    "  // See models supporting tool calling: openrouter.ai/models?supported_parameters=tools\n",
    "  tools?: Tool[];\n",
    "  tool_choice?: ToolChoice;\n",
    "  // Advanced optional parameters\n",
    "  seed?: number; // Integer only\n",
    "  top_p?: number; // Range: (0, 1]\n",
    "  top_k?: number; // Range: [1, Infinity) Not available for OpenAI models\n",
    "  frequency_penalty?: number; // Range: [-2, 2]\n",
    "  presence_penalty?: number; // Range: [-2, 2]\n",
    "  repetition_penalty?: number; // Range: (0, 2]\n",
    "  logit_bias?: { [key: number]: number };\n",
    "  top_logprobs: number; // Integer only\n",
    "  min_p?: number; // Range: [0, 1]\n",
    "  top_a?: number; // Range: [0, 1]\n",
    "  // Reduce latency by providing the model with a predicted output\n",
    "  // https://platform.openai.com/docs/guides/latency-optimization#use-predicted-outputs\n",
    "  prediction?: { type: 'content'; content: string };\n",
    "  // OpenRouter-only parameters\n",
    "  // See \"Prompt Transforms\" section: openrouter.ai/docs/guides/features/message-transforms\n",
    "  transforms?: string[];\n",
    "  // See \"Model Routing\" section: openrouter.ai/docs/guides/features/model-routing\n",
    "  models?: string[];\n",
    "  route?: 'fallback';\n",
    "  // See \"Provider Routing\" section: openrouter.ai/docs/guides/routing/provider-selection\n",
    "  provider?: ProviderPreferences;\n",
    "  user?: string; // A stable identifier for your end-users. Used to help detect and prevent abuse.\n",
    "  \n",
    "  // Debug options (streaming only)\n",
    "  debug?: {\n",
    "    echo_upstream_body?: boolean; // If true, returns the transformed request body sent to the provider\n",
    "  };\n",
    "};\n",
    "\n",
    "// Subtypes:\n",
    "type TextContent = {\n",
    "  type: 'text';\n",
    "  text: string;\n",
    "};\n",
    "type ImageContentPart = {\n",
    "  type: 'image_url';\n",
    "  image_url: {\n",
    "    url: string; // URL or base64 encoded image data\n",
    "    detail?: string; // Optional, defaults to \"auto\"\n",
    "  };\n",
    "};\n",
    "type ContentPart = TextContent | ImageContentPart;\n",
    "type Message =\n",
    "  | {\n",
    "      role: 'user' | 'assistant' | 'system';\n",
    "      // ContentParts are only for the \"user\" role:\n",
    "      content: string | ContentPart[];\n",
    "      // If \"name\" is included, it will be prepended like this\n",
    "      // for non-OpenAI models: `{name}: {content}`\n",
    "      name?: string;\n",
    "    }\n",
    "  | {\n",
    "      role: 'tool';\n",
    "      content: string;\n",
    "      tool_call_id: string;\n",
    "      name?: string;\n",
    "    };\n",
    "type FunctionDescription = {\n",
    "  description?: string;\n",
    "  name: string;\n",
    "  parameters: object; // JSON Schema object\n",
    "};\n",
    "type Tool = {\n",
    "  type: 'function';\n",
    "  function: FunctionDescription;\n",
    "};\n",
    "type ToolChoice =\n",
    "  | 'none'\n",
    "  | 'auto'\n",
    "  | {\n",
    "      type: 'function';\n",
    "      function: {\n",
    "        name: string;\n",
    "      };\n",
    "``` \n",
    "\n",
    "RESPONSE SCHEMA\n",
    "\n",
    "```typescript\n",
    "// Definitions of subtypes are below\n",
    "type Response = {\n",
    "  id: string;\n",
    "  // Depending on whether you set \"stream\" to \"true\" and\n",
    "  // whether you passed in \"messages\" or a \"prompt\", you\n",
    "  // will get a different output shape\n",
    "  choices: (NonStreamingChoice | StreamingChoice | NonChatChoice)[];\n",
    "  created: number; // Unix timestamp\n",
    "  model: string;\n",
    "  object: 'chat.completion' | 'chat.completion.chunk';\n",
    "  system_fingerprint?: string; // Only present if the provider supports it\n",
    "  // Usage data is always returned for non-streaming.\n",
    "  // When streaming, you will get one usage object at\n",
    "  // the end accompanied by an empty choices array.\n",
    "  usage?: ResponseUsage;\n",
    "};\n",
    "// If the provider returns usage, we pass it down\n",
    "// as-is. Otherwise, we count using the GPT-4 tokenizer.\n",
    "type ResponseUsage = {\n",
    "  /** Including images and tools if any */\n",
    "  prompt_tokens: number;\n",
    "  /** The tokens generated */\n",
    "  completion_tokens: number;\n",
    "  /** Sum of the above two fields */\n",
    "  total_tokens: number;\n",
    "};\n",
    "\n",
    "\n",
    "// Subtypes:\n",
    "type NonChatChoice = {\n",
    "  finish_reason: string | null;\n",
    "  text: string;\n",
    "  error?: ErrorResponse;\n",
    "};\n",
    "type NonStreamingChoice = {\n",
    "  finish_reason: string | null;\n",
    "  native_finish_reason: string | null;\n",
    "  message: {\n",
    "    content: string | null;\n",
    "    role: string;\n",
    "    tool_calls?: ToolCall[];\n",
    "  };\n",
    "  error?: ErrorResponse;\n",
    "};\n",
    "type StreamingChoice = {\n",
    "  finish_reason: string | null;\n",
    "  native_finish_reason: string | null;\n",
    "  delta: {\n",
    "    content: string | null;\n",
    "    role?: string;\n",
    "    tool_calls?: ToolCall[];\n",
    "  };\n",
    "  error?: ErrorResponse;\n",
    "};\n",
    "type ErrorResponse = {\n",
    "  code: number; // See \"Error Handling\" section\n",
    "  message: string;\n",
    "  metadata?: Record<string, unknown>; // Contains additional error information such as provider details, the raw error message, etc.\n",
    "};\n",
    "type ToolCall = {\n",
    "  id: string;\n",
    "  type: 'function';\n",
    "  function: FunctionCall;\n",
    "};\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "807c09f2-ca4a-44e7-a861-e9a9be070a76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T22:16:37.174526Z",
     "iopub.status.busy": "2026-01-06T22:16:37.174304Z",
     "iopub.status.idle": "2026-01-06T22:16:37.180933Z",
     "shell.execute_reply": "2026-01-06T22:16:37.180145Z",
     "shell.execute_reply.started": "2026-01-06T22:16:37.174511Z"
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class OpenRouterModelClient(ModelClient):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str,\n",
    "        context_size: Optional[int] = None, # For OpenRouter this parameter is ignored, we inherit the remote model config\n",
    "        base_url: str = \"https://openrouter.ai/api/v1\",\n",
    "        api_key: Optional[str] = None, # If not provided, the mandatory key will be pulled from WordslabEnv\n",
    "    ):\n",
    "        super().__init__(model, base_url, api_key, context_size)\n",
    "\n",
    "        # Initialize API client\n",
    "        if not api_key:\n",
    "            env = WordslabEnv()\n",
    "            api_key = env.cloud_openrouter_api_key\n",
    "        self.client = OpenAI(base_url=base_url, api_key=api_key)\n",
    "        \n",
    "        # Check connection\n",
    "        print(f\"openrouter: testing model {self.model} ... \", end=\"\");\n",
    "        self.client.chat.completions.create(model=self.model, messages=[{'role': 'user', 'content': 'say yes'}], max_tokens=16)\n",
    "        print(f\"ok\");\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        messages: Sequence[Mapping[str, Any]],\n",
    "        chat_turns: ChatTurns,\n",
    "        tools: Tools = None,\n",
    "        think: Union[bool, Literal[\"xhigh\", \"high\", \"medium\", \"low\", \"minimal\", \"none\"], int, None] = None,\n",
    "        max_new_tokens: Optional[int] = None,\n",
    "        seed: Optional[int] = None,\n",
    "        temperature: Optional[float] = None,\n",
    "        top_k: Optional[int] = None,  # Ignored, not supported by the openai chat completions API\n",
    "        top_p: Optional[float] = None,\n",
    "        min_p: Optional[float] = None,  # Ignored, not supported by the openai chat completions API\n",
    "    ) -> bool:\n",
    "        # Check tools parameter type\n",
    "        if tools and not isinstance(tools, Tools):\n",
    "            raise TypeError(\"Argument tools must be of type wordslab_notebooks_lib.chat.Tools. Create a tools object with the syntax: Tools([func1, func2, func3]), where the parameters are documented python functions.\")\n",
    "        \n",
    "        # Immediate user feedback\n",
    "        print(f\"openrouter: processing {_messages_words(messages)} words with `{self.model}` ...\")\n",
    "        \n",
    "        # Observable conversation turn\n",
    "        chat_turn = chat_turns.new_turn()\n",
    "        \n",
    "        # Map \"think\" → reasoning_effort / max_tokens\n",
    "        reasoning = None\n",
    "        if think is True:\n",
    "            reasoning = {\"reasoning\": {\"enabled\": True}} # reasoning on/off\n",
    "        elif think in (\"low\", \"medium\", \"high\"):\n",
    "            reasoning = {\"reasoning\": {\"effort\": think}} # \"xhigh\", \"high\", \"medium\", \"low\", \"minimal\" or \"none\" (OpenAI-style)\n",
    "        elif isinstance(think, int):\n",
    "            reasoning = {\"reasoning\": {\"max_tokens\": think}} # specified token budget for extended thinking (Anthropic-style)\n",
    "        \n",
    "        stream = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            tools = tools.get_schemas() if tools else None,\n",
    "            stream=True,\n",
    "            extra_body= reasoning,\n",
    "            max_tokens=max_new_tokens,\n",
    "            seed=seed,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "        )\n",
    "    \n",
    "        # Streaming: accumulate the partial fields\n",
    "        tool_calls = {}       \n",
    "        for chunk in stream:\n",
    "            delta = chunk.choices[0].delta\n",
    "            if hasattr(delta, \"reasoning\") and delta.reasoning:\n",
    "                chat_turn.append_thinking(delta.reasoning)                \n",
    "            if hasattr(delta, \"content\") and delta.content:\n",
    "                chat_turn.append_content(delta.content)\n",
    "            if hasattr(delta, \"tool_calls\") and delta.tool_calls:\n",
    "                for tool_call in delta.tool_calls:\n",
    "                    idx = tool_call.index\n",
    "                    # First tool call chunk\n",
    "                    if idx not in tool_calls:\n",
    "                        tool_calls[idx] = {\n",
    "                            \"id\": tool_call.id,  # only present in first chunk\n",
    "                            \"name\": tool_call.function.name,\n",
    "                            \"arguments\": \"\"\n",
    "                        }       \n",
    "                    # Append streamed argument fragments\n",
    "                    tool_calls[idx][\"arguments\"] += (\n",
    "                        tool_call.function.arguments or \"\"\n",
    "                    )\n",
    "                    \n",
    "        # We need to wait the end of the stream to make sure the tool calls are complete\n",
    "        for tc in tool_calls.values():\n",
    "            chat_turn.append_tool_call(tc[\"name\"], tc[\"arguments\"])\n",
    "        \n",
    "        # Append accumulated fields to the messages\n",
    "        if chat_turn.content or tool_calls:\n",
    "            messages.append({\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": chat_turn.content,\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": tc[\"id\"],\n",
    "                        \"type\": \"function\",\n",
    "                        \"function\": {\n",
    "                            \"name\": tc[\"name\"],\n",
    "                            \"arguments\": tc[\"arguments\"]\n",
    "                        }\n",
    "                    }\n",
    "                    for tc in tool_calls.values()\n",
    "                ]\n",
    "            })\n",
    "    \n",
    "        # end the loop if there is no more tool calls\n",
    "        if not tool_calls: \n",
    "            return False      \n",
    "            \n",
    "        # execute tool calls  \n",
    "        else:    \n",
    "            for tc in tool_calls.values():\n",
    "                if tools.has_tool(tc[\"name\"]):\n",
    "                    chat_turn.start_tool_call(tc[\"name\"])\n",
    "                    result = tools.call(tc[\"name\"], json.loads(tc[\"arguments\"]))\n",
    "                    chat_turn.end_tool_call(tc[\"name\"], result)\n",
    "                else:\n",
    "                    result = 'Unknown tool'\n",
    "        \n",
    "                # append tool call result to the messages \n",
    "                messages.append({\"role\": \"tool\", \"tool_call_id\": tc[\"id\"], \"content\": str(result)})\n",
    "\n",
    "        # continue the loop after tool calls\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "6b9ca97e-bc5b-4d9a-a1cb-6504aa8596ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T22:16:38.585685Z",
     "iopub.status.busy": "2026-01-06T22:16:38.585563Z",
     "iopub.status.idle": "2026-01-06T22:16:40.711290Z",
     "shell.execute_reply": "2026-01-06T22:16:40.710511Z",
     "shell.execute_reply.started": "2026-01-06T22:16:38.585675Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openrouter: testing model anthropic/claude-sonnet-4.5 ... ok\n"
     ]
    }
   ],
   "source": [
    "model = \"anthropic/claude-sonnet-4.5\"\n",
    "orclient = OpenRouterModelClient(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "e6571223-c99d-4a31-8e61-6be1a51a7ce9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T22:16:41.589080Z",
     "iopub.status.busy": "2026-01-06T22:16:41.588957Z",
     "iopub.status.idle": "2026-01-06T22:16:44.168767Z",
     "shell.execute_reply": "2026-01-06T22:16:44.168065Z",
     "shell.execute_reply.started": "2026-01-06T22:16:41.589069Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I need to find the smallest palindrome greater than 130.\n",
       "\n",
       "Let me check numbers starting from 131:\n",
       "\n",
       "- 131: Is this a palindrome? 1-3-1 → Yes, it reads the same forwards and backwards.\n",
       "\n",
       "Therefore, the smallest palindrome greater than 130 is **131**.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = [{'role': 'user', 'content': 'What is the smallest number palindrome greater than 130?'}]\n",
    "turns = ChatTurns()\n",
    "tool_calls_to_process = orclient(messages, turns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "3e01f5da-6f15-40eb-a33f-fcb2579370a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T22:17:41.314329Z",
     "iopub.status.busy": "2026-01-06T22:17:41.314247Z",
     "iopub.status.idle": "2026-01-06T22:17:44.321561Z",
     "shell.execute_reply": "2026-01-06T22:17:44.320111Z",
     "shell.execute_reply.started": "2026-01-06T22:17:41.314323Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> [Thinking] ... thought in 53 words\n",
       "\n",
       "The smallest palindrome greater than 130 is **131**.\n",
       "\n",
       "A palindrome reads the same forwards and backwards, and 131 satisfies this condition.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = [{'role': 'user', 'content': 'What is the smallest number palindrome greater than 130?'}]\n",
    "turns = ChatTurns()\n",
    "tool_calls_to_process = orclient(messages, turns, think=1024, max_new_tokens=2000, seed=42, temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "90e1be7d-970e-4964-a01b-b73de4cf6f28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T22:18:15.474865Z",
     "iopub.status.busy": "2026-01-06T22:18:15.474742Z",
     "iopub.status.idle": "2026-01-06T22:18:18.685154Z",
     "shell.execute_reply": "2026-01-06T22:18:18.684739Z",
     "shell.execute_reply.started": "2026-01-06T22:18:15.474854Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> [Thinking] ... thought in 40 words\n",
       "\n",
       "I'll solve this step by step using the provided tools.\n",
       "\n",
       "First, let me add 11545468 + 78782431:\n",
       "\n",
       "> [Tool call] ... `add` returned `90327899`\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = [{'role': 'user', 'content': \"Using only the provided tools to make no mistake, what is (11545468+78782431)*418742?\"}]\n",
    "turns = ChatTurns()\n",
    "tools = Tools([add, multiply])\n",
    "tool_calls_to_process = orclient(messages, turns, tools=tools, think=2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "b18ffd4c-db73-48bc-9f49-8fdfe8bc32c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T22:18:23.222350Z",
     "iopub.status.busy": "2026-01-06T22:18:23.222093Z",
     "iopub.status.idle": "2026-01-06T22:18:23.226129Z",
     "shell.execute_reply": "2026-01-06T22:18:23.225579Z",
     "shell.execute_reply.started": "2026-01-06T22:18:23.222332Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_calls_to_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "4473032c-9f31-43c4-9431-acfccb40cbd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T22:18:23.522674Z",
     "iopub.status.busy": "2026-01-06T22:18:23.522559Z",
     "iopub.status.idle": "2026-01-06T22:18:23.525350Z",
     "shell.execute_reply": "2026-01-06T22:18:23.524905Z",
     "shell.execute_reply.started": "2026-01-06T22:18:23.522665Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'Using only the provided tools to make no mistake, what is (11545468+78782431)*418742?'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"I'll solve this step by step using the provided tools.\\n\\nFirst, let me add 11545468 + 78782431:\",\n",
       "  'tool_calls': [{'id': 'toolu_vrtx_01Apanb8oW6scZnYaCXhAKQ1',\n",
       "    'type': 'function',\n",
       "    'function': {'name': 'add',\n",
       "     'arguments': '{\"a\": 11545468, \"b\": 78782431}'}}]},\n",
       " {'role': 'tool',\n",
       "  'tool_call_id': 'toolu_vrtx_01Apanb8oW6scZnYaCXhAKQ1',\n",
       "  'content': '90327899'}]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "b1972808-70da-465b-b03b-c46651a86a37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T22:18:36.207694Z",
     "iopub.status.busy": "2026-01-06T22:18:36.207615Z",
     "iopub.status.idle": "2026-01-06T22:18:38.251409Z",
     "shell.execute_reply": "2026-01-06T22:18:38.250836Z",
     "shell.execute_reply.started": "2026-01-06T22:18:36.207688Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> [Thinking] ... thought in 40 words\n",
       "\n",
       "I'll solve this step by step using the provided tools.\n",
       "\n",
       "First, let me add 11545468 + 78782431:\n",
       "\n",
       "> [Tool call] ... `add` returned `90327899`\n",
       "\n",
       "Now let me multiply that result by 418742:\n",
       "\n",
       "> [Tool call] ... `multiply` returned `37824085083058`\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tool_calls_to_process = orclient(messages, turns, tools=tools, think=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "a386ad9e-7162-4ed3-b9e0-2aef00218288",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T22:18:46.860607Z",
     "iopub.status.busy": "2026-01-06T22:18:46.860502Z",
     "iopub.status.idle": "2026-01-06T22:18:46.864608Z",
     "shell.execute_reply": "2026-01-06T22:18:46.864210Z",
     "shell.execute_reply.started": "2026-01-06T22:18:46.860600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_calls_to_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "22743ba4-fe49-45ba-a5b9-b7eb3e3bba51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T22:18:47.562318Z",
     "iopub.status.busy": "2026-01-06T22:18:47.562233Z",
     "iopub.status.idle": "2026-01-06T22:18:49.601554Z",
     "shell.execute_reply": "2026-01-06T22:18:49.601075Z",
     "shell.execute_reply.started": "2026-01-06T22:18:47.562311Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> [Thinking] ... thought in 40 words\n",
       "\n",
       "I'll solve this step by step using the provided tools.\n",
       "\n",
       "First, let me add 11545468 + 78782431:\n",
       "\n",
       "> [Tool call] ... `add` returned `90327899`\n",
       "\n",
       "Now let me multiply that result by 418742:\n",
       "\n",
       "> [Tool call] ... `multiply` returned `37824085083058`\n",
       "\n",
       "The answer is **(11545468 + 78782431) × 418742 = 37,824,085,083,058**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tool_calls_to_process = orclient(messages, turns, tools=tools, think=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d1b005-b6ab-4713-a30c-2d510652ff7b",
   "metadata": {},
   "source": [
    "## Models providers\n",
    "\n",
    "### Design concepts\n",
    "\n",
    "#### User centric workflow\n",
    "\n",
    "1. identify your self-hosted inference or inference as a service options\n",
    "2. understand your task type, properties, privacy needs and scale\n",
    "3. find the best model for your task, given your constraints\n",
    "4. prepare and start your self hosted inference or connect to your inference as a service provider\n",
    "5. monitor your resource usage and cost\n",
    "\n",
    "#### Self-hosted inference or inference as a service\n",
    "\n",
    "Model families\n",
    "- architecture name\n",
    "- parameter size\n",
    "- training type: base / instruct / thinking\n",
    "- version: relase date\n",
    "- quantization\n",
    "\n",
    "Model constraints\n",
    "- model capabilities\n",
    "  - modalities in/out\n",
    "  - context length\n",
    "  - instruction\n",
    "  - thinking\n",
    "  - tools\n",
    "- model usage\n",
    "  - prompt template and special tokens\n",
    "  - languages supported\n",
    "  - recommended use cases\n",
    "  - prompting guidelines \n",
    "- model license\n",
    "  - use case restrictions\n",
    "  - commercial usage restrictions\n",
    "  - outputs usage restrictions \n",
    "- model transparency\n",
    "\n",
    "Self-hosted inference constraints\n",
    "- model requirements\n",
    "  - size on disk -> download time / load time in vram\n",
    "  - size in vram -> max context length / num parallel sequence\n",
    "  - tensor flops -> input tokens/sec\n",
    "  - memory bandwidth -> output tokens/sec\n",
    "- inference machine constraints\n",
    "  - download speed\n",
    "  - disk size and speed\n",
    "  - GPU vram, memory bandwidth, tensor flops\n",
    "- rented machine constraints\n",
    "  - GPU availability\n",
    "  - price when you use per GPU\n",
    "  - price when you don't use per GB (storage)\n",
    "\n",
    "Inference as a service constraints\n",
    "- router constraints\n",
    "  - ... same as provider constraints below ... \n",
    "- provider constraints\n",
    "  - terms of service\n",
    "  - privacy options\n",
    "  - inference quotas\n",
    "  - service availability\n",
    "- per model provider constraints\n",
    "  - model capabilities exposed \n",
    "  - input/output tokens cost\n",
    "  - input/output tokens/sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc3cc74-8fd8-4899-ae78-904b72159d29",
   "metadata": {},
   "source": [
    "### List, download and load models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0f44df-1f37-49c9-912e-7fbe4fd33cf0",
   "metadata": {},
   "source": [
    "#### Explore ollama API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798c7ec5-e1d2-463c-a892-2c2a7f3dc814",
   "metadata": {},
   "source": [
    "Get ollama version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143e8e1d-5041-4115-9e66-40757b9f1a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Request\n",
    "curl http://localhost:11434/api/version\n",
    "Response\n",
    "{\n",
    "  \"version\": \"0.5.1\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5cfbc9-2b9a-4e18-b564-7e4d2a775d0b",
   "metadata": {},
   "source": [
    "List remote models\n",
    "\n",
    "As of december 2025, there is no API to get the ollama catalog of models, web scraping is the only solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e2ead5-6c44-4aae-8620-6119163bd102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import re\n",
    "from html import unescape\n",
    "\n",
    "def updated_to_months(updated):\n",
    "    \"\"\"\n",
    "    Convert strings like:\n",
    "      \"1 year ago\", \"2 years ago\",\n",
    "      \"1 month ago\", \"3 weeks ago\",\n",
    "      \"7 days ago\", \"yesterday\",\n",
    "      \"4 hours ago\"\n",
    "    into integer months.\n",
    "    \"\"\"\n",
    "    if not updated:\n",
    "        return None\n",
    "\n",
    "    updated = updated.lower().strip()\n",
    "\n",
    "    # handle 'yesterday' explicitly\n",
    "    if updated == \"yesterday\":\n",
    "        return 0\n",
    "\n",
    "    # years → months\n",
    "    m = re.match(r'(\\d+)\\s+year', updated)\n",
    "    if m:\n",
    "        years = int(m.group(1))\n",
    "        return years * 12\n",
    "\n",
    "    # months\n",
    "    m = re.match(r'(\\d+)\\s+month', updated)\n",
    "    if m:\n",
    "        return int(m.group(1))\n",
    "\n",
    "    # weeks\n",
    "    m = re.match(r'(\\d+)\\s+week', updated)\n",
    "    if m:\n",
    "        weeks = int(m.group(1))\n",
    "        return max(0, weeks // 4)\n",
    "\n",
    "    # days\n",
    "    m = re.match(r'(\\d+)\\s+day', updated)\n",
    "    if m:\n",
    "        return 0\n",
    "\n",
    "    # hours / minutes / seconds → treat as < 1 month\n",
    "    if any(unit in updated for unit in [\"hour\", \"minute\", \"second\"]):\n",
    "        return 0\n",
    "\n",
    "    return None\n",
    "\n",
    "def pulls_to_int(pulls_str):\n",
    "    \"\"\"\n",
    "    Convert a pulls string like:\n",
    "        '5M', '655.8K', '49K', '73.7M', '957.4K', '27.7M'\n",
    "    into an integer.\n",
    "    \"\"\"\n",
    "    if not pulls_str:\n",
    "        return None\n",
    "\n",
    "    pulls_str = pulls_str.strip().upper()\n",
    "\n",
    "    match = re.match(r'([\\d,.]+)\\s*([KM]?)', pulls_str)\n",
    "    if not match:\n",
    "        return None\n",
    "\n",
    "    number, suffix = match.groups()\n",
    "    # Remove commas and convert to float\n",
    "    number = float(number.replace(',', ''))\n",
    "\n",
    "    if suffix == 'M':\n",
    "        number *= 1_000_000\n",
    "    elif suffix == 'K':\n",
    "        number *= 1_000\n",
    "\n",
    "    return int(number)\n",
    "\n",
    "def parse_model_list_regex(html):\n",
    "    models = []\n",
    "\n",
    "    # --- Extract each <li x-test-model>...</li> block ---\n",
    "    li_blocks = re.findall(\n",
    "        r'<li[^>]*x-test-model[^>]*>(.*?)</li>',\n",
    "        html,\n",
    "        flags=re.DOTALL\n",
    "    )\n",
    "\n",
    "    for block in li_blocks:\n",
    "\n",
    "        # name from <a href=\"/library/...\">\n",
    "        name = None\n",
    "        m = re.search(r'href=\"/library/([^\"]+)\"', block)\n",
    "        if m:\n",
    "            name = m.group(1)\n",
    "\n",
    "        # description <p class=\"max-w-lg ...\">...</p>\n",
    "        description = \"\"\n",
    "        m = re.search(\n",
    "            r'<p[^>]*text-neutral-800[^>]*>(.*?)</p>',\n",
    "            block,\n",
    "            flags=re.DOTALL\n",
    "        )\n",
    "        if m:\n",
    "            description = re.sub(r'<.*?>', '', m.group(1)).strip()\n",
    "            description = unescape(description)\n",
    "\n",
    "        # capabilities (x-test-capability)\n",
    "        capabilities = re.findall(\n",
    "            r'<span[^>]*x-test-capability[^>]*>(.*?)</span>',\n",
    "            block,\n",
    "            flags=re.DOTALL\n",
    "        )\n",
    "        capabilities = [c.strip() for c in capabilities]\n",
    "\n",
    "        # check for the special 'cloud' span \n",
    "        cloud = False\n",
    "        if re.search(\n",
    "            r'<span[^>]*>cloud</span>',\n",
    "            block,\n",
    "            flags=re.DOTALL\n",
    "        ):\n",
    "            cloud = True\n",
    "\n",
    "        # sizes (x-test-size)\n",
    "        sizes = re.findall(\n",
    "            r'<span[^>]*x-test-size[^>]*>(.*?)</span>',\n",
    "            block,\n",
    "            flags=re.DOTALL\n",
    "        )\n",
    "        sizes = [s.strip() for s in sizes]\n",
    "\n",
    "        # pulls <span x-test-pull-count>5M</span>\n",
    "        pulls = None\n",
    "        m = re.search(\n",
    "            r'<span[^>]*x-test-pull-count[^>]*>(.*?)</span>',\n",
    "            block\n",
    "        )\n",
    "        if m:\n",
    "            pulls = m.group(1).strip()\n",
    "\n",
    "        # tag count <span x-test-tag-count>5</span>\n",
    "        tag_count = None\n",
    "        m = re.search(\n",
    "            r'<span[^>]*x-test-tag-count[^>]*>(.*?)</span>',\n",
    "            block\n",
    "        )\n",
    "        if m:\n",
    "            tag_count = m.group(1).strip()\n",
    "\n",
    "        # updated text <span x-test-updated>...</span>\n",
    "        updated = None\n",
    "        m = re.search(\n",
    "            r'<span[^>]*x-test-updated[^>]*>(.*?)</span>',\n",
    "            block\n",
    "        )\n",
    "        if m:\n",
    "            updated = m.group(1).strip()\n",
    "\n",
    "        models.append({\n",
    "            \"name\": name,\n",
    "            \"description\": description,\n",
    "            \"capabilities\": capabilities,\n",
    "            \"cloud\": cloud,\n",
    "            \"sizes\": sizes,\n",
    "            \"pulls\": pulls_to_int(pulls),\n",
    "            \"tag_count\": int(tag_count),\n",
    "            \"updated_months\": updated_to_months(updated),\n",
    "            \"url\": f\"https://ollama.com/library/{name}\" if name else None\n",
    "        })\n",
    "\n",
    "    return models   \n",
    "\n",
    "def list_models(contains=None):\n",
    "    \"\"\"\n",
    "    Extract model names and properties from https://ollama.com/library\n",
    "    Optionally filter by substring.\n",
    "    \"\"\"\n",
    "\n",
    "    html = httpx.get(\"https://ollama.com/library\").text\n",
    "    models = parse_model_list_regex(html)\n",
    "\n",
    "    if contains:\n",
    "        models = [\n",
    "            m for m in models\n",
    "            if contains.lower() in m[\"name\"].lower()\n",
    "        ]\n",
    "        models = sorted(models, key=lambda m:m[\"name\"])\n",
    "\n",
    "    return models\n",
    "\n",
    "def list_recent_models_from_family(familyfilter):\n",
    "    return [f\"{m['name']} {m['capabilities'] if len(m['capabilities'])>0 else ''} {m['sizes'] if len(m['sizes'])>0 else ''}{' [cloud]' if m['cloud'] else ''}\" for m in list_models(familyfilter) if m[\"updated_months\"] is not None and m[\"updated_months\"]<12]\n",
    "\n",
    "def list_tags(model):\n",
    "    \"\"\"\n",
    "    Extract valid quantized tags only, without HTML noise,\n",
    "    and apply the same exclusions as original greps.\n",
    "    \"\"\"\n",
    "    html = httpx.get(f\"https://ollama.com/library/{model}/tags\").text\n",
    "\n",
    "    # Capture ONLY the tag part after model:..., e.g. 3b-instruct-q4_K_M\n",
    "    raw_tags = re.findall(\n",
    "        rf'{re.escape(model)}:([A-Za-z0-9._-]*q[A-Za-z0-9._-]*)',\n",
    "        html\n",
    "    )\n",
    "\n",
    "    # Re-add full prefix model:<tag>\n",
    "    tags = [f\"{model}:{t}\" for t in raw_tags]\n",
    "\n",
    "    # Exclude text|base|fp|q4_[01]|q5_[01]\n",
    "    tags = [\n",
    "        t for t in tags\n",
    "        if not re.search(r'(text|base|fp|q[45]_[01])', t)\n",
    "    ]\n",
    "\n",
    "    # Deduplicate\n",
    "    return set(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392377be-d847-46b9-9658-430ee348aba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'gpt-oss',\n",
       "  'description': 'OpenAI’s open-weight models designed for powerful reasoning, agentic tasks, and versatile developer use cases.',\n",
       "  'capabilities': ['tools', 'thinking'],\n",
       "  'cloud': True,\n",
       "  'sizes': ['20b', '120b'],\n",
       "  'pulls': 5000000,\n",
       "  'tag_count': 5,\n",
       "  'updated_months': 1,\n",
       "  'url': 'https://ollama.com/library/gpt-oss'},\n",
       " {'name': 'qwen3-vl',\n",
       "  'description': 'The most powerful vision-language model in the Qwen model family to date.',\n",
       "  'capabilities': ['vision', 'tools'],\n",
       "  'cloud': True,\n",
       "  'sizes': ['2b', '4b', '8b', '30b', '32b', '235b'],\n",
       "  'pulls': 656300,\n",
       "  'tag_count': 59,\n",
       "  'updated_months': 1,\n",
       "  'url': 'https://ollama.com/library/qwen3-vl'},\n",
       " {'name': 'ministral-3',\n",
       "  'description': 'The Ministral 3 family is designed for edge deployment, capable of running on a wide range of hardware.',\n",
       "  'capabilities': ['vision', 'tools'],\n",
       "  'cloud': True,\n",
       "  'sizes': ['3b', '8b', '14b'],\n",
       "  'pulls': 49100,\n",
       "  'tag_count': 16,\n",
       "  'updated_months': 0,\n",
       "  'url': 'https://ollama.com/library/ministral-3'},\n",
       " {'name': 'deepseek-r1',\n",
       "  'description': 'DeepSeek-R1 is a family of open reasoning models with performance approaching that of leading models, such as O3 and Gemini 2.5 Pro.',\n",
       "  'capabilities': ['tools', 'thinking'],\n",
       "  'cloud': False,\n",
       "  'sizes': ['1.5b', '7b', '8b', '14b', '32b', '70b', '671b'],\n",
       "  'pulls': 73700000,\n",
       "  'tag_count': 35,\n",
       "  'updated_months': 5,\n",
       "  'url': 'https://ollama.com/library/deepseek-r1'},\n",
       " {'name': 'qwen3-coder',\n",
       "  'description': \"Alibaba's performant long context models for agentic and coding tasks.\",\n",
       "  'capabilities': ['tools'],\n",
       "  'cloud': True,\n",
       "  'sizes': ['30b', '480b'],\n",
       "  'pulls': 958100,\n",
       "  'tag_count': 10,\n",
       "  'updated_months': 2,\n",
       "  'url': 'https://ollama.com/library/qwen3-coder'}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_models()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a513f8cf-0679-44c0-bdb4-4fca3aaf1a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"qwen2.5-coder ['tools'] ['0.5b', '1.5b', '3b', '7b', '14b', '32b']\",\n",
       " \"qwen2.5vl ['vision'] ['3b', '7b', '32b', '72b']\",\n",
       " \"qwen3 ['tools', 'thinking'] ['0.6b', '1.7b', '4b', '8b', '14b', '30b', '32b', '235b']\",\n",
       " \"qwen3-coder ['tools'] ['30b', '480b'] [cloud]\",\n",
       " \"qwen3-embedding ['embedding'] ['0.6b', '4b', '8b']\",\n",
       " \"qwen3-vl ['vision', 'tools'] ['2b', '4b', '8b', '30b', '32b', '235b'] [cloud]\"]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_recent_models_from_family(\"qwen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f04212a-ad4a-4811-a8f9-833dee73e09d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"embeddinggemma ['embedding'] ['300m']\",\n",
       " \"gemma3 ['vision'] ['270m', '1b', '4b', '12b', '27b'] [cloud]\",\n",
       " \"gemma3n  ['e2b', 'e4b']\"]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_recent_models_from_family(\"gemma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3cae3b-d873-468d-9d80-8058b785ab8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"devstral ['tools'] ['24b']\",\n",
       " \"magistral ['tools', 'thinking'] ['24b']\",\n",
       " \"ministral-3 ['vision', 'tools'] ['3b', '8b', '14b'] [cloud]\",\n",
       " \"mistral ['tools'] ['7b']\",\n",
       " 'mistral-large-3   [cloud]',\n",
       " \"mistral-nemo ['tools'] ['12b']\",\n",
       " \"mistral-small ['tools'] ['22b', '24b']\",\n",
       " \"mistral-small3.1 ['vision', 'tools'] ['24b']\",\n",
       " \"mistral-small3.2 ['vision', 'tools'] ['24b']\"]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_recent_models_from_family(\"stral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e33d36-4e00-4d8c-9ce1-d6a0c9eafa84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"gpt-oss ['tools', 'thinking'] ['20b', '120b'] [cloud]\",\n",
       " \"gpt-oss-safeguard ['tools', 'thinking'] ['20b', '120b']\"]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_recent_models_from_family(\"gpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524e9f87-1b3d-4190-9f56-dfbcff10bf15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"deepseek-ocr ['vision'] ['3b']\",\n",
       " \"deepseek-r1 ['tools', 'thinking'] ['1.5b', '7b', '8b', '14b', '32b', '70b', '671b']\",\n",
       " \"deepseek-v3  ['671b']\",\n",
       " \"deepseek-v3.1 ['tools', 'thinking'] ['671b'] [cloud]\"]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_recent_models_from_family(\"deepseek\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e455be-b1ae-4e12-bc16-d72ea08ea494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['glm-4.6   [cloud]']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_recent_models_from_family(\"glm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e6dd40-3fc6-4feb-b2a2-209865434abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"granite-embedding ['embedding'] ['30m', '278m']\",\n",
       " \"granite3.1-dense ['tools'] ['2b', '8b']\",\n",
       " \"granite3.1-moe ['tools'] ['1b', '3b']\",\n",
       " \"granite3.2 ['tools'] ['2b', '8b']\",\n",
       " \"granite3.2-vision ['vision', 'tools'] ['2b']\",\n",
       " \"granite3.3 ['tools'] ['2b', '8b']\",\n",
       " \"granite4 ['tools'] ['350m', '1b', '3b']\"]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_recent_models_from_family(\"granite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71f35d1-2ae8-4564-9f9c-17ca98e378f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"llama3.2-vision ['vision'] ['11b', '90b']\",\n",
       " \"llama4 ['vision', 'tools'] ['16x17b', '128x17b']\"]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_recent_models_from_family(\"llama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7893a9-afa2-4fb1-bfd0-9b5936e5eea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"dolphin-mixtral  ['8x7b', '8x22b']\",\n",
       " \"dolphin3  ['8b']\",\n",
       " \"phi4  ['14b']\",\n",
       " \"phi4-mini ['tools'] ['3.8b']\",\n",
       " \"phi4-mini-reasoning  ['3.8b']\",\n",
       " \"phi4-reasoning  ['14b']\"]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_recent_models_from_family(\"phi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bdadec-2b71-40a7-be49-31fcf9e7a227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"hermes3 ['tools'] ['3b', '8b', '70b', '405b']\",\n",
       " \"nous-hermes2-mixtral  ['8x7b']\"]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_recent_models_from_family(\"hermes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88de106f-16a6-49f8-bacb-fdae0515e3e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"olmo2  ['7b', '13b']\"]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_recent_models_from_family(\"olmo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee669d8-b586-4e7b-aa37-cf65489fabcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"embeddinggemma ['embedding'] ['300m']\",\n",
       " \"granite-embedding ['embedding'] ['30m', '278m']\",\n",
       " \"qwen3-embedding ['embedding'] ['0.6b', '4b', '8b']\"]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_recent_models_from_family(\"embed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c24bc81-f6d7-433e-b3a8-53e70f914442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ministral-3:14b-instruct-2512-q4_K_M',\n",
       " 'ministral-3:14b-instruct-2512-q8_0',\n",
       " 'ministral-3:3b-instruct-2512-q4_K_M',\n",
       " 'ministral-3:3b-instruct-2512-q8_0',\n",
       " 'ministral-3:8b-instruct-2512-q4_K_M',\n",
       " 'ministral-3:8b-instruct-2512-q8_0'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_tags(\"ministral-3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd1969b-2832-47f9-8569-add48c966f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mistral-small3.2:24b-instruct-2506-q4_K_M',\n",
       " 'mistral-small3.2:24b-instruct-2506-q8_0'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_tags(\"mistral-small3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81100df9-ed00-4bd4-afd6-96c8ddf9f1d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qwen3-vl:235b-a22b-instruct-q4_K_M',\n",
       " 'qwen3-vl:235b-a22b-instruct-q8_0',\n",
       " 'qwen3-vl:235b-a22b-thinking-q4_K_M',\n",
       " 'qwen3-vl:235b-a22b-thinking-q8_0',\n",
       " 'qwen3-vl:2b-instruct-q4_K_M',\n",
       " 'qwen3-vl:2b-instruct-q8_0',\n",
       " 'qwen3-vl:2b-thinking-q4_K_M',\n",
       " 'qwen3-vl:2b-thinking-q8_0',\n",
       " 'qwen3-vl:30b-a3b-instruct-q4_K_M',\n",
       " 'qwen3-vl:30b-a3b-instruct-q8_0',\n",
       " 'qwen3-vl:30b-a3b-thinking-q4_K_M',\n",
       " 'qwen3-vl:30b-a3b-thinking-q8_0',\n",
       " 'qwen3-vl:32b-instruct-q4_K_M',\n",
       " 'qwen3-vl:32b-instruct-q8_0',\n",
       " 'qwen3-vl:32b-thinking-q4_K_M',\n",
       " 'qwen3-vl:32b-thinking-q8_0',\n",
       " 'qwen3-vl:4b-instruct-q4_K_M',\n",
       " 'qwen3-vl:4b-instruct-q8_0',\n",
       " 'qwen3-vl:4b-thinking-q4_K_M',\n",
       " 'qwen3-vl:4b-thinking-q8_0',\n",
       " 'qwen3-vl:8b-instruct-q4_K_M',\n",
       " 'qwen3-vl:8b-instruct-q8_0',\n",
       " 'qwen3-vl:8b-thinking-q4_K_M',\n",
       " 'qwen3-vl:8b-thinking-q8_0'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_tags(\"qwen3-vl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48681ee4-9ab4-4858-bc08-3b1970567acd",
   "metadata": {},
   "source": [
    "https://github.com/ollama/ollama/blob/main/docs/api.md#list-local-models\n",
    "\n",
    "ollama.list().models -> list(ollama._types.ListResponse.Model)\n",
    "\n",
    "```yaml\n",
    "ollama._types.ListResponse.Model\n",
    "- model: str 'qwen3:4b'\n",
    "- modified_at: datetime.datetime datetime(2025, 11, 22, 18, 53, 11)\n",
    "- digest: str '359d7dd4bcdab3d86b87d73ac27966f4dbb9f5efdfcc75d34a8764a09474fae7'\n",
    "- size: pydantic.types.ByteSize 2497293931\n",
    "- details: ollama._types.ModelDetails\n",
    "  - parent_model: str ''\n",
    "  - format: str 'gguf'\n",
    "  - family: str 'qwen3'\n",
    "  - families: Sequence[str] ['qwen3']\n",
    "  - parameter_size: str '4.0B'\n",
    "  - quantization_level: str 'Q4_K_M'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1465604-236a-4e4d-b149-acd1e27d6721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(model='qwen3:4b', modified_at=datetime.datetime(2025, 11, 22, 18, 53, 11, 586211, tzinfo=TzInfo(3600)), digest='359d7dd4bcdab3d86b87d73ac27966f4dbb9f5efdfcc75d34a8764a09474fae7', size=2497293931, details=ModelDetails(parent_model='', format='gguf', family='qwen3', families=['qwen3'], parameter_size='4.0B', quantization_level='Q4_K_M'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.list().models[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b470e97c-1a8d-464b-8b03-09898457d198",
   "metadata": {},
   "source": [
    "https://github.com/ollama/ollama/blob/main/docs/api.md#show-model-information\n",
    "\n",
    "```yaml\n",
    "ollama._types.ShowResponse\n",
    "- modified_at: datetime.datetime datetime.datetime(2025, 11, 22, 18, 53, 11)\n",
    "- template: str '{{- $lastUserIdx := -1 -}}...\\n{{- end }}'\n",
    "- modelfile: str '...'\n",
    "- license: str '...'\n",
    "- details: ollama._types.ModelDetails -> see above\n",
    "- model_info: Mapping[str, Any]\n",
    "  -'general.architecture': 'qwen3'\n",
    "  -'general.basename': 'Qwen3' \n",
    "  -'general.file_type': 15\n",
    "  -'general.finetune': 'Thinking' \n",
    "  -'general.license': 'apache-2.0'\n",
    "  -'general.license.link': 'https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507/blob/main/LICENSE'\n",
    "  -'general.parameter_count': 4022468096\n",
    "  -'general.quantization_version': 2, \n",
    "  -'general.size_label': '4B'\n",
    "  -'general.tags': None\n",
    "  -'general.type': 'model'\n",
    "  -'general.version': '2507'\n",
    "  -'qwen3.attention.head_count': 32\n",
    "  -'qwen3.attention.head_count_kv': 8\n",
    "  -'qwen3.attention.key_length': 128\n",
    "  -'qwen3.attention.layer_norm_rms_epsilon': 1e-06\n",
    "  -'qwen3.attention.value_length': 128\n",
    "  -'qwen3.block_count': 36\n",
    "  -'qwen3.context_length': 262144\n",
    "  -'qwen3.embedding_length': 2560\n",
    "  -'qwen3.feed_forward_length': 9728\n",
    "  -'qwen3.rope.freq_base': 5000000\n",
    "  -'tokenizer.ggml.add_bos_token': False\n",
    "  -'tokenizer.ggml.bos_token_id': 151643\n",
    "  -'tokenizer.ggml.eos_token_id': 151645\n",
    "  -'tokenizer.ggml.merges': None\n",
    "  -'tokenizer.ggml.model': 'gpt2'\n",
    "  -'tokenizer.ggml.padding_token_id': 151643\n",
    "  -'tokenizer.ggml.pre': 'qwen2'\n",
    "  -'tokenizer.ggml.token_type': None\n",
    "  -'tokenizer.ggml.tokens': None\n",
    "- parameters: str 'top_p 0.95\\n repeat_penalty 1\\n stop \"<|im_start|>\"\\n stop \"<|im_end|>\"\\n temperature 0.6\\ n top_k 20'\n",
    "- capabilities: List[str] ['completion', 'tools', 'thinking']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c73dfb-fabd-4ee1-9fba-7a24b74cfdde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['completion', 'vision'],\n",
       " {'gemma3.attention.head_count': 8,\n",
       "  'gemma3.attention.head_count_kv': 4,\n",
       "  'gemma3.attention.key_length': 256,\n",
       "  'gemma3.attention.sliding_window': 1024,\n",
       "  'gemma3.attention.value_length': 256,\n",
       "  'gemma3.block_count': 34,\n",
       "  'gemma3.context_length': 131072,\n",
       "  'gemma3.embedding_length': 2560,\n",
       "  'gemma3.feed_forward_length': 10240,\n",
       "  'gemma3.mm.tokens_per_image': 256,\n",
       "  'gemma3.vision.attention.head_count': 16,\n",
       "  'gemma3.vision.attention.layer_norm_epsilon': 1e-06,\n",
       "  'gemma3.vision.block_count': 27,\n",
       "  'gemma3.vision.embedding_length': 1152,\n",
       "  'gemma3.vision.feed_forward_length': 4304,\n",
       "  'gemma3.vision.image_size': 896,\n",
       "  'gemma3.vision.num_channels': 3,\n",
       "  'gemma3.vision.patch_size': 14,\n",
       "  'general.architecture': 'gemma3',\n",
       "  'general.file_type': 15,\n",
       "  'general.parameter_count': 4299915632,\n",
       "  'general.quantization_version': 2,\n",
       "  'tokenizer.ggml.add_bos_token': True,\n",
       "  'tokenizer.ggml.add_eos_token': False,\n",
       "  'tokenizer.ggml.add_padding_token': False,\n",
       "  'tokenizer.ggml.add_unknown_token': False,\n",
       "  'tokenizer.ggml.bos_token_id': 2,\n",
       "  'tokenizer.ggml.eos_token_id': 1,\n",
       "  'tokenizer.ggml.merges': None,\n",
       "  'tokenizer.ggml.model': 'llama',\n",
       "  'tokenizer.ggml.padding_token_id': 0,\n",
       "  'tokenizer.ggml.pre': 'default',\n",
       "  'tokenizer.ggml.scores': None,\n",
       "  'tokenizer.ggml.token_type': None,\n",
       "  'tokenizer.ggml.tokens': None,\n",
       "  'tokenizer.ggml.unknown_token_id': 3})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.show('gemma3:4b').capabilities, ollama.show('gemma3:4b').modelinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efffa6e8-2816-4c1c-a051-dc29b5bc477c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mSignature:\u001b[39m ollama.pull(model: str, *, insecure: bool = \u001b[38;5;28;01mFalse\u001b[39;00m, stream: bool = \u001b[38;5;28;01mFalse\u001b[39;00m) -> Union[ollama._types.ProgressResponse, collections.abc.Iterator[ollama._types.ProgressResponse]]\n",
       "\u001b[31mSource:\u001b[39m   \n",
       "  \u001b[38;5;28;01mdef\u001b[39;00m pull(\n",
       "    self,\n",
       "    model: str,\n",
       "    *,\n",
       "    insecure: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    stream: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "  ) -> Union[ProgressResponse, Iterator[ProgressResponse]]:\n",
       "    \u001b[33m\"\"\"\u001b[39m\n",
       "\u001b[33m    Raises `ResponseError` if the request could not be fulfilled.\u001b[39m\n",
       "\n",
       "\u001b[33m    Returns `ProgressResponse` if `stream` is `False`, otherwise returns a `ProgressResponse` generator.\u001b[39m\n",
       "\u001b[33m    \"\"\"\u001b[39m\n",
       "    \u001b[38;5;28;01mreturn\u001b[39;00m self._request(\n",
       "      ProgressResponse,\n",
       "      \u001b[33m'POST'\u001b[39m,\n",
       "      \u001b[33m'/api/pull'\u001b[39m,\n",
       "      json=PullRequest(\n",
       "        model=model,\n",
       "        insecure=insecure,\n",
       "        stream=stream,\n",
       "      ).model_dump(exclude_none=\u001b[38;5;28;01mTrue\u001b[39;00m),\n",
       "      stream=stream,\n",
       "    )\n",
       "\u001b[31mFile:\u001b[39m      /home/workspace/wordslab-notebooks-lib/.venv/lib/python3.12/site-packages/ollama/_client.py\n",
       "\u001b[31mType:\u001b[39m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ollama.pull??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333b8647-58f5-495f-add3-bc2309fcc3a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mSignature:\u001b[39m ollama.delete(model: str) -> ollama._types.StatusResponse\n",
       "\u001b[31mDocstring:\u001b[39m <no docstring>\n",
       "\u001b[31mSource:\u001b[39m   \n",
       "  \u001b[38;5;28;01mdef\u001b[39;00m delete(self, model: str) -> StatusResponse:\n",
       "    r = self._request_raw(\n",
       "      \u001b[33m'DELETE'\u001b[39m,\n",
       "      \u001b[33m'/api/delete'\u001b[39m,\n",
       "      json=DeleteRequest(\n",
       "        model=model,\n",
       "      ).model_dump(exclude_none=\u001b[38;5;28;01mTrue\u001b[39;00m),\n",
       "    )\n",
       "    \u001b[38;5;28;01mreturn\u001b[39;00m StatusResponse(\n",
       "      status=\u001b[33m'success'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m r.status_code == \u001b[32m200\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'error'\u001b[39m,\n",
       "    )\n",
       "\u001b[31mFile:\u001b[39m      /home/workspace/wordslab-notebooks-lib/.venv/lib/python3.12/site-packages/ollama/_client.py\n",
       "\u001b[31mType:\u001b[39m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ollama.delete??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b955cc0-51f2-4a6f-be1a-987a3404570f",
   "metadata": {},
   "source": [
    "**Streaming responses**\n",
    "\n",
    "Certain endpoints stream responses as JSON objects. Streaming can be disabled by providing {\"stream\": false} for these endpoints.\n",
    "\n",
    "**Structured outputs**\n",
    "\n",
    "Structured outputs are supported by providing a JSON schema in the format parameter. The model will generate a response that matches the schema. See the structured outputs example below.\n",
    "\n",
    "**JSON mode**\n",
    "\n",
    "Enable JSON mode by setting the format parameter to json. This will structure the response as a valid JSON object. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef72d274-4d55-46e3-809d-ed1a67b63d09",
   "metadata": {},
   "source": [
    "https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-completion\n",
    "\n",
    "Parameters\n",
    "- model: (required) the model name\n",
    "- prompt: the prompt to generate a response for\n",
    "- suffix: the text after the model response\n",
    "- images: (optional) a list of base64-encoded images (for multimodal models such as llava)\n",
    "- think: (for thinking models) should the model think before responding?\n",
    "\n",
    "Advanced parameters (optional):\n",
    "- format: the format to return a response in. Format can be json or a JSON schema\n",
    "- options: additional model parameters listed in the documentation for the Modelfile such as temperature\n",
    "- system: system message to (overrides what is defined in the Modelfile)\n",
    "- template: the prompt template to use (overrides what is defined in the Modelfile)\n",
    "- stream: if false the response will be returned as a single response object, rather than a stream of objects\n",
    "- raw: if true no formatting will be applied to the prompt. You may choose to use the raw parameter if you are specifying a full templated prompt in your request to the API\n",
    "- keep_alive: controls how long the model will stay loaded into memory following the request (default: 5m)\n",
    "\n",
    "Response\n",
    "\n",
    "A stream of JSON objects is returned:\n",
    "\n",
    "{\n",
    "  \"model\": \"llama3.2\",\n",
    "  \"created_at\": \"2023-08-04T08:52:19.385406455-07:00\",\n",
    "  \"response\": \"The\",\n",
    "  \"done\": false\n",
    "}\n",
    "\n",
    "The final response in the stream also includes additional data about the generation:\n",
    "- total_duration: time spent generating the response\n",
    "- load_duration: time spent in nanoseconds loading the model\n",
    "- prompt_eval_count: number of tokens in the prompt\n",
    "- prompt_eval_duration: time spent in nanoseconds evaluating the prompt\n",
    "- eval_count: number of tokens in the response\n",
    "- eval_duration: time in nanoseconds spent generating the response\n",
    "- response: empty if the response was streamed, if not streamed, this will contain the full response\n",
    "\n",
    "A response can be received in one reply when streaming is off.\n",
    "\n",
    "To calculate how fast the response is generated in tokens per second (token/s), divide eval_count / eval_duration * 10^9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043e0236-9c4f-4bd8-8a48-d0b0e4f17500",
   "metadata": {},
   "source": [
    "**Images**\n",
    "\n",
    "To submit images to multimodal models, provide a list of base64-encoded images:\n",
    "\n",
    "- \"images\": [\"iVBORw0KGgoAAAANSUhEUgAAAG0AAABmCAYAAADBPx+VAAAACXBI...\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a9a4ff-fc63-44ef-97ae-26af7567da6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama.generate(model='gemma3', prompt='Why is the sky blue?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fc66e5-9a58-4243-89c3-2ccdb710673e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama.chat(model='gemma3', messages=[{'role': 'user', 'content': 'Why is the sky blue?'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2707163e-156d-4fe7-a57d-8b7f768e9c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama.embed(model='gemma3', input='The sky is blue because of rayleigh scattering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e169690-b0c3-4e1d-82a8-c61027cfec73",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama.embed(model='gemma3', input=['The sky is blue because of rayleigh scattering', 'Grass is green because of chlorophyll'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f141425-512e-4596-a0f6-673a8fccd9cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProcessResponse(models=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.ps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9c2574-6372-4969-9636-6e638ec7fcee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mSignature:\u001b[39m ollama.web_search(query: str, max_results: int = \u001b[32m3\u001b[39m) -> ollama._types.WebSearchResponse\n",
       "\u001b[31mSource:\u001b[39m   \n",
       "  \u001b[38;5;28;01mdef\u001b[39;00m web_search(self, query: str, max_results: int = \u001b[32m3\u001b[39m) -> WebSearchResponse:\n",
       "    \u001b[33m\"\"\"\u001b[39m\n",
       "\u001b[33m    Performs a web search\u001b[39m\n",
       "\n",
       "\u001b[33m    Args:\u001b[39m\n",
       "\u001b[33m      query: The query to search for\u001b[39m\n",
       "\u001b[33m      max_results: The maximum number of results to return (default: 3)\u001b[39m\n",
       "\n",
       "\u001b[33m    Returns:\u001b[39m\n",
       "\u001b[33m      WebSearchResponse with the search results\u001b[39m\n",
       "\u001b[33m    Raises:\u001b[39m\n",
       "\u001b[33m      ValueError: If OLLAMA_API_KEY environment variable is not set\u001b[39m\n",
       "\u001b[33m    \"\"\"\u001b[39m\n",
       "    \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m self._client.headers.get(\u001b[33m'authorization'\u001b[39m, \u001b[33m''\u001b[39m).startswith(\u001b[33m'Bearer '\u001b[39m):\n",
       "      \u001b[38;5;28;01mraise\u001b[39;00m ValueError(\u001b[33m'Authorization header with Bearer token is required for web search'\u001b[39m)\n",
       "\n",
       "    \u001b[38;5;28;01mreturn\u001b[39;00m self._request(\n",
       "      WebSearchResponse,\n",
       "      \u001b[33m'POST'\u001b[39m,\n",
       "      \u001b[33m'https://ollama.com/api/web_search'\u001b[39m,\n",
       "      json=WebSearchRequest(\n",
       "        query=query,\n",
       "        max_results=max_results,\n",
       "      ).model_dump(exclude_none=\u001b[38;5;28;01mTrue\u001b[39;00m),\n",
       "    )\n",
       "\u001b[31mFile:\u001b[39m      /home/workspace/wordslab-notebooks-lib/.venv/lib/python3.12/site-packages/ollama/_client.py\n",
       "\u001b[31mType:\u001b[39m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ollama.web_search??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5753e78-cde5-4f6c-bb76-ffe6e7231170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mSignature:\u001b[39m ollama.web_fetch(url: str) -> ollama._types.WebFetchResponse\n",
       "\u001b[31mSource:\u001b[39m   \n",
       "  \u001b[38;5;28;01mdef\u001b[39;00m web_fetch(self, url: str) -> WebFetchResponse:\n",
       "    \u001b[33m\"\"\"\u001b[39m\n",
       "\u001b[33m    Fetches the content of a web page for the provided URL.\u001b[39m\n",
       "\n",
       "\u001b[33m    Args:\u001b[39m\n",
       "\u001b[33m      url: The URL to fetch\u001b[39m\n",
       "\n",
       "\u001b[33m    Returns:\u001b[39m\n",
       "\u001b[33m      WebFetchResponse with the fetched result\u001b[39m\n",
       "\u001b[33m    \"\"\"\u001b[39m\n",
       "    \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m self._client.headers.get(\u001b[33m'authorization'\u001b[39m, \u001b[33m''\u001b[39m).startswith(\u001b[33m'Bearer '\u001b[39m):\n",
       "      \u001b[38;5;28;01mraise\u001b[39;00m ValueError(\u001b[33m'Authorization header with Bearer token is required for web fetch'\u001b[39m)\n",
       "\n",
       "    \u001b[38;5;28;01mreturn\u001b[39;00m self._request(\n",
       "      WebFetchResponse,\n",
       "      \u001b[33m'POST'\u001b[39m,\n",
       "      \u001b[33m'https://ollama.com/api/web_fetch'\u001b[39m,\n",
       "      json=WebFetchRequest(\n",
       "        url=url,\n",
       "      ).model_dump(exclude_none=\u001b[38;5;28;01mTrue\u001b[39;00m),\n",
       "    )\n",
       "\u001b[31mFile:\u001b[39m      /home/workspace/wordslab-notebooks-lib/.venv/lib/python3.12/site-packages/ollama/_client.py\n",
       "\u001b[31mType:\u001b[39m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ollama.web_fetch??"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wordslab-notebooks-lib",
   "language": "python",
   "name": "wordslab-notebooks-lib"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
