{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0bdbfa3-6397-46f5-9cbf-ce6d574bbd11",
   "metadata": {},
   "source": [
    "# wordslab-notebooks-lib.chat\n",
    "\n",
    "> Chat with local and remote LLMs in the context of the wordslab-notebooks environment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09d2065-de01-4758-9101-fd8238aac9c0",
   "metadata": {},
   "source": [
    "** WORK IN PROGRESS - not exported yet **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c915d8d-bf1a-41e4-a262-409595268e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## #| default_exp chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16eebe41-aa01-45fa-8d37-22dcb74ba621",
   "metadata": {},
   "outputs": [],
   "source": [
    "## #| export\n",
    "from ollama import chat\n",
    "from openai import OpenAI\n",
    "\n",
    "import os\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "\n",
    "from wordslab_notebooks_lib.env import WordslabNotebooksEnv\n",
    "from wordslab_notebooks_lib.notebook import JupyterlabNotebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8d8979-6a3f-47b6-8654-e3a1e9254011",
   "metadata": {},
   "source": [
    "## ollama chat client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8f0da6-eb12-4687-ae0a-ae20be8bc5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = WordslabNotebooksEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d77f8e6-8322-4da8-830e-b2b26ccec705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gemma3:27b'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = env.default_model_chat\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0795c383-f0a7-4d3a-ac1c-c7a19799ffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{'role': 'user', 'content': 'In one sentence: why is the sky blue?'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc80788-6527-4731-9b87-5d8269ebceb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ollama_chat_stream(model, messages):\n",
    "    stream = chat(model=model, messages=messages, stream=True)\n",
    "    for chunk in stream:\n",
    "        yield chunk['message']['content']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fdcf40-384d-4920-8794-5cc8e7d44e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The sky is blue because of a phenomenon called Rayleigh scattering, where shorter wavelengths of light (like blue and violet) are scattered more by the Earth's atmosphere than other colors, making blue appear to dominate our view.\n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stream = ollama_chat_stream(model, messages)\n",
    "\n",
    "streamed_text = \"\"\n",
    "for chunk in stream:\n",
    "    streamed_text += chunk  \n",
    "    clear_output(wait=True)\n",
    "    display(Markdown(streamed_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a589734-14f6-476d-8445-6bc0fc1a65f4",
   "metadata": {},
   "source": [
    "## openrouter chat client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac5ddbb-3907-4bdc-a922-33c7345d0f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ[\"OPENROUTER_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8107815-648b-4434-8300-15795a8f10ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mistralai/mistral-small-creative'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = \"mistralai/mistral-small-creative\"\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c0a058-d4df-4063-9870-52f47f4fd691",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(base_url=\"https://openrouter.ai/api/v1\", api_key=api_key)\n",
    "\n",
    "def openrouter_chat_stream(model, messages):\n",
    "    stream = client.chat.completions.create(model=model, messages=messages, stream=True)\n",
    "    for chunk in stream:\n",
    "        yield chunk.choices[0].delta.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97ce419-4fde-4026-a0c0-86431caad059",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = openrouter_chat_stream(model, messages)\n",
    "\n",
    "streamed_text = \"\"\n",
    "for chunk in stream:\n",
    "    streamed_text += chunk  \n",
    "    clear_output(wait=True)\n",
    "    display(Markdown(streamed_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290d9215-0b80-4664-8385-3c8ecca0984e",
   "metadata": {},
   "source": [
    "## Notebook chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55786d5f-710d-434e-931d-4d29da19bac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook = JupyterlabNotebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fa7f1a-aafc-472a-9363-bd65f8860fc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a4fa7f1a-aafc-472a-9363-bd65f8860fc5'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notebook.cell_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a8cae6-d10d-4050-8c6a-04d7b12409bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c8a8cae6-d10d-4050-8c6a-04d7b12409bd'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notebook.cell_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d96c931-4043-4425-a119-d4055928a6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## #| exports\n",
    "class NbChat:\n",
    "\n",
    "    \n",
    "    def __init__(self, model=None):\n",
    "        if model is None:\n",
    "            wlnb = WordslabNotebooks()\n",
    "            self.model = wlnb.default_model_chat\n",
    "        else:\n",
    "            self.model = model\n",
    "\n",
    "        self.prompt_template = \"\"\"You are an AI assistant designed to help the user learn and solve problems interactively.\n",
    "You run in an interactive Jupyter notebook environment where the user can write code, take notes, and chat with you.\n",
    "You work with the user step-by-step rather than just giving complete answers. You are especially good at:\n",
    "- Breaking down complex topics into manageable pieces\n",
    "- Helping the user work through coding problems in Python\n",
    "- Encouraging the user to try things himself, with guidance when he needs it\n",
    "- Adapting to the user level and interests\n",
    "You are designed to be collaborative - you ask questions, check the user understanding, and let him explore ideas rather than just lecturing. \n",
    "You can help with teaching, coding, problem-solving, research, and creative projects.\n",
    "What sets you apart is your teaching approach - you focus on helping the user develop his skills rather than just giving him answers. \n",
    "You provide information in small chunks, check in frequently to see if things make sense, and encourage the user to try things himself.\n",
    "\n",
    "# Jupyter notebook - all cells above the user instruction in XML\n",
    "\n",
    "{notebook_context}\n",
    "\n",
    "# User Instruction - in the last code cell of the notebook\n",
    "\n",
    "Execute this user instruction in the context of the code cells above:\n",
    "\n",
    "{user_instruction}\n",
    "\"\"\"\n",
    "\n",
    "    async def __call__(self, user_instruction, timeout=1):\n",
    "        notebook_context = await get_notebook_context(timeout=timeout)\n",
    "        prompt = self.prompt_template.format(user_instruction=user_instruction, notebook_context=notebook_context)\n",
    "        stream = chat(model=self.model, messages=[{'role': 'user', 'content': prompt}], stream=True)\n",
    "        streamed_text = \"\"\n",
    "        for chunk in stream:\n",
    "            streamed_text += chunk['message']['content']    \n",
    "            clear_output(wait=True)\n",
    "            display(Markdown(streamed_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85bd7e2-e8b8-4100-99f5-278fff76bed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbchat = NbChat(\"mistral-small3.2:24b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce3f187-3603-4586-b6f9-637fdb22a72f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This notebook appears to be focused on exploring and demonstrating the capabilities of the **Ollama API**, particularly for interacting with large language models (LLMs) like Gemma3, Mistral, and others. Here's a breakdown of its key components:\n",
       "\n",
       "### Key Features Demonstrated:\n",
       "1. **Model Interaction**:\n",
       "   - The notebook shows how to use the `ollama` Python client to interact with different models (e.g., `gemma3`, `mistral-small3.2`).\n",
       "   - It includes examples of generating text, chatting, and embedding text using these models.\n",
       "\n",
       "2. **Streaming Responses**:\n",
       "   - The notebook demonstrates how to handle streaming responses from the API, which is useful for real-time interactions.\n",
       "\n",
       "3. **Notebook Context**:\n",
       "   - There's a `NbChat` class designed to integrate the LLM with the Jupyter notebook environment. This class uses the notebook's context (previous code cells and outputs) to provide more relevant responses.\n",
       "\n",
       "4. **Web Search and Fetch**:\n",
       "   - The notebook explores the `web_search` and `web_fetch` methods of the `ollama` client, which allow the LLM to fetch and process web content.\n",
       "\n",
       "5. **Model Management**:\n",
       "   - The notebook includes methods to list, pull, and delete models, showing how to manage the models available to the `ollama` client.\n",
       "\n",
       "### Example Use Case:\n",
       "The notebook is particularly useful for:\n",
       "- **Educational purposes**: Teaching how to interact with LLMs programmatically.\n",
       "- **Development**: Building applications that require LLM capabilities, such as chatbots, text generation tools, or research assistants.\n",
       "- **Exploration**: Understanding the capabilities of different models and how to use them effectively.\n",
       "\n",
       "### Summary:\n",
       "This notebook serves as a comprehensive guide to using the `ollama` API for interacting with large language models in a Jupyter notebook environment. It covers everything from basic text generation to advanced features like streaming and web search integration."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "await nbchat(\"What is this notebook about?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d1b005-b6ab-4713-a30c-2d510652ff7b",
   "metadata": {},
   "source": [
    "## Design concepts\n",
    "\n",
    "### User centric workflow\n",
    "\n",
    "1. identify your self-hosted inference or inference as a service options\n",
    "2. understand your task type, properties, privacy needs and scale\n",
    "3. find the best model for your task, given your constraints\n",
    "4. prepare and start your self hosted inference or connect to your inference as a service provider\n",
    "5. monitor your resource usage and cost\n",
    "\n",
    "### Self-hosted inference or inference as a service\n",
    "\n",
    "Model families\n",
    "- architecture name\n",
    "- parameter size\n",
    "- training type: base / instruct / thinking\n",
    "- version: relase date\n",
    "- quantization\n",
    "\n",
    "Model constraints\n",
    "- model capabilities\n",
    "  - modalities in/out\n",
    "  - context length\n",
    "  - instruction\n",
    "  - thinking\n",
    "  - tools\n",
    "- model usage\n",
    "  - prompt template and special tokens\n",
    "  - languages supported\n",
    "  - recommended use cases\n",
    "  - prompting guidelines \n",
    "- model license\n",
    "  - use case restrictions\n",
    "  - commercial usage restrictions\n",
    "  - outputs usage restrictions \n",
    "- model transparency\n",
    "\n",
    "Self-hosted inference constraints\n",
    "- model requirements\n",
    "  - size on disk -> download time / load time in vram\n",
    "  - size in vram -> max context length / num parallel sequence\n",
    "  - tensor flops -> input tokens/sec\n",
    "  - memory bandwidth -> output tokens/sec\n",
    "- inference machine constraints\n",
    "  - download speed\n",
    "  - disk size and speed\n",
    "  - GPU vram, memory bandwidth, tensor flops\n",
    "- rented machine constraints\n",
    "  - GPU availability\n",
    "  - price when you use per GPU\n",
    "  - price when you don't use per GB (storage)\n",
    "\n",
    "Inference as a service constraints\n",
    "- router constraints\n",
    "  - ... same as provider constraints below ... \n",
    "- provider constraints\n",
    "  - terms of service\n",
    "  - privacy options\n",
    "  - inference quotas\n",
    "  - service availability\n",
    "- per model provider constraints\n",
    "  - model capabilities exposed \n",
    "  - input/output tokens cost\n",
    "  - input/output tokens/sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ea4051-4f99-4342-ac13-934f46de39cf",
   "metadata": {},
   "source": [
    "## ModelsProvider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc3cc74-8fd8-4899-ae78-904b72159d29",
   "metadata": {},
   "source": [
    "### List, download and load models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0f44df-1f37-49c9-912e-7fbe4fd33cf0",
   "metadata": {},
   "source": [
    "### Explore ollama API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798c7ec5-e1d2-463c-a892-2c2a7f3dc814",
   "metadata": {},
   "source": [
    "Get ollama version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143e8e1d-5041-4115-9e66-40757b9f1a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Request\n",
    "curl http://localhost:11434/api/version\n",
    "Response\n",
    "{\n",
    "  \"version\": \"0.5.1\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5cfbc9-2b9a-4e18-b564-7e4d2a775d0b",
   "metadata": {},
   "source": [
    "List remote models\n",
    "\n",
    "As of december 2025, there is no API to get the ollama catalog of models, web scraping is the only solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e2ead5-6c44-4aae-8620-6119163bd102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import re\n",
    "from html import unescape\n",
    "\n",
    "def updated_to_months(updated):\n",
    "    \"\"\"\n",
    "    Convert strings like:\n",
    "      \"1 year ago\", \"2 years ago\",\n",
    "      \"1 month ago\", \"3 weeks ago\",\n",
    "      \"7 days ago\", \"yesterday\",\n",
    "      \"4 hours ago\"\n",
    "    into integer months.\n",
    "    \"\"\"\n",
    "    if not updated:\n",
    "        return None\n",
    "\n",
    "    updated = updated.lower().strip()\n",
    "\n",
    "    # handle 'yesterday' explicitly\n",
    "    if updated == \"yesterday\":\n",
    "        return 0\n",
    "\n",
    "    # years → months\n",
    "    m = re.match(r'(\\d+)\\s+year', updated)\n",
    "    if m:\n",
    "        years = int(m.group(1))\n",
    "        return years * 12\n",
    "\n",
    "    # months\n",
    "    m = re.match(r'(\\d+)\\s+month', updated)\n",
    "    if m:\n",
    "        return int(m.group(1))\n",
    "\n",
    "    # weeks\n",
    "    m = re.match(r'(\\d+)\\s+week', updated)\n",
    "    if m:\n",
    "        weeks = int(m.group(1))\n",
    "        return max(0, weeks // 4)\n",
    "\n",
    "    # days\n",
    "    m = re.match(r'(\\d+)\\s+day', updated)\n",
    "    if m:\n",
    "        return 0\n",
    "\n",
    "    # hours / minutes / seconds → treat as < 1 month\n",
    "    if any(unit in updated for unit in [\"hour\", \"minute\", \"second\"]):\n",
    "        return 0\n",
    "\n",
    "    return None\n",
    "\n",
    "def pulls_to_int(pulls_str):\n",
    "    \"\"\"\n",
    "    Convert a pulls string like:\n",
    "        '5M', '655.8K', '49K', '73.7M', '957.4K', '27.7M'\n",
    "    into an integer.\n",
    "    \"\"\"\n",
    "    if not pulls_str:\n",
    "        return None\n",
    "\n",
    "    pulls_str = pulls_str.strip().upper()\n",
    "\n",
    "    match = re.match(r'([\\d,.]+)\\s*([KM]?)', pulls_str)\n",
    "    if not match:\n",
    "        return None\n",
    "\n",
    "    number, suffix = match.groups()\n",
    "    # Remove commas and convert to float\n",
    "    number = float(number.replace(',', ''))\n",
    "\n",
    "    if suffix == 'M':\n",
    "        number *= 1_000_000\n",
    "    elif suffix == 'K':\n",
    "        number *= 1_000\n",
    "\n",
    "    return int(number)\n",
    "\n",
    "def parse_model_list_regex(html):\n",
    "    models = []\n",
    "\n",
    "    # --- Extract each <li x-test-model>...</li> block ---\n",
    "    li_blocks = re.findall(\n",
    "        r'<li[^>]*x-test-model[^>]*>(.*?)</li>',\n",
    "        html,\n",
    "        flags=re.DOTALL\n",
    "    )\n",
    "\n",
    "    for block in li_blocks:\n",
    "\n",
    "        # name from <a href=\"/library/...\">\n",
    "        name = None\n",
    "        m = re.search(r'href=\"/library/([^\"]+)\"', block)\n",
    "        if m:\n",
    "            name = m.group(1)\n",
    "\n",
    "        # description <p class=\"max-w-lg ...\">...</p>\n",
    "        description = \"\"\n",
    "        m = re.search(\n",
    "            r'<p[^>]*text-neutral-800[^>]*>(.*?)</p>',\n",
    "            block,\n",
    "            flags=re.DOTALL\n",
    "        )\n",
    "        if m:\n",
    "            description = re.sub(r'<.*?>', '', m.group(1)).strip()\n",
    "            description = unescape(description)\n",
    "\n",
    "        # capabilities (x-test-capability)\n",
    "        capabilities = re.findall(\n",
    "            r'<span[^>]*x-test-capability[^>]*>(.*?)</span>',\n",
    "            block,\n",
    "            flags=re.DOTALL\n",
    "        )\n",
    "        capabilities = [c.strip() for c in capabilities]\n",
    "\n",
    "        # check for the special 'cloud' span \n",
    "        cloud = False\n",
    "        if re.search(\n",
    "            r'<span[^>]*>cloud</span>',\n",
    "            block,\n",
    "            flags=re.DOTALL\n",
    "        ):\n",
    "            cloud = True\n",
    "\n",
    "        # sizes (x-test-size)\n",
    "        sizes = re.findall(\n",
    "            r'<span[^>]*x-test-size[^>]*>(.*?)</span>',\n",
    "            block,\n",
    "            flags=re.DOTALL\n",
    "        )\n",
    "        sizes = [s.strip() for s in sizes]\n",
    "\n",
    "        # pulls <span x-test-pull-count>5M</span>\n",
    "        pulls = None\n",
    "        m = re.search(\n",
    "            r'<span[^>]*x-test-pull-count[^>]*>(.*?)</span>',\n",
    "            block\n",
    "        )\n",
    "        if m:\n",
    "            pulls = m.group(1).strip()\n",
    "\n",
    "        # tag count <span x-test-tag-count>5</span>\n",
    "        tag_count = None\n",
    "        m = re.search(\n",
    "            r'<span[^>]*x-test-tag-count[^>]*>(.*?)</span>',\n",
    "            block\n",
    "        )\n",
    "        if m:\n",
    "            tag_count = m.group(1).strip()\n",
    "\n",
    "        # updated text <span x-test-updated>...</span>\n",
    "        updated = None\n",
    "        m = re.search(\n",
    "            r'<span[^>]*x-test-updated[^>]*>(.*?)</span>',\n",
    "            block\n",
    "        )\n",
    "        if m:\n",
    "            updated = m.group(1).strip()\n",
    "\n",
    "        models.append({\n",
    "            \"name\": name,\n",
    "            \"description\": description,\n",
    "            \"capabilities\": capabilities,\n",
    "            \"cloud\": cloud,\n",
    "            \"sizes\": sizes,\n",
    "            \"pulls\": pulls_to_int(pulls),\n",
    "            \"tag_count\": int(tag_count),\n",
    "            \"updated_months\": updated_to_months(updated),\n",
    "            \"url\": f\"https://ollama.com/library/{name}\" if name else None\n",
    "        })\n",
    "\n",
    "    return models   \n",
    "\n",
    "def list_models(contains=None):\n",
    "    \"\"\"\n",
    "    Extract model names and properties from https://ollama.com/library\n",
    "    Optionally filter by substring.\n",
    "    \"\"\"\n",
    "\n",
    "    html = httpx.get(\"https://ollama.com/library\").text\n",
    "    models = parse_model_list_regex(html)\n",
    "\n",
    "    if contains:\n",
    "        models = [\n",
    "            m for m in models\n",
    "            if contains.lower() in m[\"name\"].lower()\n",
    "        ]\n",
    "        models = sorted(models, key=lambda m:m[\"name\"])\n",
    "\n",
    "    return models\n",
    "\n",
    "def list_recent_models_from_family(familyfilter):\n",
    "    return [f\"{m['name']} {m['capabilities'] if len(m['capabilities'])>0 else ''} {m['sizes'] if len(m['sizes'])>0 else ''}{' [cloud]' if m['cloud'] else ''}\" for m in list_models(familyfilter) if m[\"updated_months\"] is not None and m[\"updated_months\"]<12]\n",
    "\n",
    "def list_tags(model):\n",
    "    \"\"\"\n",
    "    Extract valid quantized tags only, without HTML noise,\n",
    "    and apply the same exclusions as original greps.\n",
    "    \"\"\"\n",
    "    html = httpx.get(f\"https://ollama.com/library/{model}/tags\").text\n",
    "\n",
    "    # Capture ONLY the tag part after model:..., e.g. 3b-instruct-q4_K_M\n",
    "    raw_tags = re.findall(\n",
    "        rf'{re.escape(model)}:([A-Za-z0-9._-]*q[A-Za-z0-9._-]*)',\n",
    "        html\n",
    "    )\n",
    "\n",
    "    # Re-add full prefix model:<tag>\n",
    "    tags = [f\"{model}:{t}\" for t in raw_tags]\n",
    "\n",
    "    # Exclude text|base|fp|q4_[01]|q5_[01]\n",
    "    tags = [\n",
    "        t for t in tags\n",
    "        if not re.search(r'(text|base|fp|q[45]_[01])', t)\n",
    "    ]\n",
    "\n",
    "    # Deduplicate\n",
    "    return set(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392377be-d847-46b9-9658-430ee348aba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'gpt-oss',\n",
       "  'description': 'OpenAI’s open-weight models designed for powerful reasoning, agentic tasks, and versatile developer use cases.',\n",
       "  'capabilities': ['tools', 'thinking'],\n",
       "  'cloud': True,\n",
       "  'sizes': ['20b', '120b'],\n",
       "  'pulls': 5000000,\n",
       "  'tag_count': 5,\n",
       "  'updated_months': 1,\n",
       "  'url': 'https://ollama.com/library/gpt-oss'},\n",
       " {'name': 'qwen3-vl',\n",
       "  'description': 'The most powerful vision-language model in the Qwen model family to date.',\n",
       "  'capabilities': ['vision', 'tools'],\n",
       "  'cloud': True,\n",
       "  'sizes': ['2b', '4b', '8b', '30b', '32b', '235b'],\n",
       "  'pulls': 656300,\n",
       "  'tag_count': 59,\n",
       "  'updated_months': 1,\n",
       "  'url': 'https://ollama.com/library/qwen3-vl'},\n",
       " {'name': 'ministral-3',\n",
       "  'description': 'The Ministral 3 family is designed for edge deployment, capable of running on a wide range of hardware.',\n",
       "  'capabilities': ['vision', 'tools'],\n",
       "  'cloud': True,\n",
       "  'sizes': ['3b', '8b', '14b'],\n",
       "  'pulls': 49100,\n",
       "  'tag_count': 16,\n",
       "  'updated_months': 0,\n",
       "  'url': 'https://ollama.com/library/ministral-3'},\n",
       " {'name': 'deepseek-r1',\n",
       "  'description': 'DeepSeek-R1 is a family of open reasoning models with performance approaching that of leading models, such as O3 and Gemini 2.5 Pro.',\n",
       "  'capabilities': ['tools', 'thinking'],\n",
       "  'cloud': False,\n",
       "  'sizes': ['1.5b', '7b', '8b', '14b', '32b', '70b', '671b'],\n",
       "  'pulls': 73700000,\n",
       "  'tag_count': 35,\n",
       "  'updated_months': 5,\n",
       "  'url': 'https://ollama.com/library/deepseek-r1'},\n",
       " {'name': 'qwen3-coder',\n",
       "  'description': \"Alibaba's performant long context models for agentic and coding tasks.\",\n",
       "  'capabilities': ['tools'],\n",
       "  'cloud': True,\n",
       "  'sizes': ['30b', '480b'],\n",
       "  'pulls': 958100,\n",
       "  'tag_count': 10,\n",
       "  'updated_months': 2,\n",
       "  'url': 'https://ollama.com/library/qwen3-coder'}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_models()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a513f8cf-0679-44c0-bdb4-4fca3aaf1a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"qwen2.5-coder ['tools'] ['0.5b', '1.5b', '3b', '7b', '14b', '32b']\",\n",
       " \"qwen2.5vl ['vision'] ['3b', '7b', '32b', '72b']\",\n",
       " \"qwen3 ['tools', 'thinking'] ['0.6b', '1.7b', '4b', '8b', '14b', '30b', '32b', '235b']\",\n",
       " \"qwen3-coder ['tools'] ['30b', '480b'] [cloud]\",\n",
       " \"qwen3-embedding ['embedding'] ['0.6b', '4b', '8b']\",\n",
       " \"qwen3-vl ['vision', 'tools'] ['2b', '4b', '8b', '30b', '32b', '235b'] [cloud]\"]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_recent_models_from_family(\"qwen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f04212a-ad4a-4811-a8f9-833dee73e09d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"embeddinggemma ['embedding'] ['300m']\",\n",
       " \"gemma3 ['vision'] ['270m', '1b', '4b', '12b', '27b'] [cloud]\",\n",
       " \"gemma3n  ['e2b', 'e4b']\"]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_recent_models_from_family(\"gemma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3cae3b-d873-468d-9d80-8058b785ab8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"devstral ['tools'] ['24b']\",\n",
       " \"magistral ['tools', 'thinking'] ['24b']\",\n",
       " \"ministral-3 ['vision', 'tools'] ['3b', '8b', '14b'] [cloud]\",\n",
       " \"mistral ['tools'] ['7b']\",\n",
       " 'mistral-large-3   [cloud]',\n",
       " \"mistral-nemo ['tools'] ['12b']\",\n",
       " \"mistral-small ['tools'] ['22b', '24b']\",\n",
       " \"mistral-small3.1 ['vision', 'tools'] ['24b']\",\n",
       " \"mistral-small3.2 ['vision', 'tools'] ['24b']\"]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_recent_models_from_family(\"stral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e33d36-4e00-4d8c-9ce1-d6a0c9eafa84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"gpt-oss ['tools', 'thinking'] ['20b', '120b'] [cloud]\",\n",
       " \"gpt-oss-safeguard ['tools', 'thinking'] ['20b', '120b']\"]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_recent_models_from_family(\"gpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524e9f87-1b3d-4190-9f56-dfbcff10bf15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"deepseek-ocr ['vision'] ['3b']\",\n",
       " \"deepseek-r1 ['tools', 'thinking'] ['1.5b', '7b', '8b', '14b', '32b', '70b', '671b']\",\n",
       " \"deepseek-v3  ['671b']\",\n",
       " \"deepseek-v3.1 ['tools', 'thinking'] ['671b'] [cloud]\"]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_recent_models_from_family(\"deepseek\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e455be-b1ae-4e12-bc16-d72ea08ea494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['glm-4.6   [cloud]']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_recent_models_from_family(\"glm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e6dd40-3fc6-4feb-b2a2-209865434abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"granite-embedding ['embedding'] ['30m', '278m']\",\n",
       " \"granite3.1-dense ['tools'] ['2b', '8b']\",\n",
       " \"granite3.1-moe ['tools'] ['1b', '3b']\",\n",
       " \"granite3.2 ['tools'] ['2b', '8b']\",\n",
       " \"granite3.2-vision ['vision', 'tools'] ['2b']\",\n",
       " \"granite3.3 ['tools'] ['2b', '8b']\",\n",
       " \"granite4 ['tools'] ['350m', '1b', '3b']\"]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_recent_models_from_family(\"granite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71f35d1-2ae8-4564-9f9c-17ca98e378f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"llama3.2-vision ['vision'] ['11b', '90b']\",\n",
       " \"llama4 ['vision', 'tools'] ['16x17b', '128x17b']\"]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_recent_models_from_family(\"llama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7893a9-afa2-4fb1-bfd0-9b5936e5eea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"dolphin-mixtral  ['8x7b', '8x22b']\",\n",
       " \"dolphin3  ['8b']\",\n",
       " \"phi4  ['14b']\",\n",
       " \"phi4-mini ['tools'] ['3.8b']\",\n",
       " \"phi4-mini-reasoning  ['3.8b']\",\n",
       " \"phi4-reasoning  ['14b']\"]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_recent_models_from_family(\"phi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bdadec-2b71-40a7-be49-31fcf9e7a227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"hermes3 ['tools'] ['3b', '8b', '70b', '405b']\",\n",
       " \"nous-hermes2-mixtral  ['8x7b']\"]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_recent_models_from_family(\"hermes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88de106f-16a6-49f8-bacb-fdae0515e3e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"olmo2  ['7b', '13b']\"]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_recent_models_from_family(\"olmo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee669d8-b586-4e7b-aa37-cf65489fabcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"embeddinggemma ['embedding'] ['300m']\",\n",
       " \"granite-embedding ['embedding'] ['30m', '278m']\",\n",
       " \"qwen3-embedding ['embedding'] ['0.6b', '4b', '8b']\"]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_recent_models_from_family(\"embed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c24bc81-f6d7-433e-b3a8-53e70f914442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ministral-3:14b-instruct-2512-q4_K_M',\n",
       " 'ministral-3:14b-instruct-2512-q8_0',\n",
       " 'ministral-3:3b-instruct-2512-q4_K_M',\n",
       " 'ministral-3:3b-instruct-2512-q8_0',\n",
       " 'ministral-3:8b-instruct-2512-q4_K_M',\n",
       " 'ministral-3:8b-instruct-2512-q8_0'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_tags(\"ministral-3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd1969b-2832-47f9-8569-add48c966f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mistral-small3.2:24b-instruct-2506-q4_K_M',\n",
       " 'mistral-small3.2:24b-instruct-2506-q8_0'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_tags(\"mistral-small3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81100df9-ed00-4bd4-afd6-96c8ddf9f1d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qwen3-vl:235b-a22b-instruct-q4_K_M',\n",
       " 'qwen3-vl:235b-a22b-instruct-q8_0',\n",
       " 'qwen3-vl:235b-a22b-thinking-q4_K_M',\n",
       " 'qwen3-vl:235b-a22b-thinking-q8_0',\n",
       " 'qwen3-vl:2b-instruct-q4_K_M',\n",
       " 'qwen3-vl:2b-instruct-q8_0',\n",
       " 'qwen3-vl:2b-thinking-q4_K_M',\n",
       " 'qwen3-vl:2b-thinking-q8_0',\n",
       " 'qwen3-vl:30b-a3b-instruct-q4_K_M',\n",
       " 'qwen3-vl:30b-a3b-instruct-q8_0',\n",
       " 'qwen3-vl:30b-a3b-thinking-q4_K_M',\n",
       " 'qwen3-vl:30b-a3b-thinking-q8_0',\n",
       " 'qwen3-vl:32b-instruct-q4_K_M',\n",
       " 'qwen3-vl:32b-instruct-q8_0',\n",
       " 'qwen3-vl:32b-thinking-q4_K_M',\n",
       " 'qwen3-vl:32b-thinking-q8_0',\n",
       " 'qwen3-vl:4b-instruct-q4_K_M',\n",
       " 'qwen3-vl:4b-instruct-q8_0',\n",
       " 'qwen3-vl:4b-thinking-q4_K_M',\n",
       " 'qwen3-vl:4b-thinking-q8_0',\n",
       " 'qwen3-vl:8b-instruct-q4_K_M',\n",
       " 'qwen3-vl:8b-instruct-q8_0',\n",
       " 'qwen3-vl:8b-thinking-q4_K_M',\n",
       " 'qwen3-vl:8b-thinking-q8_0'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_tags(\"qwen3-vl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48681ee4-9ab4-4858-bc08-3b1970567acd",
   "metadata": {},
   "source": [
    "https://github.com/ollama/ollama/blob/main/docs/api.md#list-local-models\n",
    "\n",
    "ollama.list().models -> list(ollama._types.ListResponse.Model)\n",
    "\n",
    "```yaml\n",
    "ollama._types.ListResponse.Model\n",
    "- model: str 'qwen3:4b'\n",
    "- modified_at: datetime.datetime datetime(2025, 11, 22, 18, 53, 11)\n",
    "- digest: str '359d7dd4bcdab3d86b87d73ac27966f4dbb9f5efdfcc75d34a8764a09474fae7'\n",
    "- size: pydantic.types.ByteSize 2497293931\n",
    "- details: ollama._types.ModelDetails\n",
    "  - parent_model: str ''\n",
    "  - format: str 'gguf'\n",
    "  - family: str 'qwen3'\n",
    "  - families: Sequence[str] ['qwen3']\n",
    "  - parameter_size: str '4.0B'\n",
    "  - quantization_level: str 'Q4_K_M'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1465604-236a-4e4d-b149-acd1e27d6721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(model='qwen3:4b', modified_at=datetime.datetime(2025, 11, 22, 18, 53, 11, 586211, tzinfo=TzInfo(3600)), digest='359d7dd4bcdab3d86b87d73ac27966f4dbb9f5efdfcc75d34a8764a09474fae7', size=2497293931, details=ModelDetails(parent_model='', format='gguf', family='qwen3', families=['qwen3'], parameter_size='4.0B', quantization_level='Q4_K_M'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.list().models[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b470e97c-1a8d-464b-8b03-09898457d198",
   "metadata": {},
   "source": [
    "https://github.com/ollama/ollama/blob/main/docs/api.md#show-model-information\n",
    "\n",
    "```yaml\n",
    "ollama._types.ShowResponse\n",
    "- modified_at: datetime.datetime datetime.datetime(2025, 11, 22, 18, 53, 11)\n",
    "- template: str '{{- $lastUserIdx := -1 -}}...\\n{{- end }}'\n",
    "- modelfile: str '...'\n",
    "- license: str '...'\n",
    "- details: ollama._types.ModelDetails -> see above\n",
    "- model_info: Mapping[str, Any]\n",
    "  -'general.architecture': 'qwen3'\n",
    "  -'general.basename': 'Qwen3' \n",
    "  -'general.file_type': 15\n",
    "  -'general.finetune': 'Thinking' \n",
    "  -'general.license': 'apache-2.0'\n",
    "  -'general.license.link': 'https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507/blob/main/LICENSE'\n",
    "  -'general.parameter_count': 4022468096\n",
    "  -'general.quantization_version': 2, \n",
    "  -'general.size_label': '4B'\n",
    "  -'general.tags': None\n",
    "  -'general.type': 'model'\n",
    "  -'general.version': '2507'\n",
    "  -'qwen3.attention.head_count': 32\n",
    "  -'qwen3.attention.head_count_kv': 8\n",
    "  -'qwen3.attention.key_length': 128\n",
    "  -'qwen3.attention.layer_norm_rms_epsilon': 1e-06\n",
    "  -'qwen3.attention.value_length': 128\n",
    "  -'qwen3.block_count': 36\n",
    "  -'qwen3.context_length': 262144\n",
    "  -'qwen3.embedding_length': 2560\n",
    "  -'qwen3.feed_forward_length': 9728\n",
    "  -'qwen3.rope.freq_base': 5000000\n",
    "  -'tokenizer.ggml.add_bos_token': False\n",
    "  -'tokenizer.ggml.bos_token_id': 151643\n",
    "  -'tokenizer.ggml.eos_token_id': 151645\n",
    "  -'tokenizer.ggml.merges': None\n",
    "  -'tokenizer.ggml.model': 'gpt2'\n",
    "  -'tokenizer.ggml.padding_token_id': 151643\n",
    "  -'tokenizer.ggml.pre': 'qwen2'\n",
    "  -'tokenizer.ggml.token_type': None\n",
    "  -'tokenizer.ggml.tokens': None\n",
    "- parameters: str 'top_p 0.95\\n repeat_penalty 1\\n stop \"<|im_start|>\"\\n stop \"<|im_end|>\"\\n temperature 0.6\\ n top_k 20'\n",
    "- capabilities: List[str] ['completion', 'tools', 'thinking']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c73dfb-fabd-4ee1-9fba-7a24b74cfdde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['completion', 'vision'],\n",
       " {'gemma3.attention.head_count': 8,\n",
       "  'gemma3.attention.head_count_kv': 4,\n",
       "  'gemma3.attention.key_length': 256,\n",
       "  'gemma3.attention.sliding_window': 1024,\n",
       "  'gemma3.attention.value_length': 256,\n",
       "  'gemma3.block_count': 34,\n",
       "  'gemma3.context_length': 131072,\n",
       "  'gemma3.embedding_length': 2560,\n",
       "  'gemma3.feed_forward_length': 10240,\n",
       "  'gemma3.mm.tokens_per_image': 256,\n",
       "  'gemma3.vision.attention.head_count': 16,\n",
       "  'gemma3.vision.attention.layer_norm_epsilon': 1e-06,\n",
       "  'gemma3.vision.block_count': 27,\n",
       "  'gemma3.vision.embedding_length': 1152,\n",
       "  'gemma3.vision.feed_forward_length': 4304,\n",
       "  'gemma3.vision.image_size': 896,\n",
       "  'gemma3.vision.num_channels': 3,\n",
       "  'gemma3.vision.patch_size': 14,\n",
       "  'general.architecture': 'gemma3',\n",
       "  'general.file_type': 15,\n",
       "  'general.parameter_count': 4299915632,\n",
       "  'general.quantization_version': 2,\n",
       "  'tokenizer.ggml.add_bos_token': True,\n",
       "  'tokenizer.ggml.add_eos_token': False,\n",
       "  'tokenizer.ggml.add_padding_token': False,\n",
       "  'tokenizer.ggml.add_unknown_token': False,\n",
       "  'tokenizer.ggml.bos_token_id': 2,\n",
       "  'tokenizer.ggml.eos_token_id': 1,\n",
       "  'tokenizer.ggml.merges': None,\n",
       "  'tokenizer.ggml.model': 'llama',\n",
       "  'tokenizer.ggml.padding_token_id': 0,\n",
       "  'tokenizer.ggml.pre': 'default',\n",
       "  'tokenizer.ggml.scores': None,\n",
       "  'tokenizer.ggml.token_type': None,\n",
       "  'tokenizer.ggml.tokens': None,\n",
       "  'tokenizer.ggml.unknown_token_id': 3})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.show('gemma3:4b').capabilities, ollama.show('gemma3:4b').modelinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efffa6e8-2816-4c1c-a051-dc29b5bc477c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mSignature:\u001b[39m ollama.pull(model: str, *, insecure: bool = \u001b[38;5;28;01mFalse\u001b[39;00m, stream: bool = \u001b[38;5;28;01mFalse\u001b[39;00m) -> Union[ollama._types.ProgressResponse, collections.abc.Iterator[ollama._types.ProgressResponse]]\n",
       "\u001b[31mSource:\u001b[39m   \n",
       "  \u001b[38;5;28;01mdef\u001b[39;00m pull(\n",
       "    self,\n",
       "    model: str,\n",
       "    *,\n",
       "    insecure: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    stream: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "  ) -> Union[ProgressResponse, Iterator[ProgressResponse]]:\n",
       "    \u001b[33m\"\"\"\u001b[39m\n",
       "\u001b[33m    Raises `ResponseError` if the request could not be fulfilled.\u001b[39m\n",
       "\n",
       "\u001b[33m    Returns `ProgressResponse` if `stream` is `False`, otherwise returns a `ProgressResponse` generator.\u001b[39m\n",
       "\u001b[33m    \"\"\"\u001b[39m\n",
       "    \u001b[38;5;28;01mreturn\u001b[39;00m self._request(\n",
       "      ProgressResponse,\n",
       "      \u001b[33m'POST'\u001b[39m,\n",
       "      \u001b[33m'/api/pull'\u001b[39m,\n",
       "      json=PullRequest(\n",
       "        model=model,\n",
       "        insecure=insecure,\n",
       "        stream=stream,\n",
       "      ).model_dump(exclude_none=\u001b[38;5;28;01mTrue\u001b[39;00m),\n",
       "      stream=stream,\n",
       "    )\n",
       "\u001b[31mFile:\u001b[39m      /home/workspace/wordslab-notebooks-lib/.venv/lib/python3.12/site-packages/ollama/_client.py\n",
       "\u001b[31mType:\u001b[39m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ollama.pull??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333b8647-58f5-495f-add3-bc2309fcc3a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mSignature:\u001b[39m ollama.delete(model: str) -> ollama._types.StatusResponse\n",
       "\u001b[31mDocstring:\u001b[39m <no docstring>\n",
       "\u001b[31mSource:\u001b[39m   \n",
       "  \u001b[38;5;28;01mdef\u001b[39;00m delete(self, model: str) -> StatusResponse:\n",
       "    r = self._request_raw(\n",
       "      \u001b[33m'DELETE'\u001b[39m,\n",
       "      \u001b[33m'/api/delete'\u001b[39m,\n",
       "      json=DeleteRequest(\n",
       "        model=model,\n",
       "      ).model_dump(exclude_none=\u001b[38;5;28;01mTrue\u001b[39;00m),\n",
       "    )\n",
       "    \u001b[38;5;28;01mreturn\u001b[39;00m StatusResponse(\n",
       "      status=\u001b[33m'success'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m r.status_code == \u001b[32m200\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'error'\u001b[39m,\n",
       "    )\n",
       "\u001b[31mFile:\u001b[39m      /home/workspace/wordslab-notebooks-lib/.venv/lib/python3.12/site-packages/ollama/_client.py\n",
       "\u001b[31mType:\u001b[39m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ollama.delete??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b955cc0-51f2-4a6f-be1a-987a3404570f",
   "metadata": {},
   "source": [
    "**Streaming responses**\n",
    "\n",
    "Certain endpoints stream responses as JSON objects. Streaming can be disabled by providing {\"stream\": false} for these endpoints.\n",
    "\n",
    "**Structured outputs**\n",
    "\n",
    "Structured outputs are supported by providing a JSON schema in the format parameter. The model will generate a response that matches the schema. See the structured outputs example below.\n",
    "\n",
    "**JSON mode**\n",
    "\n",
    "Enable JSON mode by setting the format parameter to json. This will structure the response as a valid JSON object. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef72d274-4d55-46e3-809d-ed1a67b63d09",
   "metadata": {},
   "source": [
    "https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-completion\n",
    "\n",
    "Parameters\n",
    "- model: (required) the model name\n",
    "- prompt: the prompt to generate a response for\n",
    "- suffix: the text after the model response\n",
    "- images: (optional) a list of base64-encoded images (for multimodal models such as llava)\n",
    "- think: (for thinking models) should the model think before responding?\n",
    "\n",
    "Advanced parameters (optional):\n",
    "- format: the format to return a response in. Format can be json or a JSON schema\n",
    "- options: additional model parameters listed in the documentation for the Modelfile such as temperature\n",
    "- system: system message to (overrides what is defined in the Modelfile)\n",
    "- template: the prompt template to use (overrides what is defined in the Modelfile)\n",
    "- stream: if false the response will be returned as a single response object, rather than a stream of objects\n",
    "- raw: if true no formatting will be applied to the prompt. You may choose to use the raw parameter if you are specifying a full templated prompt in your request to the API\n",
    "- keep_alive: controls how long the model will stay loaded into memory following the request (default: 5m)\n",
    "\n",
    "Response\n",
    "\n",
    "A stream of JSON objects is returned:\n",
    "\n",
    "{\n",
    "  \"model\": \"llama3.2\",\n",
    "  \"created_at\": \"2023-08-04T08:52:19.385406455-07:00\",\n",
    "  \"response\": \"The\",\n",
    "  \"done\": false\n",
    "}\n",
    "\n",
    "The final response in the stream also includes additional data about the generation:\n",
    "- total_duration: time spent generating the response\n",
    "- load_duration: time spent in nanoseconds loading the model\n",
    "- prompt_eval_count: number of tokens in the prompt\n",
    "- prompt_eval_duration: time spent in nanoseconds evaluating the prompt\n",
    "- eval_count: number of tokens in the response\n",
    "- eval_duration: time in nanoseconds spent generating the response\n",
    "- response: empty if the response was streamed, if not streamed, this will contain the full response\n",
    "\n",
    "A response can be received in one reply when streaming is off.\n",
    "\n",
    "To calculate how fast the response is generated in tokens per second (token/s), divide eval_count / eval_duration * 10^9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043e0236-9c4f-4bd8-8a48-d0b0e4f17500",
   "metadata": {},
   "source": [
    "**Images**\n",
    "\n",
    "To submit images to multimodal models, provide a list of base64-encoded images:\n",
    "\n",
    "- \"images\": [\"iVBORw0KGgoAAAANSUhEUgAAAG0AAABmCAYAAADBPx+VAAAACXBI...\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a9a4ff-fc63-44ef-97ae-26af7567da6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama.generate(model='gemma3', prompt='Why is the sky blue?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fc66e5-9a58-4243-89c3-2ccdb710673e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama.chat(model='gemma3', messages=[{'role': 'user', 'content': 'Why is the sky blue?'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2707163e-156d-4fe7-a57d-8b7f768e9c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama.embed(model='gemma3', input='The sky is blue because of rayleigh scattering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e169690-b0c3-4e1d-82a8-c61027cfec73",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama.embed(model='gemma3', input=['The sky is blue because of rayleigh scattering', 'Grass is green because of chlorophyll'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f141425-512e-4596-a0f6-673a8fccd9cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProcessResponse(models=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.ps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9c2574-6372-4969-9636-6e638ec7fcee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mSignature:\u001b[39m ollama.web_search(query: str, max_results: int = \u001b[32m3\u001b[39m) -> ollama._types.WebSearchResponse\n",
       "\u001b[31mSource:\u001b[39m   \n",
       "  \u001b[38;5;28;01mdef\u001b[39;00m web_search(self, query: str, max_results: int = \u001b[32m3\u001b[39m) -> WebSearchResponse:\n",
       "    \u001b[33m\"\"\"\u001b[39m\n",
       "\u001b[33m    Performs a web search\u001b[39m\n",
       "\n",
       "\u001b[33m    Args:\u001b[39m\n",
       "\u001b[33m      query: The query to search for\u001b[39m\n",
       "\u001b[33m      max_results: The maximum number of results to return (default: 3)\u001b[39m\n",
       "\n",
       "\u001b[33m    Returns:\u001b[39m\n",
       "\u001b[33m      WebSearchResponse with the search results\u001b[39m\n",
       "\u001b[33m    Raises:\u001b[39m\n",
       "\u001b[33m      ValueError: If OLLAMA_API_KEY environment variable is not set\u001b[39m\n",
       "\u001b[33m    \"\"\"\u001b[39m\n",
       "    \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m self._client.headers.get(\u001b[33m'authorization'\u001b[39m, \u001b[33m''\u001b[39m).startswith(\u001b[33m'Bearer '\u001b[39m):\n",
       "      \u001b[38;5;28;01mraise\u001b[39;00m ValueError(\u001b[33m'Authorization header with Bearer token is required for web search'\u001b[39m)\n",
       "\n",
       "    \u001b[38;5;28;01mreturn\u001b[39;00m self._request(\n",
       "      WebSearchResponse,\n",
       "      \u001b[33m'POST'\u001b[39m,\n",
       "      \u001b[33m'https://ollama.com/api/web_search'\u001b[39m,\n",
       "      json=WebSearchRequest(\n",
       "        query=query,\n",
       "        max_results=max_results,\n",
       "      ).model_dump(exclude_none=\u001b[38;5;28;01mTrue\u001b[39;00m),\n",
       "    )\n",
       "\u001b[31mFile:\u001b[39m      /home/workspace/wordslab-notebooks-lib/.venv/lib/python3.12/site-packages/ollama/_client.py\n",
       "\u001b[31mType:\u001b[39m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ollama.web_search??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5753e78-cde5-4f6c-bb76-ffe6e7231170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mSignature:\u001b[39m ollama.web_fetch(url: str) -> ollama._types.WebFetchResponse\n",
       "\u001b[31mSource:\u001b[39m   \n",
       "  \u001b[38;5;28;01mdef\u001b[39;00m web_fetch(self, url: str) -> WebFetchResponse:\n",
       "    \u001b[33m\"\"\"\u001b[39m\n",
       "\u001b[33m    Fetches the content of a web page for the provided URL.\u001b[39m\n",
       "\n",
       "\u001b[33m    Args:\u001b[39m\n",
       "\u001b[33m      url: The URL to fetch\u001b[39m\n",
       "\n",
       "\u001b[33m    Returns:\u001b[39m\n",
       "\u001b[33m      WebFetchResponse with the fetched result\u001b[39m\n",
       "\u001b[33m    \"\"\"\u001b[39m\n",
       "    \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m self._client.headers.get(\u001b[33m'authorization'\u001b[39m, \u001b[33m''\u001b[39m).startswith(\u001b[33m'Bearer '\u001b[39m):\n",
       "      \u001b[38;5;28;01mraise\u001b[39;00m ValueError(\u001b[33m'Authorization header with Bearer token is required for web fetch'\u001b[39m)\n",
       "\n",
       "    \u001b[38;5;28;01mreturn\u001b[39;00m self._request(\n",
       "      WebFetchResponse,\n",
       "      \u001b[33m'POST'\u001b[39m,\n",
       "      \u001b[33m'https://ollama.com/api/web_fetch'\u001b[39m,\n",
       "      json=WebFetchRequest(\n",
       "        url=url,\n",
       "      ).model_dump(exclude_none=\u001b[38;5;28;01mTrue\u001b[39;00m),\n",
       "    )\n",
       "\u001b[31mFile:\u001b[39m      /home/workspace/wordslab-notebooks-lib/.venv/lib/python3.12/site-packages/ollama/_client.py\n",
       "\u001b[31mType:\u001b[39m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ollama.web_fetch??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1e1f38-bdde-4288-84a3-a59251075286",
   "metadata": {},
   "source": [
    "## Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390d650a-c704-4fd8-a025-7bc10c74df85",
   "metadata": {},
   "source": [
    "### Explore ollama, vllm and openrouter APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45a8fc4-a08d-4787-a134-1b2eaf0a64da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the information in this notebook, we can create a function to filter Ollama models that can run on a GPU with 16GB VRAM. Here's how we can approach this:\n",
       "\n",
       "1. We'll use the `ollama.list()` method to get all available models\n",
       "2. We'll filter models based on their VRAM requirements\n",
       "3. We'll consider that 16GB VRAM can typically handle models up to about 13B parameters (though this varies by model architecture)\n",
       "\n",
       "Here's the function:\n",
       "\n",
       "```python\n",
       "def get_models_for_16gb_vram():\n",
       "    \"\"\"Returns a list of Ollama models that can run on a GPU with 16GB VRAM\"\"\"\n",
       "    all_models = ollama.list()\n",
       "\n",
       "    # Define a mapping of model sizes to VRAM requirements\n",
       "    # These are approximate values based on typical requirements\n",
       "    model_vram_map = {\n",
       "        'gemma3': {\n",
       "            '27b': 24,  # 27B model needs ~24GB VRAM\n",
       "            '13b': 16,  # 13B model fits in 16GB\n",
       "            '7b': 8,    # 7B model fits in 8GB\n",
       "            '3b': 4     # 3B model fits in 4GB\n",
       "        },\n",
       "        'mistral': {\n",
       "            'small3.2': 12,  # 24B model needs ~12GB VRAM\n",
       "            'small3': 10,    # 10B model fits in 10GB\n",
       "            'tiny': 4        # Tiny model fits in 4GB\n",
       "        },\n",
       "        # Add other models as needed\n",
       "    }\n",
       "\n",
       "    compatible_models = []\n",
       "\n",
       "    for model in all_models.models:\n",
       "        model_name = model.name\n",
       "        model_size = model.modelfile.get('parameters', {}).get('size', 'unknown')\n",
       "\n",
       "        # Get VRAM requirement\n",
       "        vram_required = 0\n",
       "        if model_name in model_vram_map:\n",
       "            if model_size in model_vram_map[model_name]:\n",
       "                vram_required = model_vram_map[model_name][model_size]\n",
       "\n",
       "        # Check if model fits in 16GB VRAM\n",
       "        if vram_required <= 16:\n",
       "            compatible_models.append({\n",
       "                'name': model_name,\n",
       "                'size': model_size,\n",
       "                'vram_required': vram_required\n",
       "            })\n",
       "\n",
       "    return compatible_models\n",
       "```\n",
       "\n",
       "Example usage:\n",
       "```python\n",
       "compatible_models = get_models_for_16gb_vram()\n",
       "for model in compatible_models:\n",
       "    print(f\"{model['name']} ({model['size']}): {model['vram_required']}GB VRAM required\")\n",
       "```\n",
       "\n",
       "Note: The VRAM requirements are approximate and may vary based on:\n",
       "- The specific GPU architecture\n",
       "- Batch size\n",
       "- Other system configurations\n",
       "- Model quantization (if supported)\n",
       "\n",
       "For more accurate information, you might want to:\n",
       "1. Check the official documentation for each model\n",
       "2. Test the models on your specific hardware\n",
       "3. Consider using model quantization to reduce VRAM usage"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "await nbchat(\"using only the bits of information present in this notebooks, try to generate a function to get the list of ollama models which can run in a GPU with 16GB vram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c31b72-b90a-469a-8b08-b98430dd0a33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57d06641-308c-4ea9-b78e-3cc5b2f17e06",
   "metadata": {},
   "source": [
    "#### Format the notebook cells for LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8179693c-6429-4b1f-986c-24211b8d271a",
   "metadata": {},
   "source": [
    "Convert notebook contents to compact XML - code and format copied from **toolslm by AnswerDotAI**:\n",
    "\n",
    "https://github.com/AnswerDotAI/toolslm/blob/main/00_xml.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaa9e78-f789-4a4b-ac2c-71460fc5eeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## #| exports\n",
    "def get_mime_text(data):\n",
    "    \"Get text from MIME bundle, preferring markdown over plain\"\n",
    "    if 'text/markdown' in data: return ''.join(list(data['text/markdown']))\n",
    "    if 'text/plain' in data: return ''.join(list(data['text/plain']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a994bed3-b6ef-4629-8473-30bc8ceef13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## #| exports\n",
    "def cell2out(o):\n",
    "    \"Convert single notebook output to XML format\"\n",
    "    if hasattr(o, 'data'): \n",
    "        txt = get_mime_text(o.data)\n",
    "        if txt: return Out(txt, mime='markdown' if 'text/markdown' in o.data else 'plain')\n",
    "    if hasattr(o, 'text'):\n",
    "        txt = o.text if isinstance(o.text, str) else ''.join(o.text)\n",
    "        return Out(txt, type='stream', name=o.get('name', 'stdout'))\n",
    "    if hasattr(o, 'ename'): return Out(f\"{o.ename}: {o.evalue}\", type='error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e55eb50-5ea0-42b6-b0e2-65183a68b27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## #| exports\n",
    "def cell2xml(cell):\n",
    "    \"Convert notebook cell to concise XML format\"\n",
    "    cts = Source(''.join(cell.source)) if hasattr(cell, 'source') and cell.source else None\n",
    "    out_items = L(getattr(cell,'outputs',[])).map(cell2out).filter()\n",
    "    outs = []\n",
    "    if out_items: outs = Outs(*out_items)\n",
    "    parts = [p for p in [cts, outs] if p]\n",
    "    return Cell(*parts, type=cell.cell_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edba80e6-770a-4383-b31c-892ab2590548",
   "metadata": {},
   "outputs": [],
   "source": [
    "## #| exports\n",
    "def nb2xml(nb, until_cell_id):\n",
    "    cells_xml = []\n",
    "    for c in nb.cells:\n",
    "        if c.id == until_cell_id: break\n",
    "        if c.cell_type in ('code','markdown'):\n",
    "            cells_xml.append(to_xml(cell2xml(c), do_escape=False))\n",
    "    return '\\n'.join(cells_xml)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4c1ae5-d585-4940-a62c-1df5f33952d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb2xml(nb, executing_cell_id)[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea144aca-f0e8-462e-9800-2401292e40be",
   "metadata": {},
   "outputs": [],
   "source": [
    "## #| exports\n",
    "async def get_notebook_context(timeout=1):\n",
    "    data = await get_notebook_data(timeout=timeout)\n",
    "    notebook_content = data[\"notebook\"]\n",
    "    nb = nbformat.from_dict(notebook_content)\n",
    "    cell_id = data[\"cell_id\"]\n",
    "    return nb2xml(nb, cell_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f080093-98cc-4fe5-aaa8-888576cd69f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "await get_notebook_context(timeout=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994e4c57-1a44-4aa3-a12d-14be370679aa",
   "metadata": {},
   "source": [
    "You can see that the content of this cell, which is below the call to get_notebook_context(), doesn't appear in the context."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wordslab-notebooks-lib",
   "language": "python",
   "name": "wordslab-notebooks-lib"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
