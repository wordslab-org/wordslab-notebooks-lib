[
  {
    "objectID": "env.html",
    "href": "env.html",
    "title": "wordslab-notebooks-lib.env",
    "section": "",
    "text": "WordslabNotebooksEnv class is the entry point to explore the wordslab-notebooks environment variables.\n\n\n\n\ndef WordslabEnv(\n    \n):\n\nInitialize self. See help(type(self)) for accurate signature.\n\nenv = WordslabEnv()\n\n\n\n\n\nenv.version\n\n'2026-01'\n\n\n\nenv.platform\n\n'WindowsSubsystemForLinux'\n\n\n\n\n\n\nenv.url_dashboard\n\n'http://192.168.1.16:8888'\n\n\n\nenv.url_openwebui\n\n'https://192.168.1.16:8882'\n\n\n\nenv.url_jupyterlab\n\n'https://192.168.1.16:8880'\n\n\n\nenv.url_vscode\n\n'https://192.168.1.16:8881'\n\n\n\nenv.url_userapp1\n\n'https://192.168.1.16:8883'\n\n\n\nenv.url_userapp2\n\n'https://192.168.1.16:8884'\n\n\n\nenv.url_userapp3\n\n'https://192.168.1.16:8885'\n\n\n\nenv.url_userapp4\n\n'https://192.168.1.16:8886'\n\n\n\nenv.url_userapp5\n\n'https://192.168.1.16:8887'\n\n\n\n\n\n\nenv.port_ollama\n\n11434\n\n\n\nenv.port_vllm\n\n8000\n\n\n\nenv.port_docling\n\n5001\n\n\n\nenv.port_userapp1\n\n'8883'\n\n\n\nenv.port_userapp2\n\n'8884'\n\n\n\nenv.port_userapp3\n\n'8885'\n\n\n\nenv.port_userapp4\n\n'8886'\n\n\n\nenv.port_userapp5\n\n'8887'\n\n\n\n\n\n\nenv.dir_home\n\n'/home'\n\n\n\nenv.dir_scripts\n\n'/home/wordslab-notebooks-2026-01'\n\n\n\nenv.dir_python\n\n'/home/python'\n\n\n\nenv.dir_workspace\n\n'/home/workspace'\n\n\n\nenv.dir_models\n\n'/home/models'\n\n\n\n\n\n\nenv.dir_openwebui\n\n'/home/open-webui'\n\n\n\nenv.dir_jupyterlab\n\n'/home/jupyterlab'\n\n\n\nenv.dir_vscode\n\n'/home/code-server'\n\n\n\nenv.dir_ollama\n\n'/home/ollama'\n\n\n\nenv.dir_docling\n\n'/home/docling'\n\n\n\n\n\n\nenv.dir_openwebui_data\n\n'/home/workspace/.openwebui'\n\n\n\nenv.dir_jupyterlab_data\n\n'/home/workspace/.jupyter'\n\n\n\nenv.dir_vscode_data\n\n'/home/workspace/.codeserver'\n\n\n\nenv.dir_docling_data\n\n'/home/workspace/.docling'\n\n\n\n\n\n\nenv.dir_models_ollama\n\n'/home/models/ollama'\n\n\n\nenv.dir_models_vllm\n\n'/home/models/huggingface'\n\n\n\nenv.dir_models_hugginface\n\n'/home/models/huggingface'\n\n\n\nenv.dir_models_fastai\n\n'/home/models/fastai'\n\n\n\nenv.dir_models_pytorch\n\n'/home/models/torch'\n\n\n\nenv.dir_models_keras\n\n'/home/models/keras'\n\n\n\nenv.dir_models_tensorflow\n\n'/home/models/tfhub_modules'\n\n\n\nenv.dir_models_docling\n\n'/home/models/docling'\n\n\n\n\n\n\nenv.default_model_chat\n\n'gemma3:27b'\n\n\n\nenv.default_model_code\n\n'qwen3:30b'\n\n\n\nenv.default_model_agent\n\n'glm-4.7-flash:q4_K_M'\n\n\n\nenv.default_model_embedding\n\n'embeddinggemma:300m'\n\n\n\nenv.self.default_model_context_length",
    "crumbs": [
      "wordslab-notebooks-lib.env"
    ]
  },
  {
    "objectID": "env.html#wordslab-notebooks-environment-variables",
    "href": "env.html#wordslab-notebooks-environment-variables",
    "title": "wordslab-notebooks-lib.env",
    "section": "",
    "text": "WordslabNotebooksEnv class is the entry point to explore the wordslab-notebooks environment variables.\n\n\n\n\ndef WordslabEnv(\n    \n):\n\nInitialize self. See help(type(self)) for accurate signature.\n\nenv = WordslabEnv()\n\n\n\n\n\nenv.version\n\n'2026-01'\n\n\n\nenv.platform\n\n'WindowsSubsystemForLinux'\n\n\n\n\n\n\nenv.url_dashboard\n\n'http://192.168.1.16:8888'\n\n\n\nenv.url_openwebui\n\n'https://192.168.1.16:8882'\n\n\n\nenv.url_jupyterlab\n\n'https://192.168.1.16:8880'\n\n\n\nenv.url_vscode\n\n'https://192.168.1.16:8881'\n\n\n\nenv.url_userapp1\n\n'https://192.168.1.16:8883'\n\n\n\nenv.url_userapp2\n\n'https://192.168.1.16:8884'\n\n\n\nenv.url_userapp3\n\n'https://192.168.1.16:8885'\n\n\n\nenv.url_userapp4\n\n'https://192.168.1.16:8886'\n\n\n\nenv.url_userapp5\n\n'https://192.168.1.16:8887'\n\n\n\n\n\n\nenv.port_ollama\n\n11434\n\n\n\nenv.port_vllm\n\n8000\n\n\n\nenv.port_docling\n\n5001\n\n\n\nenv.port_userapp1\n\n'8883'\n\n\n\nenv.port_userapp2\n\n'8884'\n\n\n\nenv.port_userapp3\n\n'8885'\n\n\n\nenv.port_userapp4\n\n'8886'\n\n\n\nenv.port_userapp5\n\n'8887'\n\n\n\n\n\n\nenv.dir_home\n\n'/home'\n\n\n\nenv.dir_scripts\n\n'/home/wordslab-notebooks-2026-01'\n\n\n\nenv.dir_python\n\n'/home/python'\n\n\n\nenv.dir_workspace\n\n'/home/workspace'\n\n\n\nenv.dir_models\n\n'/home/models'\n\n\n\n\n\n\nenv.dir_openwebui\n\n'/home/open-webui'\n\n\n\nenv.dir_jupyterlab\n\n'/home/jupyterlab'\n\n\n\nenv.dir_vscode\n\n'/home/code-server'\n\n\n\nenv.dir_ollama\n\n'/home/ollama'\n\n\n\nenv.dir_docling\n\n'/home/docling'\n\n\n\n\n\n\nenv.dir_openwebui_data\n\n'/home/workspace/.openwebui'\n\n\n\nenv.dir_jupyterlab_data\n\n'/home/workspace/.jupyter'\n\n\n\nenv.dir_vscode_data\n\n'/home/workspace/.codeserver'\n\n\n\nenv.dir_docling_data\n\n'/home/workspace/.docling'\n\n\n\n\n\n\nenv.dir_models_ollama\n\n'/home/models/ollama'\n\n\n\nenv.dir_models_vllm\n\n'/home/models/huggingface'\n\n\n\nenv.dir_models_hugginface\n\n'/home/models/huggingface'\n\n\n\nenv.dir_models_fastai\n\n'/home/models/fastai'\n\n\n\nenv.dir_models_pytorch\n\n'/home/models/torch'\n\n\n\nenv.dir_models_keras\n\n'/home/models/keras'\n\n\n\nenv.dir_models_tensorflow\n\n'/home/models/tfhub_modules'\n\n\n\nenv.dir_models_docling\n\n'/home/models/docling'\n\n\n\n\n\n\nenv.default_model_chat\n\n'gemma3:27b'\n\n\n\nenv.default_model_code\n\n'qwen3:30b'\n\n\n\nenv.default_model_agent\n\n'glm-4.7-flash:q4_K_M'\n\n\n\nenv.default_model_embedding\n\n'embeddinggemma:300m'\n\n\n\nenv.self.default_model_context_length",
    "crumbs": [
      "wordslab-notebooks-lib.env"
    ]
  },
  {
    "objectID": "env.html#user-defined-environment-variables",
    "href": "env.html#user-defined-environment-variables",
    "title": "wordslab-notebooks-lib.env",
    "section": "User-defined environment variables",
    "text": "User-defined environment variables\nYou can save user-defined environment variables in the following file and they will be loaded by wordslab-notebooks each time a new shell is created:\n\n$WORDSLAB_WORKSPACE/.secrets/user-env.bashrc\n\nThis module provides Python functions to programmatically manage these user-defined environment variables.\nThey should primarily be used to register API keys use to connect to external services.\n\n\nWordslabEnv.write_user_env_var\n\ndef write_user_env_var(\n    var_name:str, var_value:str\n)-&gt;None:\n\nWrite or update an environment variable in $WORDSLAB_WORKSPACE/.secrets/user-env.bashrc\nThe file is created if it doesn’t exist. The variable is created if it doesn’t exist and updated if it already exist. The value is safely quoted for Bash while preserving $VAR references.\nThe new variable value is automatically injected in the current python process. The environment variable will be automatically loaded each time you start a new shell. But you need to restart all the currently running shells and other processes to see the new variable or value.\n\n\n\nWordslabEnv.read_user_env_var\n\ndef read_user_env_var(\n    var_name:str\n)-&gt;Optional:\n\nRead a variable value from $WORDSLAB_WORKSPACE/.secrets/user-env.bashrc\nReturns the raw (unexpanded) value, or None if not found.\n\nenv.write_user_env_var(\"SECRETS_PATH\", \"$WORDSLAB_WORKSPACE/.secrets\")\n\n\n# !cat $WORDSLAB_WORKSPACE/.secrets/user-env.bashrc\n\n\nos.environ[\"SECRETS_PATH\"]\n\n'/home/workspace/.secrets'\n\n\n\nenv.read_user_env_var(\"SECRETS_PATH\")\n\n'$WORDSLAB_WORKSPACE/.secrets'",
    "crumbs": [
      "wordslab-notebooks-lib.env"
    ]
  },
  {
    "objectID": "env.html#connections-to-external-services",
    "href": "env.html#connections-to-external-services",
    "title": "wordslab-notebooks-lib.env",
    "section": "Connections to external services",
    "text": "Connections to external services\n\nOpenRouter\n\n\n\nWordslabEnv.setup_openrouter\n\ndef setup_openrouter(\n    openrouter_api_key:str\n)-&gt;None:\n\n\n# env.setup_openrouter(openrouter_api_key=\"sk-...\")\n\n\n# env = WordslabEnv()\n# env.cloud_openrouter_api_key\n\n\n\nReplicate\n\n\n\nWordslabEnv.setup_replicate\n\ndef setup_replicate(\n    replicate_api_token:str\n)-&gt;None:\n\n\n# env.setup_replicate(replicate_api_token=\"r8_...\")\n\n\n# env = WordslabEnv()\n# env.cloud_replicate_api_token\n\n\n\nTavily\n\n\n\nWordslabEnv.setup_tavily\n\ndef setup_tavily(\n    tavily_api_key:str\n)-&gt;None:\n\n\n# env.setup_tavily(tavily_api_key=\"tvly-...\")\n\n\n# env = WordslabEnv()\n# env.cloud_tavily_api_key\n\n\n\nOllama cloud\n\n\n\nWordslabEnv.setup_ollamacloud\n\ndef setup_ollamacloud(\n    ollama_api_key:str\n)-&gt;None:\n\n\n# env.setup_ollamacloud(ollama_api_key=\"0d2...\")\n\n\n# env = WordslabEnv()\n# env.cloud_ollama_api_key\n\n\n\nGithub\n\n\n\nWordslabEnv.setup_github\n\ndef setup_github(\n    git_user_name:str, git_user_email:str, github_username:str, github_access_token:str\n)-&gt;None:\n\n\n# env.setup_github(git_user_name=\"John Doe\", git_user_email=\"john.doe@email.com\", github_username=\"johndoe\", github_access_token=\"ghp_...\")\n\nWrote Github credentials\n\n\n\n\nHuggingface\n\n\n\nWordslabEnv.setup_huggingface\n\ndef setup_huggingface(\n    huggingface_access_token:str\n)-&gt;None:\n\n\n# env.setup_huggingface(huggingface_access_token=\"hf_...\")\n\n\n# env = WordslabEnv()\n# env.cloud_huggingface_access_token\n\n\n\nPypi\n\n\n\nWordslabEnv.setup_pypi\n\ndef setup_pypi(\n    pypi_api_token:str\n)-&gt;None:\n\n\n# env.setup_pypi(pypi_api_token=\"pypi-...\")\n\nWrote Pypi credentials",
    "crumbs": [
      "wordslab-notebooks-lib.env"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html",
    "href": "tools_study_mistral-vibe.html",
    "title": "Mistral vibe 2.0",
    "section": "",
    "text": "Mistral Vibe is a command-line coding assistant powered by Mistral’s models, providing a conversational interface to interact with codebases through natural language.\n\n\n\n\n\nPurpose: Command-line interface and argument parsing\nKey Features:\n\nArgument parsing for interactive vs programmatic modes\nTrust folder system for security\nSession continuation/resumption\nSetup workflow for API keys\nWorking directory management\n\n\n\n\n\n\nPurpose: Main conversation loop and orchestration\nKey Responsibilities:\n\nManages conversation state and message history\nHandles LLM interactions (streaming and non-streaming)\nTool execution lifecycle (approval, execution, result handling)\nMiddleware pipeline for turn limits, price limits, auto-compaction\nSession management and logging\nContext compaction for long conversations\nAgent switching and configuration\n\n\n\n\n\n\nPurpose: Agent profile management and configuration\nKey Components:\n\nAgentProfile: Defines agent behavior (safety level, tool permissions, etc.)\nAgentManager: Manages active agent and profile switching\nBuilt-in agents: default, plan, accept-edits, auto-approve, explore\nAgent types: AGENT (interactive) and SUBAGENT (background task worker)\n\n\n\n\n\n\nPurpose: Tool discovery, management, and execution\nKey Components:\n\nToolManager: Discovers and manages available tools\nBaseTool: Abstract base class for all tools\nBuilt-in tools:\n\nread_file, write_file, search_replace: File operations\nbash: Shell command execution\ngrep: Code searching\ntodo: Task management\nask_user_question: Interactive user queries\ntask: Subagent delegation\n\nMCP (Model Context Protocol) integration for external tools\n\n\n\n\n\n\nPurpose: Extensible functionality through reusable components\nKey Features:\n\nSkill discovery from multiple paths (global, local, custom)\nPattern-based skill enabling/disabling\nSkill metadata parsing from SKILL.md files\nFollows Agent Skills specification\n\n\n\n\n\n\nPurpose: Centralized configuration management\nKey Features:\n\nTOML-based configuration\nMultiple provider support (Mistral, generic LLM APIs)\nModel configuration and pricing\nTool permissions and allowlists/denylists\nMCP server configuration\nSession logging settings\nProject context scanning configuration\n\n\n\n\n\n\nPurpose: LLM provider integration and message formatting\nKey Components:\n\nBackend factory for different providers\nMessage formatting and tool schema generation\nStreaming and non-streaming completion support\nToken counting and usage tracking\n\n\n\n\n\n\nPurpose: Interactive terminal interface\nKey Features:\n\nRich terminal UI with Textual framework\nMessage display and history\nTool output viewing\nTodo list management\nAutocompletion for commands and file paths\nTheme support and customization\nExternal editor integration\n\n\n\n\n\n\nPurpose: Persistent conversation state\nKey Features:\n\nSession logging and saving\nSession continuation/resumption\nSession migration for version compatibility\nContext compaction for long conversations\n\n\n\n\n\n\nPurpose: Extensible conversation flow control\nKey Middleware:\n\nTurnLimitMiddleware: Limits conversation turns\nPriceLimitMiddleware: Enforces cost limits\nAutoCompactMiddleware: Automatically compacts context\nContextWarningMiddleware: Warns about context size\nPlanAgentMiddleware: Special handling for plan agent\n\n\n\n\n\n\n\nInteractive Mode:\n\nReal-time conversation with the AI agent\nTool execution with approval workflow\nRich terminal UI with message history\nAutocompletion for commands and file paths\n\nProgrammatic Mode:\n\nNon-interactive execution via --prompt flag\nAuto-approve mode for scripting\nMultiple output formats (text, JSON, streaming)\nTurn and price limits\n\nTool Execution:\n\nApproval workflow (manual, auto-approve, or per-tool)\nAllowlist/denylist filtering\nStreaming tool output\nError handling and recovery\n\nSkills System:\n\nExtend functionality with reusable components\nCustom slash commands\nPattern-based skill management\n\nSecurity Features:\n\nTrust folder system\nTool permission levels (never, ask, always)\nAllowlist/denylist patterns\nSession isolation\n\n\n\n\n\n\nDependency Injection:\n\nConfiguration is passed through callables to support dynamic updates\nBackend factory pattern for LLM providers\n\nEvent-Driven Architecture:\n\nConversation events (user messages, assistant responses, tool calls)\nStreaming events for real-time updates\n\nMiddleware Pattern:\n\nExtensible pipeline for conversation flow control\nBefore/after turn hooks\n\nPlugin System:\n\nSkills for extensible functionality\nMCP servers for external tools\n\nState Management:\n\nImmutable message history\nSession logging and continuation\nContext compaction for long conversations\n\n\n\n\n\n\nUser Input → CLI Entry Point → Agent Loop\nAgent Loop → LLM Backend (for completions) → Message Formatting\nTool Calls → Tool Manager → Tool Execution → Results\nEvents → UI Updates → User Feedback Loop\nSession Data → Session Logger → Persistent Storage\n\nThis architecture provides a flexible, extensible foundation for building a powerful CLI coding assistant with support for multiple workflows, security features, and extensibility through skills and MCP servers.",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#mistral-vibe-architecture-overview",
    "href": "tools_study_mistral-vibe.html#mistral-vibe-architecture-overview",
    "title": "Mistral vibe 2.0",
    "section": "",
    "text": "Mistral Vibe is a command-line coding assistant powered by Mistral’s models, providing a conversational interface to interact with codebases through natural language.\n\n\n\n\n\nPurpose: Command-line interface and argument parsing\nKey Features:\n\nArgument parsing for interactive vs programmatic modes\nTrust folder system for security\nSession continuation/resumption\nSetup workflow for API keys\nWorking directory management\n\n\n\n\n\n\nPurpose: Main conversation loop and orchestration\nKey Responsibilities:\n\nManages conversation state and message history\nHandles LLM interactions (streaming and non-streaming)\nTool execution lifecycle (approval, execution, result handling)\nMiddleware pipeline for turn limits, price limits, auto-compaction\nSession management and logging\nContext compaction for long conversations\nAgent switching and configuration\n\n\n\n\n\n\nPurpose: Agent profile management and configuration\nKey Components:\n\nAgentProfile: Defines agent behavior (safety level, tool permissions, etc.)\nAgentManager: Manages active agent and profile switching\nBuilt-in agents: default, plan, accept-edits, auto-approve, explore\nAgent types: AGENT (interactive) and SUBAGENT (background task worker)\n\n\n\n\n\n\nPurpose: Tool discovery, management, and execution\nKey Components:\n\nToolManager: Discovers and manages available tools\nBaseTool: Abstract base class for all tools\nBuilt-in tools:\n\nread_file, write_file, search_replace: File operations\nbash: Shell command execution\ngrep: Code searching\ntodo: Task management\nask_user_question: Interactive user queries\ntask: Subagent delegation\n\nMCP (Model Context Protocol) integration for external tools\n\n\n\n\n\n\nPurpose: Extensible functionality through reusable components\nKey Features:\n\nSkill discovery from multiple paths (global, local, custom)\nPattern-based skill enabling/disabling\nSkill metadata parsing from SKILL.md files\nFollows Agent Skills specification\n\n\n\n\n\n\nPurpose: Centralized configuration management\nKey Features:\n\nTOML-based configuration\nMultiple provider support (Mistral, generic LLM APIs)\nModel configuration and pricing\nTool permissions and allowlists/denylists\nMCP server configuration\nSession logging settings\nProject context scanning configuration\n\n\n\n\n\n\nPurpose: LLM provider integration and message formatting\nKey Components:\n\nBackend factory for different providers\nMessage formatting and tool schema generation\nStreaming and non-streaming completion support\nToken counting and usage tracking\n\n\n\n\n\n\nPurpose: Interactive terminal interface\nKey Features:\n\nRich terminal UI with Textual framework\nMessage display and history\nTool output viewing\nTodo list management\nAutocompletion for commands and file paths\nTheme support and customization\nExternal editor integration\n\n\n\n\n\n\nPurpose: Persistent conversation state\nKey Features:\n\nSession logging and saving\nSession continuation/resumption\nSession migration for version compatibility\nContext compaction for long conversations\n\n\n\n\n\n\nPurpose: Extensible conversation flow control\nKey Middleware:\n\nTurnLimitMiddleware: Limits conversation turns\nPriceLimitMiddleware: Enforces cost limits\nAutoCompactMiddleware: Automatically compacts context\nContextWarningMiddleware: Warns about context size\nPlanAgentMiddleware: Special handling for plan agent\n\n\n\n\n\n\n\nInteractive Mode:\n\nReal-time conversation with the AI agent\nTool execution with approval workflow\nRich terminal UI with message history\nAutocompletion for commands and file paths\n\nProgrammatic Mode:\n\nNon-interactive execution via --prompt flag\nAuto-approve mode for scripting\nMultiple output formats (text, JSON, streaming)\nTurn and price limits\n\nTool Execution:\n\nApproval workflow (manual, auto-approve, or per-tool)\nAllowlist/denylist filtering\nStreaming tool output\nError handling and recovery\n\nSkills System:\n\nExtend functionality with reusable components\nCustom slash commands\nPattern-based skill management\n\nSecurity Features:\n\nTrust folder system\nTool permission levels (never, ask, always)\nAllowlist/denylist patterns\nSession isolation\n\n\n\n\n\n\nDependency Injection:\n\nConfiguration is passed through callables to support dynamic updates\nBackend factory pattern for LLM providers\n\nEvent-Driven Architecture:\n\nConversation events (user messages, assistant responses, tool calls)\nStreaming events for real-time updates\n\nMiddleware Pattern:\n\nExtensible pipeline for conversation flow control\nBefore/after turn hooks\n\nPlugin System:\n\nSkills for extensible functionality\nMCP servers for external tools\n\nState Management:\n\nImmutable message history\nSession logging and continuation\nContext compaction for long conversations\n\n\n\n\n\n\nUser Input → CLI Entry Point → Agent Loop\nAgent Loop → LLM Backend (for completions) → Message Formatting\nTool Calls → Tool Manager → Tool Execution → Results\nEvents → UI Updates → User Feedback Loop\nSession Data → Session Logger → Persistent Storage\n\nThis architecture provides a flexible, extensible foundation for building a powerful CLI coding assistant with support for multiple workflows, security features, and extensibility through skills and MCP servers.",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#mistral-vibe-tool-system-analysis",
    "href": "tools_study_mistral-vibe.html#mistral-vibe-tool-system-analysis",
    "title": "Mistral vibe 2.0",
    "section": "Mistral Vibe Tool System Analysis",
    "text": "Mistral Vibe Tool System Analysis\nMistral Vibe’s tool system provides a powerful, extensible framework for interacting with the filesystem, running commands, and performing various operations. Each tool is a self-contained unit with its own configuration, state, and security model.",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#built-in-tools",
    "href": "tools_study_mistral-vibe.html#built-in-tools",
    "title": "Mistral vibe 2.0",
    "section": "Built-in Tools",
    "text": "Built-in Tools\n\n1. read_file\nPurpose: Read a UTF-8 file, returning content from a specific line range.\nInput Signature:\nclass ReadFileArgs(BaseModel):\n    path: str\n    offset: int = 0  # Line number to start reading from (0-indexed, inclusive)\n    limit: int | None = None  # Maximum number of lines to read\nOutput Signature:\nclass ReadFileResult(BaseModel):\n    path: str\n    content: str\n    lines_read: int\n    was_truncated: bool = False\nSecurity Mechanisms: 1. Path Validation: - Expands user home directory (~) - Resolves relative paths to absolute - Validates path exists and is a file (not directory) - Checks path is within project directory using path.relative_to(Path.cwd().resolve()) - Raises ToolError with security message if path is outside project\n\nSize Limits:\n\nConfigurable max_read_bytes (default: 64,000 bytes)\nTracks bytes read and stops at limit\nSets was_truncated flag in result\n\nAllowlist/Denylist:\n\nUses fnmatch for pattern matching\nChecks file path against configured patterns\nReturns ToolPermission.ALWAYS or ToolPermission.NEVER\n\nError Handling:\n\nCatches OSError and wraps in ToolError\nValidates input parameters (empty path, negative offset, invalid limit)\n\nState Tracking:\n\nMaintains history of recently read files (max 10)\nHelps prevent infinite loops\n\n\nPrompt:\nUse `read_file` to read the content of a file. It's designed to handle large files safely.\n\n- By default, it reads from the beginning of the file.\n- Use `offset` (line number) and `limit` (number of lines) to read specific parts or chunks of a file. This is efficient for exploring large files.\n- The result includes `was_truncated: true` if the file content was cut short due to size limits.\n\n**Strategy for large files:**\n\n1. Call `read_file` with a `limit` (e.g., 1000 lines) to get the start of the file.\n2. If `was_truncated` is true, you know the file is large.\n3. To read the next chunk, call `read_file` again with an `offset`. For example, `offset=1000, limit=1000`.\n\nThis is more efficient than using `bash` with `cat` or `wc`.\n\n\n\n2. write_file\nPurpose: Create or overwrite a UTF-8 file.\nInput Signature:\nclass WriteFileArgs(BaseModel):\n    path: str\n    content: str\n    overwrite: bool = False  # Must be true to overwrite existing files\nOutput Signature:\nclass WriteFileResult(BaseModel):\n    path: str\n    bytes_written: int\n    file_existed: bool\n    content: str\nSecurity Mechanisms: 1. Path Validation: - Expands user home directory - Resolves relative paths to absolute - Validates path is within project directory - Raises ToolError if path is outside project\n\nOverwrite Protection:\n\nDefault overwrite=False prevents accidental overwrites\nExplicit overwrite=True required to replace existing files\nValidates file existence before writing\n\nSize Limits:\n\nConfigurable max_write_bytes (default: 64,000 bytes)\nRejects content exceeding limit\n\nDirectory Creation:\n\nConfigurable create_parent_dirs (default: True)\nCreates parent directories if enabled\nValidates parent directory exists if disabled\n\nAllowlist/Denylist:\n\nUses fnmatch for pattern matching\nChecks file path against configured patterns\n\nError Handling:\n\nValidates non-empty path\nCatches OSError and wraps in ToolError\nValidates content size before writing\n\n\nPrompt:\nUse `write_file` to write content to a file.\n\n**Arguments:**\n- `path`: The file path (relative or absolute)\n- `content`: The content to write to the file\n- `overwrite`: Must be set to `true` to overwrite an existing file (default: `false`)\n\n**IMPORTANT SAFETY RULES:**\n\n- By default, the tool will **fail if the file already exists** to prevent accidental data loss\n- To **overwrite** an existing file, you **MUST** set `overwrite: true`\n- To **create a new file**, just provide the `path` and `content` (overwrite defaults to false)\n- If parent directories don't exist, they will be created automatically\n\n**BEST PRACTICES:**\n\n- **ALWAYS** use the `read_file` tool first before overwriting an existing file to understand its current contents\n- **ALWAYS** prefer using `search_replace` to edit existing files rather than overwriting them completely\n- **NEVER** write new files unless explicitly required - prefer modifying existing files\n- **NEVER** proactively create documentation files (*.md) or README files unless explicitly requested\n- **AVOID** using emojis in file content unless the user explicitly requests them\n\n**Usage Examples:**\n\n``python\n# Create a new file (will error if file exists)\nwrite_file(\n    path=\"src/new_module.py\",\n    content=\"def hello():\\n    return 'Hello World'\"\n)\n\n# Overwrite an existing file (must read it first!)\n# First: read_file(path=\"src/existing.py\")\n# Then:\nwrite_file(\n    path=\"src/existing.py\",\n    content=\"# Updated content\\ndef new_function():\\n    pass\",\n    overwrite=True\n)\n``\n\n**Remember:** For editing existing files, prefer `search_replace` over `write_file` to preserve unchanged portions and avoid accidental data loss.\n\n\n\n3. search_replace\nPurpose: Replace sections of files using SEARCH/REPLACE blocks.\nInput Signature:\nclass SearchReplaceArgs(BaseModel):\n    file_path: str\n    content: str  # Contains SEARCH/REPLACE blocks\nOutput Signature:\nclass SearchReplaceResult(BaseModel):\n    file: str\n    blocks_applied: int\n    lines_changed: int\n    content: str\n    warnings: list[str] = []\nSecurity Mechanisms: 1. Path Validation: - Validates file exists and is a file (not directory) - Resolves relative paths to absolute - Checks path is within project directory\n\nContent Validation:\n\nValidates non-empty content\nConfigurable max_content_size (default: 100,000 bytes)\nParses SEARCH/REPLACE blocks with regex\nValidates block format and content\n\nBackup Support:\n\nConfigurable create_backup (default: False)\nCreates .bak files when enabled\n\nFuzzy Matching:\n\nConfigurable fuzzy_threshold (default: 0.9)\nProvides context when search text not found\nShows unified diff of closest match\n\nError Handling:\n\nDetailed error messages with context\nShows line numbers and surrounding content\nWarns about multiple occurrences\nHandles Unicode decode errors\nPermission error handling\n\n\nPrompt\nUse `search_replace` to make targeted changes to files using SEARCH/REPLACE blocks. This tool finds exact text matches and replaces them.\n\nArguments:\n- `file_path`: The path to the file to modify\n- `content`: The SEARCH/REPLACE blocks defining the changes\n\nThe content format is:\n\n``\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; SEARCH\n[exact text to find in the file]\n=======\n[exact text to replace it with]\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; REPLACE\n``\n\nYou can include multiple SEARCH/REPLACE blocks to make multiple changes to the same file:\n\n``\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; SEARCH\ndef old_function():\n    return \"old value\"\n=======\ndef new_function():\n    return \"new value\"\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; REPLACE\n\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; SEARCH\nimport os\n=======\nimport os\nimport sys\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; REPLACE\n``\n\nIMPORTANT:\n\n- The SEARCH text must match EXACTLY (including whitespace, indentation, and line endings)\n- The SEARCH text must appear exactly once in the file - if it appears multiple times, the tool will error\n- Use at least 5 equals signs (=====) between SEARCH and REPLACE sections\n- The tool will provide detailed error messages showing context if search text is not found\n- Each search/replace block is applied in order, so later blocks see the results of earlier ones\n- Be careful with escape sequences in string literals - use \\n not \\\\n for newlines in code\n\n\n\n4. bash\nPurpose: Run one-off bash commands and capture output.\nInput Signature:\nclass BashArgs(BaseModel):\n    command: str\n    timeout: int | None = None  # Override default timeout\nOutput Signature:\nclass BashResult(BaseModel):\n    command: str\n    stdout: str\n    stderr: str\n    returncode: int\nSecurity Mechanisms: 1. Command Analysis: - Uses tree-sitter to parse bash commands - Extracts individual commands from compound statements - Analyzes command structure for security\n\nAllowlist/Denylist:\n\nDefault allowlist: safe commands (echo, find, git, etc.)\nDefault denylist: interactive shells, editors, debuggers\nDenylist for standalone commands (python, bash, etc.)\nPattern matching using startswith()\n\nProcess Isolation:\n\nCreates new process group (start_new_session on Unix)\nSets environment variables to prevent interaction:\n\nCI=true, NONINTERACTIVE=1, NO_TTY=1\nTERM=dumb, DEBIAN_FRONTEND=noninteractive\nDisables pagers (PAGER=cat, GIT_PAGER=cat)\n\n\nTimeout Enforcement:\n\nConfigurable default timeout (300 seconds)\nUses asyncio.wait_for() to kill hanging processes\nProcess tree killing on timeout\nDifferent methods for Windows vs Unix\n\nOutput Limits:\n\nConfigurable max_output_bytes (default: 16,000 bytes)\nTruncates stdout/stderr at limit\n\nError Handling:\n\nNon-zero return codes raise ToolError\nTimeout errors are caught and wrapped\nProcess cleanup in finally block\nEncoding handling for Windows\n\n\nPrompt\nUse the `bash` tool to run one-off shell commands.\n\n**Key characteristics:**\n- **Stateless**: Each command runs independently in a fresh environment\n\n**Timeout:**\n- The `timeout` argument controls how long the command can run before being killed\n- When `timeout` is not specified (or set to `None`), the config default is used\n- If a command is timing out, do not hesitate to increase the timeout using the `timeout` argument\n\n**IMPORTANT: Use dedicated tools if available instead of these bash commands:**\n\n**File Operations - DO NOT USE:**\n- `cat filename` → Use `read_file(path=\"filename\")`\n- `head -n 20 filename` → Use `read_file(path=\"filename\", limit=20)`\n- `tail -n 20 filename` → Read with offset: `read_file(path=\"filename\", offset=&lt;line_number&gt;, limit=20)`\n- `sed -n '100,200p' filename` → Use `read_file(path=\"filename\", offset=99, limit=101)`\n- `less`, `more`, `vim`, `nano` → Use `read_file` with offset/limit for navigation\n- `echo \"content\" &gt; file` → Use `write_file(path=\"file\", content=\"content\")`\n- `echo \"content\" &gt;&gt; file` → Read first, then `write_file` with overwrite=true\n\n**Search Operations - DO NOT USE:**\n- `grep -r \"pattern\" .` → Use `grep(pattern=\"pattern\", path=\".\")`\n- `find . -name \"*.py\"` → Use `bash(\"ls -la\")` for current dir or `grep` with appropriate pattern\n- `ag`, `ack`, `rg` commands → Use the `grep` tool\n- `locate` → Use `grep` tool\n\n**File Modification - DO NOT USE:**\n- `sed -i 's/old/new/g' file` → Use `search_replace` tool\n- `awk` for file editing → Use `search_replace` tool\n- Any in-place file editing → Use `search_replace` tool\n\n**APPROPRIATE bash uses:**\n- System information: `pwd`, `whoami`, `date`, `uname -a`\n- Directory listings: `ls -la`, `tree` (if available)\n- Git operations: `git status`, `git log --oneline -10`, `git diff`\n- Process info: `ps aux | grep process`, `top -n 1`\n- Network checks: `ping -c 1 google.com`, `curl -I https://example.com`\n- Package management: `pip list`, `npm list`\n- Environment checks: `env | grep VAR`, `which python`\n- File metadata: `stat filename`, `file filename`, `wc -l filename`\n\n**Example: Reading a large file efficiently**\n\nWRONG:\n``bash\nbash(\"cat large_file.txt\")  # May hit size limits\nbash(\"head -1000 large_file.txt\")  # Inefficient\n``\n\nRIGHT:\n``python\n# First chunk\nread_file(path=\"large_file.txt\", limit=1000)\n# If was_truncated=true, read next chunk\nread_file(path=\"large_file.txt\", offset=1000, limit=1000)\n``\n\n**Example: Searching for patterns**\n\nWRONG:\n``bash\nbash(\"grep -r 'TODO' src/\")  # Don't use bash for grep\nbash(\"find . -type f -name '*.py' | xargs grep 'import'\")  # Too complex\n``\n\nRIGHT:\n``python\ngrep(pattern=\"TODO\", path=\"src/\")\ngrep(pattern=\"import\", path=\".\")\n``\n\n**Remember:** Bash is best for quick system checks and git operations. For file operations, searching, and editing, always use the dedicated tools when they are available.\n\n\n\n5. grep\nPurpose: Recursively search files for a regex pattern.\nInput Signature:\nclass GrepArgs(BaseModel):\n    pattern: str\n    path: str = \".\"\n    max_matches: int | None = None\n    use_default_ignore: bool = True\nOutput Signature:\nclass GrepResult(BaseModel):\n    matches: str\n    match_count: int\n    was_truncated: bool = False\nSecurity Mechanisms: 1. Backend Detection: - Prefers ripgrep (rg) over grep - Falls back to GNU grep if rg not available - Raises error if neither is installed\n\nExclusion Patterns:\n\nDefault exclusion list for common directories:\n\n.venv/, venv/, .env/, env/\nnode_modules/, .git/, __pycache__/\nCache directories, build directories, IDE files\nBinary files, system files\n\nLoads additional patterns from .vibeignore\nRespects .gitignore and .ignore files\n\nOutput Limits:\n\nConfigurable max_output_bytes (default: 64,000 bytes)\nConfigurable default_max_matches (default: 100)\nTracks truncation state\n\nTimeout:\n\nConfigurable default_timeout (default: 60 seconds)\nKills process on timeout\n\nPath Validation:\n\nValidates search path exists\nExpands relative paths\n\n\nPrompt\nUse `grep` to recursively search for a regular expression pattern in files.\n\n- It's very fast and automatically ignores files that you should not read like .pyc files, .venv directories, etc.\n- Use this to find where functions are defined, how variables are used, or to locate specific error messages.\n\n\n\n6. todo\nPurpose: Manage a simple task list.\nInput Signature:\nclass TodoArgs(BaseModel):\n    action: str  # \"read\" or \"write\"\n    todos: list[TodoItem] | None = None\nOutput Signature:\nclass TodoResult(BaseModel):\n    message: str\n    todos: list[TodoItem]\n    total_count: int\nSecurity Mechanisms: 1. State Isolation: - Maintains separate state per tool instance - No external filesystem access - No command execution\n\nLimit Enforcement:\n\nConfigurable max_todos (default: 100)\nValidates todo count on write\n\nData Validation:\n\nValidates unique IDs\nValidates status and priority values\nPydantic model validation\n\nPermission:\n\nDefault permission: ToolPermission.ALWAYS\nNo sensitive operations\n\n\nPrompt\nUse the `todo` tool to manage a simple task list. This tool helps you track tasks and their progress.\n\n## How it works\n\n- **Reading:** Use `action: \"read\"` to view the current todo list\n- **Writing:** Use `action: \"write\"` with the complete `todos` list to update. You must provide the ENTIRE list - this replaces everything.\n\n## Todo Structure\nEach todo item has:\n- `id`: A unique string identifier (e.g., \"1\", \"2\", \"task-a\")\n- `content`: The task description\n- `status`: One of: \"pending\", \"in_progress\", \"completed\", \"cancelled\"\n- `priority`: One of: \"high\", \"medium\", \"low\"\n\n## When to Use This Tool\n\n**Use proactively for:**\n- Complex multi-step tasks (3+ distinct steps)\n- Non-trivial tasks requiring careful planning\n- Multiple tasks provided by the user (numbered or comma-separated)\n- Tracking progress on ongoing work\n- After receiving new instructions - immediately capture requirements\n- When starting work - mark task as in_progress BEFORE beginning\n- After completing work - mark as completed and add any follow-up tasks discovered\n\n**Skip this tool for:**\n- Single, straightforward tasks\n- Trivial operations (&lt; 3 simple steps)\n- Purely conversational or informational requests\n- Tasks that provide no organizational benefit\n\n## Task Management Best Practices\n\n1. **Status Management:**\n   - Only ONE task should be `in_progress` at a time\n   - Mark tasks `in_progress` BEFORE starting work on them\n   - Mark tasks `completed` IMMEDIATELY after finishing\n   - Keep tasks `in_progress` if blocked or encountering errors\n\n2. **Task Completion Rules:**\n   - ONLY mark as `completed` when FULLY accomplished\n   - Never mark complete if tests are failing, implementation is partial, or errors are unresolved\n   - When blocked, create a new task describing what needs resolution\n\n3. **Task Organization:**\n   - Create specific, actionable items\n   - Break complex tasks into manageable steps\n   - Use clear, descriptive task names\n   - Remove irrelevant tasks entirely (don't just mark cancelled)\n\n## Examples\n\n**Example 1: Reading todos**\n``json\n{\n  \"action\": \"read\"\n}\n``\n\n**Example 2: Initial task creation (user requests multiple features)**\n``json\n{\n  \"action\": \"write\",\n  \"todos\": [\n    {\n      \"id\": \"1\",\n      \"content\": \"Add dark mode toggle to settings\",\n      \"status\": \"pending\",\n      \"priority\": \"high\"\n    },\n    {\n      \"id\": \"2\",\n      \"content\": \"Implement theme context/state management\",\n      \"status\": \"pending\",\n      \"priority\": \"high\"\n    },\n    {\n      \"id\": \"3\",\n      \"content\": \"Update components for theme switching\",\n      \"status\": \"pending\",\n      \"priority\": \"medium\"\n    },\n    {\n      \"id\": \"4\",\n      \"content\": \"Run tests and verify build\",\n      \"status\": \"pending\",\n      \"priority\": \"medium\"\n    }\n  ]\n}\n``\n\n**Example 3: Starting work (marking one task in_progress)**\n``json\n{\n  \"action\": \"write\",\n  \"todos\": [\n    {\n      \"id\": \"1\",\n      \"content\": \"Add dark mode toggle to settings\",\n      \"status\": \"in_progress\",\n      \"priority\": \"high\"\n    },\n    {\n      \"id\": \"2\",\n      \"content\": \"Implement theme context/state management\",\n      \"status\": \"pending\",\n      \"priority\": \"high\"\n    },\n    {\n      \"id\": \"3\",\n      \"content\": \"Update components for theme switching\",\n      \"status\": \"pending\",\n      \"priority\": \"medium\"\n    },\n    {\n      \"id\": \"4\",\n      \"content\": \"Run tests and verify build\",\n      \"status\": \"pending\",\n      \"priority\": \"medium\"\n    }\n  ]\n}\n``\n\n**Example 4: Completing task and adding discovered subtask**\n``json\n{\n  \"action\": \"write\",\n  \"todos\": [\n    {\n      \"id\": \"1\",\n      \"content\": \"Add dark mode toggle to settings\",\n      \"status\": \"completed\",\n      \"priority\": \"high\"\n    },\n    {\n      \"id\": \"2\",\n      \"content\": \"Implement theme context/state management\",\n      \"status\": \"in_progress\",\n      \"priority\": \"high\"\n    },\n    {\n      \"id\": \"3\",\n      \"content\": \"Update components for theme switching\",\n      \"status\": \"pending\",\n      \"priority\": \"medium\"\n    },\n    {\n      \"id\": \"4\",\n      \"content\": \"Fix TypeScript errors in theme types\",\n      \"status\": \"pending\",\n      \"priority\": \"high\"\n    },\n    {\n      \"id\": \"5\",\n      \"content\": \"Run tests and verify build\",\n      \"status\": \"pending\",\n      \"priority\": \"medium\"\n    }\n  ]\n}\n``\n\n**Example 5: Handling blockers (keeping task in_progress)**\n``json\n{\n  \"action\": \"write\",\n  \"todos\": [\n    {\n      \"id\": \"1\",\n      \"content\": \"Deploy to production\",\n      \"status\": \"in_progress\",\n      \"priority\": \"high\"\n    },\n    {\n      \"id\": \"2\",\n      \"content\": \"BLOCKER: Fix failing deployment pipeline\",\n      \"status\": \"pending\",\n      \"priority\": \"high\"\n    },\n    {\n      \"id\": \"3\",\n      \"content\": \"Update documentation\",\n      \"status\": \"pending\",\n      \"priority\": \"low\"\n    }\n  ]\n}\n``\n\n## Common Scenarios\n\n**Multi-file refactoring:** Create todos for each file that needs updating\n**Performance optimization:** List specific bottlenecks as individual tasks\n**Bug fixing:** Track reproduction, diagnosis, fix, and verification as separate tasks\n**Feature implementation:** Break down into UI, logic, tests, and documentation tasks\n\nRemember: When writing, you must include ALL todos you want to keep. Any todo not in the list will be removed. Be proactive with task management to demonstrate thoroughness and ensure all requirements are completed successfully.\n\n\n\n7. ask_user_question\nPurpose: Ask the user one or more questions and wait for responses.\nInput Signature:\nclass AskUserQuestionArgs(BaseModel):\n    questions: list[Question]  # 1-4 questions\nOutput Signature:\nclass AskUserQuestionResult(BaseModel):\n    answers: list[Answer]\n    cancelled: bool = False\nSecurity Mechanisms: 1. Context Validation: - Requires InvokeContext with user_input_callback - Fails if not in interactive UI - Prevents use in non-interactive contexts\n\nInput Validation:\n\nValidates question count (1-4)\nValidates option count (2-4 per question)\nValidates header length (max 12 characters)\nPydantic model validation\n\nNo External Access:\n\nNo filesystem operations\nNo command execution\nPure UI interaction\n\nPermission:\n\nDefault permission: ToolPermission.ALWAYS\nSafe, read-only operation\n\n\nPrompt: See vibe/core/tools/builtins/prompts/ask_user_question.md\n\n\n\n8. task\nPurpose: Delegate work to a subagent for independent execution.\nInput Signature:\nclass TaskArgs(BaseModel):\n    task: str\n    agent: str = \"explore\"  # Must be a subagent\nOutput Signature:\nclass TaskResult(BaseModel):\n    response: str\n    turns_used: int\n    completed: bool\nSecurity Mechanisms: 1. Agent Type Validation: - Validates agent exists - Checks agent_type == AgentType.SUBAGENT - Prevents recursive spawning of regular agents - Prevents spawning interactive agents\n\nIsolation:\n\nCreates separate AgentLoop instance\nDisables session logging for subagents\nSeparate configuration and state\n\nResource Limits:\n\nTracks turns used\nDetects interruption/completion\nLimits output accumulation\n\nPermission:\n\nDefault permission: ToolPermission.ASK\nRequires explicit approval\n\n\nPrompt\nUse `ask_user_question` to gather information from the user when you need clarification, want to validate assumptions, or need help making a decision. **Don't hesitate to use this tool** - it's better to ask than to guess wrong.\n\n## When to Use\n\n- **Clarifying requirements**: Ambiguous instructions, unclear scope\n- **Technical decisions**: Architecture choices, library selection, tradeoffs\n- **Preference gathering**: UI style, naming conventions, approach options\n- **Validation**: Confirming understanding before starting significant work\n- **Multiple valid paths**: When several approaches could work and you want user input\n\n## Question Structure\n\nEach question has these fields:\n\n- `question`: The full question text (be specific and clear)\n- `header`: A short label displayed as a chip (max 12 characters, e.g., \"Auth\", \"Database\", \"Approach\")\n- `options`: 2-4 choices (an \"Other\" option is automatically added for free text)\n- `multi_select`: Set to `true` if user can pick multiple options (default: `false`)\n\n### Options Structure\n\nEach option has:\n- `label`: Short display text (1-5 words)\n- `description`: Brief explanation of what this choice means or its implications\n\n## Examples\n\n**Single question with recommended option:**\n``json\n{\n  \"questions\": [{\n    \"question\": \"Which authentication method should we use?\",\n    \"header\": \"Auth\",\n    \"options\": [\n      {\"label\": \"JWT tokens (Recommended)\", \"description\": \"Stateless, scalable, works well with APIs\"},\n      {\"label\": \"Session cookies\", \"description\": \"Traditional approach, requires session storage\"},\n      {\"label\": \"OAuth 2.0\", \"description\": \"Third-party auth, more complex setup\"}\n    ],\n    \"multi_select\": false\n  }]\n}\n``\n\n**Multiple questions (displayed as tabs):**\n``json\n{\n  \"questions\": [\n    {\n      \"question\": \"Which database should we use?\",\n      \"header\": \"Database\",\n      \"options\": [\n        {\"label\": \"PostgreSQL\", \"description\": \"Relational, ACID compliant\"},\n        {\"label\": \"MongoDB\", \"description\": \"Document store, flexible schema\"}\n      ],\n      \"multi_select\": false\n    },\n    {\n      \"question\": \"Which features should be included in v1?\",\n      \"header\": \"Features\",\n      \"options\": [\n        {\"label\": \"User auth\", \"description\": \"Login, signup, password reset\"},\n        {\"label\": \"Search\", \"description\": \"Full-text search across content\"},\n        {\"label\": \"Export\", \"description\": \"CSV and PDF export\"}\n      ],\n      \"multi_select\": true\n    }\n  ]\n}\n``\n\n## Key Constraints\n\n- **Header max length**: 12 characters (keeps UI clean)\n- **Options count**: 2-4 per question (plus automatic \"Other\")\n- **Questions count**: 1-4 per call\n- **Label length**: Keep to 1-5 words for readability\n\n## Tips\n\n1. **Put recommended option first** and add \"(Recommended)\" to its label\n2. **Use descriptive headers** that categorize the question type\n3. **Keep descriptions concise** but informative about tradeoffs\n4. **Use multi_select** when choices aren't mutually exclusive (e.g., features to include)\n5. **Ask early** - it's better to clarify before starting than to redo work",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#base-tool-security-framework",
    "href": "tools_study_mistral-vibe.html#base-tool-security-framework",
    "title": "Mistral vibe 2.0",
    "section": "Base Tool Security Framework",
    "text": "Base Tool Security Framework\n\nBaseTool Class\nAll tools inherit from BaseTool which provides:\n\nType Safety:\n\nGeneric types for arguments, results, config, and state\nPydantic model validation\nType extraction from annotations\n\nConfiguration:\n\nBaseToolConfig with permission model\nToolPermission enum (ALWAYS, NEVER, ASK)\nAllowlist/denylist patterns\n\nState Management:\n\nBaseToolState for persistent tool state\nSeparate state per tool instance\n\n\n\n\nToolPermission System\nThree permission levels: - ALWAYS: Tool executes without approval - NEVER: Tool is permanently disabled - ASK: User must approve each execution\nPermission Check Flow: 1. Tool calls check_allowlist_denylist(args) 2. If returns ALWAYS or NEVER, use that permission 3. Otherwise, use config permission 4. Agent loop applies final approval logic\n\n\nInvokeContext\nProvides execution context: - tool_call_id: Unique identifier for this invocation - approval_callback: Function to request user approval - agent_manager: Access to agent system - user_input_callback: Function to ask user questions",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#security-best-practices",
    "href": "tools_study_mistral-vibe.html#security-best-practices",
    "title": "Mistral vibe 2.0",
    "section": "Security Best Practices",
    "text": "Security Best Practices\n\nPath Security\n\nAlways resolve relative paths to absolute\nValidate paths are within project directory\nUse path.relative_to() for containment checks\nNever allow .. to escape project root\n\n\n\nCommand Security\n\nParse and analyze command structure\nUse allowlist/denylist patterns\nIsolate processes (new session/group)\nSet safe environment variables\nEnforce timeouts\nLimit output size\n\n\n\nFile Security\n\nValidate file existence and type\nEnforce size limits\nRequire explicit overwrite for existing files\nCreate backups when appropriate\nHandle encoding errors gracefully\n\n\n\nProcess Security\n\nKill process trees on timeout/interrupt\nClean up resources in finally blocks\nHandle platform differences (Windows vs Unix)\nUse asyncio for process management\n\n\n\nData Security\n\nValidate all inputs with Pydantic models\nEnforce limits on content size\nSanitize output (truncate when needed)\nNever expose sensitive information in errors",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#summary",
    "href": "tools_study_mistral-vibe.html#summary",
    "title": "Mistral vibe 2.0",
    "section": "Summary",
    "text": "Summary\nMistral Vibe’s tool system implements a comprehensive security model:\n\nDefense in Depth: Multiple layers of validation and checking\nLeast Privilege: Tools have minimal required permissions\nExplicit Approval: Most tools require user approval\nInput Validation: Strict validation of all parameters\nOutput Limits: Prevents resource exhaustion\nProcess Isolation: Safe command execution\nPath Containment: Prevents directory traversal\nError Handling: Graceful degradation and recovery\n\nEach tool follows the same security patterns while adapting them to the specific requirements of its operation.",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#mistral-vibe-tool-system-comprehensive-security-analysis",
    "href": "tools_study_mistral-vibe.html#mistral-vibe-tool-system-comprehensive-security-analysis",
    "title": "Mistral vibe 2.0",
    "section": "Mistral Vibe Tool System: Comprehensive Security Analysis",
    "text": "Mistral Vibe Tool System: Comprehensive Security Analysis\nI’ve completed an in-depth exploration of Mistral Vibe’s tool system, documenting each built-in tool, its security mechanisms, and the overall architecture. Here’s a comprehensive summary:\n\nBuilt-in Tools Overview\nMistral Vibe provides 8 built-in tools, each with sophisticated security mechanisms:\n\nread_file - Safe file reading with byte limits and path validation\nwrite_file - Protected file writing with overwrite confirmation\nsearch_replace - Precise file editing with backup support\nbash - Secure command execution with allowlist/denylist and process isolation\ngrep - Safe code searching with exclusion patterns\ntodo - Task management with state isolation\nask_user_question - Interactive user queries with context validation\ntask - Subagent delegation with agent type validation\n\n\n\nKey Security Features\n1. Path Security - All tools validate paths are within the project directory using path.relative_to() - Prevents directory traversal attacks - Expands and resolves paths consistently\n2. Permission System - Three-level permission model: ALWAYS, NEVER, ASK - Allowlist/denylist patterns using fnmatch - Configurable per-tool permissions - Explicit user approval for most operations\n3. Resource Limits - Byte limits for file operations (64KB-100KB) - Output limits for commands (16KB-64KB) - Match limits for searches (100 matches) - Timeout enforcement (60-300 seconds)\n4. Process Isolation - Bash commands run in isolated process groups - Safe environment variables (CI=true, NONINTERACTIVE=1) - Process tree killing on timeout - Platform-specific handling (Windows vs Unix)\n5. Input Validation - Pydantic model validation for all parameters - Strict type checking and constraints - Parameter range validation - Empty/null checking\n6. Error Handling - Graceful error recovery - Detailed error messages with context - Wrapping of low-level errors - Prevention of information leakage\n\n\nSecurity Architecture\nThe tool system implements a defense-in-depth approach:\n\nBaseTool Framework - Provides type safety, configuration, and state management\nPermission System - Three-tier approval model with allowlist/denylist\nInvokeContext - Execution context with approval callbacks\nToolManager - Centralized tool discovery and instantiation\nAgent Loop - Orchestrates tool execution with middleware\n\n\n\nMost Important Security Aspects\nPath Containment: The most critical security feature is path validation. All file operations check that paths remain within the project directory using path.relative_to(Path.cwd().resolve()), preventing directory traversal attacks.\nProcess Isolation: For command execution, the system uses tree-sitter to parse commands, sets safe environment variables, creates isolated process groups, and enforces strict timeouts to prevent command injection and resource exhaustion.\nPermission Model: The three-level permission system (ALWAYS/NEVER/ASK) with allowlist/denylist support provides fine-grained control over tool execution, requiring explicit user approval for most operations.\nResource Limits: Byte limits, output limits, and timeouts prevent resource exhaustion attacks, ensuring the system remains responsive even with malicious input.\nThe documentation file mistral-vibe-tool-system-analysis.md provides a detailed, comprehensive analysis of each tool’s security mechanisms, input/output signatures, and implementation details.\n\n\nSystem Prompt Construction\nThe system prompt is constructed by the get_universal_system_prompt() function in vibe/core/system_prompt.py. It combines multiple components:\n\nBase System Prompt (vibe/core/prompts/cli.md)\n\nGeneral guidelines for the AI assistant\nTool usage best practices\nCode modification guidelines\nTone and style requirements\n\nOptional Components (controlled by config flags)\n\nCommit signature instructions\nModel information\nPlatform-specific information (OS, shell)\nTool-specific prompts\nAvailable skills\nAvailable subagents\n\nProject Context (if enabled)\n\nDirectory structure with limits\nGit status and recent commits\nProject documentation\n\n\n\n\nTool Integration\nTools are integrated in two ways:\n\n1. Tool Prompts in System Message\nEach tool has an associated .md file in vibe/core/tools/builtins/prompts/ that provides detailed usage guidelines. These are loaded when config.include_prompt_detail is True.\nTool Prompts Include: - read_file.md - File reading best practices - write_file.md - File writing safety rules - search_replace.md - Search/replace block format - bash.md - Command execution guidelines - grep.md - Search operation best practices - todo.md - Task management guidelines - ask_user_question.md - User interaction patterns - task.md - Subagent delegation best practices\n\n\n2. Tool Schemas in LLM API Calls\nThe APIToolFormatHandler class provides tool schemas to the LLM:\ndef get_available_tools(self, tool_manager: ToolManager) -&gt; list[AvailableTool]:\n    return [\n        AvailableTool(\n            function=AvailableFunction(\n                name=tool_class.get_name(),\n                description=tool_class.description,\n                parameters=tool_class.get_parameters(),\n            )\n        )\n        for tool_class in tool_manager.available_tools.values()\n    ]\nTool Schema Components: - Name: Derived from class name (e.g., ReadFile → read_file) - Description: Class variable description - Parameters: JSON schema from Pydantic model\n\n\n\nKey Integration Points\n\nget_universal_system_prompt() - Builds the complete system prompt\nBaseTool.get_tool_prompt() - Loads tool-specific documentation\nAPIToolFormatHandler.get_available_tools() - Provides tool schemas to LLM\nToolManager - Discovers and manages available tools\n\n\n\nTool Execution Flow\n\nLLM receives system prompt + tool schemas\nLLM generates tool calls in API format\nparse_message() extracts tool calls\nresolve_tool_calls() validates tool names and arguments\nAgent loop executes tools with permission checks\nResults are formatted and added to conversation\n\n\n\nMulti-Layered Architecture\nThe system uses a multi-layered approach:\n\nGuidance Layer: System prompt with best practices\nDocumentation Layer: Tool-specific prompts for complex operations\nAPI Layer: Tool schemas for function calling\nContext Layer: Project-specific information\n\nThis separation allows the LLM to understand both the “how” (guidelines) and the “what” (API structure) of tool usage, resulting in more accurate and safe tool execution.\nThe documentation file system-prompt-and-tool-integration.md provides a complete, detailed explanation of the construction process and integration mechanisms.\nThe ToolManager discovers tools through a multi-step process:",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#search-path-discovery",
    "href": "tools_study_mistral-vibe.html#search-path-discovery",
    "title": "Mistral vibe 2.0",
    "section": "1. Search Path Discovery",
    "text": "1. Search Path Discovery\nThe manager first computes search paths from configuration: - Default tool directory (DEFAULT_TOOL_DIR.path) - User-configured tool paths (config.tool_paths) - Local tools directory (discovered from current working directory) - Global tools directory (GLOBAL_TOOLS_DIR.path)\nThese paths are deduplicated to avoid scanning the same location multiple times.",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#tool-class-discovery",
    "href": "tools_study_mistral-vibe.html#tool-class-discovery",
    "title": "Mistral vibe 2.0",
    "section": "2. Tool Class Discovery",
    "text": "2. Tool Class Discovery\nThe _iter_tool_classes() method recursively scans all search paths:\n@staticmethod\ndef _iter_tool_classes(search_paths: list[Path]) -&gt; Iterator[type[BaseTool]]:\n    \"\"\"Iterate over all search_paths to find tool classes.\"\"\"\n    for base in search_paths:\n        if not base.is_dir() and base.name.endswith(\".py\"):\n            if tools := ToolManager._load_tools_from_file(base):\n                for tool in tools:\n                    yield tool\n\n        for path in base.rglob(\"*.py\"):\n            if tools := ToolManager._load_tools_from_file(path):\n                for tool in tools:\n                    yield tool\nThis method: - Handles both directories and individual .py files - Recursively scans directories with rglob(\"*.py\") - Uses _load_tools_from_file() to extract tool classes from each Python file",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#tool-loading-from-files",
    "href": "tools_study_mistral-vibe.html#tool-loading-from-files",
    "title": "Mistral vibe 2.0",
    "section": "3. Tool Loading from Files",
    "text": "3. Tool Loading from Files\nThe _load_tools_from_file() method: - Skips files starting with _ (private modules) - Computes a module name (canonical for vibe package files, hash-based for external files) - Dynamically imports the module using importlib.util - Inspects the module’s namespace for classes that: - Are subclasses of BaseTool - Are not abstract - Are not the BaseTool class itself",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#mcp-integration",
    "href": "tools_study_mistral-vibe.html#mcp-integration",
    "title": "Mistral vibe 2.0",
    "section": "4. MCP Integration",
    "text": "4. MCP Integration\nAdditionally, the manager integrates MCP (Model Context Protocol) tools: - HTTP-based MCP servers - Stdio-based MCP servers - Each MCP server is queried for available tools - Remote tools are wrapped in proxy classes that inherit from BaseTool",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#filtering-and-availability",
    "href": "tools_study_mistral-vibe.html#filtering-and-availability",
    "title": "Mistral vibe 2.0",
    "section": "5. Filtering and Availability",
    "text": "5. Filtering and Availability\nAfter discovery, tools are filtered based on configuration: - enabled_tools: Only tools matching these patterns are available - disabled_tools: Tools matching these patterns are excluded - If neither is specified, all discovered tools are available\nThe discovered tools are stored in the _available dictionary with tool names as keys and tool classes as values.\nThis comprehensive discovery mechanism allows the ToolManager to find tools from multiple sources: built-in tools, user-provided tools, local project tools, and MCP servers.",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#default-search-paths",
    "href": "tools_study_mistral-vibe.html#default-search-paths",
    "title": "Mistral vibe 2.0",
    "section": "Default Search Paths",
    "text": "Default Search Paths\nThe default search paths for tool discovery are:\n\nDEFAULT_TOOL_DIR: VIBE_ROOT / \"core\" / \"tools\" / \"builtins\"\n\nThis is the built-in tools directory in the vibe package\nContains the 8 built-in tools: ask_user_question, bash, grep, read_file, search_replace, write_file, task, ui\n\nGLOBAL_TOOLS_DIR: VIBE_HOME / \"tools\"\n\nLocated at ~/.vibe/tools (or $VIBE_HOME/tools if VIBE_HOME env var is set)\nUser can place custom tools here for global availability\n\nLocal Tools Directory: Path.cwd() / \".vibe\" / \"tools\"\n\nDiscovered from the current working directory\nOnly available if the current directory is trusted\nAllows project-specific tools\n\nUser-configured paths: Additional paths can be specified in the configuration",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#basetool-class-explanation",
    "href": "tools_study_mistral-vibe.html#basetool-class-explanation",
    "title": "Mistral vibe 2.0",
    "section": "BaseTool Class Explanation",
    "text": "BaseTool Class Explanation\nThe BaseTool class is an abstract base class that defines the interface for all tools in the system. It uses generic type parameters to ensure type safety:\nBaseTool[\n    ToolArgs: BaseModel,      # Input arguments model\n    ToolResult: BaseModel,    # Output result model\n    ToolConfig: BaseToolConfig,  # Configuration model\n    ToolState: BaseToolState,  # State model\n](ABC)\n\nKey Components:\n1. InvokeContext - Contains execution context: tool_call_id, approval_callback, agent_manager, user_input_callback - Passed to tools during invocation for access to system services\n2. ToolPermission - Enum with three values: ALWAYS, NEVER, ASK - Controls whether a tool can be used without user approval - Default is ASK (requires user approval)\n3. BaseToolConfig - Base configuration class with: - permission: ToolPermission level - allowlist: Patterns that automatically allow execution - denylist: Patterns that automatically deny execution - Supports extra fields via extra=\"allow\"\n4. BaseToolState - Base state class for maintaining tool-specific state - Uses Pydantic BaseModel with strict validation\n5. Core Methods:\n\nrun(args, ctx): Abstract method that tools must implement\n\nTakes input arguments and optional context\nReturns an AsyncGenerator yielding ToolStreamEvent or ToolResult\nThis is where the actual tool logic goes\n\ninvoke(ctx, **raw): Validates arguments and runs the tool\n\nValidates input using Pydantic\nHandles errors gracefully\nYields results from the run method\n\nfrom_config(config): Factory method to create tool instances\n\nCreates initial state and returns configured tool instance\n\n\n6. Utility Methods:\n\nget_tool_prompt(): Loads the tool’s prompt file (from prompts/ subdirectory)\nget_name(): Returns the tool name in snake_case format\nget_parameters(): Returns JSON schema for the tool’s arguments\ncheck_allowlist_denylist(): Checks if arguments match allowlist/denylist (can be overridden)\n\n7. Type Extraction:\n\n_get_tool_config_class(): Extracts ToolConfig type from class hierarchy\n_get_tool_state_class(): Extracts ToolState type from class hierarchy\n_get_tool_args_results(): Extracts ToolArgs and ToolResult types from run method signature\n_extract_result_type(): Extracts ToolResult from AsyncGenerator return type\n\n\n\nDesign Patterns:\n\nGeneric Base Class: Uses Python generics to ensure type safety while allowing customization\nPydantic Validation: All arguments, results, config, and state use Pydantic models for validation\nAsync Streaming: Tools yield results asynchronously for better UX\nDependency Injection: Configuration and state are injected at creation time\nType Introspection: Extracts types from method signatures for runtime validation\n\nThe BaseTool class provides a solid foundation for creating type-safe, validated tools with consistent behavior across the system.",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#mcp-server-discovery-and-integration",
    "href": "tools_study_mistral-vibe.html#mcp-server-discovery-and-integration",
    "title": "Mistral vibe 2.0",
    "section": "MCP Server Discovery and Integration",
    "text": "MCP Server Discovery and Integration\n\n1. Configuration Structure\nMCP servers are configured in the VibeConfig with three types:\nMCPServer = Annotated[\n    MCPHttp | MCPStreamableHttp | MCPStdio,\n    Field(discriminator=\"transport\")\n]\nEach MCP server has: - name: Short alias used to prefix tool names - prompt: Optional usage hint - startup_timeout_sec: Timeout for server initialization (default: 10s) - tool_timeout_sec: Timeout for tool execution (default: 60s)\nTransport Types: - HTTP: Remote servers accessed via HTTP - url: Base URL - headers: Additional HTTP headers - api_key_env: Environment variable for API token - api_key_header: HTTP header for the token - api_key_format: Format string for header value\n\nStreamable HTTP: Similar to HTTP but with streaming support\n\nSame fields as HTTP\n\nStdio: Local servers run as subprocesses\n\ncommand: Command to run (string or list)\nargs: Additional arguments\nenv: Environment variables\n\n\n\n\n2. Discovery Process\nThe discovery happens in ToolManager._integrate_mcp():\nasync def _integrate_mcp_async(self) -&gt; None:\n    try:\n        http_count = 0\n        stdio_count = 0\n\n        for srv in self._config.mcp_servers:\n            match srv.transport:\n                case \"http\" | \"streamable-http\":\n                    http_count += await self._register_http_server(srv)\n                case \"stdio\":\n                    stdio_count += await self._register_stdio_server(srv)\n                case _:\n                    logger.warning(\"Unsupported MCP transport: %r\", srv.transport)\n\n        logger.info(\n            \"MCP integration registered %d tools (http=%d, stdio=%d)\",\n            http_count + stdio_count,\n            http_count,\n            stdio_count,\n        )\n    except Exception as exc:\n        logger.warning(\"Failed to integrate MCP tools: %s\", exc)\nFor HTTP/Streamable HTTP servers: 1. Connect to the server URL with optional headers 2. Initialize the MCP client session 3. Call session.list_tools() to get available tools 4. For each remote tool, create a proxy class using create_mcp_http_proxy_tool_class()\nFor Stdio servers: 1. Start the subprocess with the configured command 2. Initialize the MCP client session 3. Call session.list_tools() to get available tools 4. For each remote tool, create a proxy class using create_mcp_stdio_proxy_tool_class()\n\n\n3. Proxy Tool Creation\nTwo factory functions create proxy tool classes:\n\ncreate_mcp_http_proxy_tool_class()\nCreates a BaseTool subclass that wraps an HTTP MCP tool:\ndef create_mcp_http_proxy_tool_class(\n    *,\n    url: str,\n    remote: RemoteTool,\n    alias: str | None = None,\n    server_hint: str | None = None,\n    headers: dict[str, str] | None = None,\n    startup_timeout_sec: float | None = None,\n    tool_timeout_sec: float | None = None,\n) -&gt; type[BaseTool[_OpenArgs, MCPToolResult, BaseToolConfig, BaseToolState]]:\nKey features: - Generates a unique tool name: {alias}_{remote.name} or {host}_{port}_{remote.name} - Stores connection parameters as class variables - Implements run() to call the remote tool via HTTP - Returns MCPToolResult containing the tool execution result - Provides custom display methods for UI integration\n\n\ncreate_mcp_stdio_proxy_tool_class()\nCreates a BaseTool subclass that wraps a stdio MCP tool:\ndef create_mcp_stdio_proxy_tool_class(\n    *,\n    command: list[str],\n    remote: RemoteTool,\n    alias: str | None = None,\n    server_hint: str | None = None,\n    env: dict[str, str] | None = None,\n    startup_timeout_sec: float | None = None,\n    tool_timeout_sec: float | None = None,\n) -&gt; type[BaseTool[_OpenArgs, MCPToolResult, BaseToolConfig, BaseToolState]]:\nKey features: - Generates a unique tool name using command hash for stability - Stores command and environment as class variables - Implements run() to call the remote tool via stdio - Returns MCPToolResult containing the tool execution result - Provides custom display methods for UI integration\n\n\n\n4. Tool Execution Flow\nWhen an MCP proxy tool is invoked:\n\nArgument Validation: Input is validated against the remote tool’s schema\nConnection Setup: HTTP connection or stdio subprocess is established\nTool Call: The remote tool is called with the provided arguments\nResult Parsing: The MCP response is parsed into MCPToolResult\nResult Yielding: The result is yielded as an async generator\n\n\n\n5. Result Handling\nThe MCPToolResult model contains: - ok: Boolean indicating success - server: Server identifier (URL or command) - tool: Tool name - text: Text content (if any) - structured: Structured content (if any)\nResults are displayed in the UI with custom get_call_display() and get_result_display() methods.\n\n\n6. Error Handling\n\nConnection timeouts are handled via startup_timeout_sec and tool_timeout_sec\nFailed calls raise ToolError with descriptive messages\nDiscovery failures are logged but don’t crash the system\nIndividual tool registration failures are logged and skipped\n\n\n\n7. Integration Points\nMCP tools are fully integrated into the tool system: - Discovered tools are added to ToolManager._available - Can be enabled/disabled via enabled_tools/disabled_tools config - Respect the same permission model as built-in tools - Use the same configuration and state management - Appear in tool listings and autocompletion\nThis architecture allows seamless integration of external MCP servers as first-class tools in the Vibe system, with proper type safety, error handling, and UI integration.",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#web-search-not-activated",
    "href": "tools_study_mistral-vibe.html#web-search-not-activated",
    "title": "Mistral vibe 2.0",
    "section": "Web search not activated",
    "text": "Web search not activated\nUsage of extra_headers Parameter in LLM Calls\nThe extra_headers parameter is used in three places in the agent loop to pass additional HTTP headers to the LLM provider:\n\n1. Non-Streaming Completion (_chat() method, line 575-580)\nresult = await backend.complete(\n    model=active_model,\n    messages=self.messages,\n    temperature=active_model.temperature,\n    tools=available_tools,\n    tool_choice=tool_choice,\n    extra_headers={\n        \"user-agent\": get_user_agent(provider.backend),\n        \"x-affinity\": self.session_id,\n    },\n    max_tokens=max_tokens,\n)\n\n\n2. Streaming Completion (_chat_streaming() method, line 619-624)\nasync for chunk in backend.complete_streaming(\n    model=active_model,\n    messages=self.messages,\n    temperature=active_model.temperature,\n    tools=available_tools,\n    tool_choice=tool_choice,\n    extra_headers={\n        \"user-agent\": get_user_agent(provider.backend),\n        \"x-affinity\": self.session_id,\n    },\n    max_tokens=max_tokens,\n):\n\n\n3. Token Counting (_compact_session() method, line 843)\nactual_context_tokens = await backend.count_tokens(\n    model=active_model,\n    messages=self.messages,\n    tools=self.format_handler.get_available_tools(self.tool_manager),\n    extra_headers={\"user-agent\": get_user_agent(provider.backend)},\n)\n\n\nCurrent Headers\nThe system currently sends two headers: 1. user-agent: Identifies the client as “Mistral-Vibe” with version info 2. x-affinity: Contains the session ID for request correlation\n\n\nHow to Extend for Web Search\nTo enable web search capabilities, additional headers could be added to extra_headers. For example:\nextra_headers={\n    \"user-agent\": get_user_agent(provider.backend),\n    \"x-affinity\": self.session_id,\n    \"X-Web-Search\": \"enabled\",  # Provider-specific web search header\n    \"X-Web-Search-Model\": \"web-search-preview\",  # Specific model for web search\n}\n\n\nImplementation Approach\nThe architecture allows for web search to be enabled by: 1. Adding provider-specific headers through the extra_headers parameter 2. These headers are passed to the backend (GenericBackend or MistralBackend) 3. The backend includes them in the HTTP request to the provider 4. The provider handles the web search internally and returns results\nThis mechanism provides a clean way to enable provider-specific capabilities like web search without modifying the core tool system.",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#mistral-vibe-concepts",
    "href": "tools_study_mistral-vibe.html#mistral-vibe-concepts",
    "title": "Mistral vibe 2.0",
    "section": "Mistral vibe concepts",
    "text": "Mistral vibe concepts\n\n1. Agents\n\nMain entities that interact with users\nHave profiles defining behavior, safety level, and tool permissions\nBuilt-in agents: default, plan, accept-edits, auto-approve, explore\nSafety levels: SAFE, NEUTRAL, DESTRUCTIVE, YOLO\n\n\n\n2. Subagents\n\nSpecialized agents for task delegation\nRun independently to prevent context overload\nInvoked using the task tool\nCan be created by setting agent_type = \"subagent\"\n\n\n\n3. Skills\n\nReusable components that extend functionality\nDefined in directories with SKILL.md files\nSupport metadata like name, description, allowed tools\nDiscovered from multiple paths (global, local, custom)\n\n\n\n4. Relationship Between Components\n\nAgents define which tools are available and their permissions\nSkills can add new tools or modify behavior (experimental)\nTools are the actual implementations that perform actions\nSubagents are specialized agents for delegation\n\n\n\n5. Personalization Files\nUsers can create these files to customize Mistral Vibe:\n\nAgent Configuration Files (~/.vibe/agents/*.toml)\nSkill Definition Files (~/.vibe/skills/*/SKILL.md)\nTool Definition Files (~/.vibe/tools/*.py)\nSystem Prompt Files (~/.vibe/prompts/*.md)\nMain Configuration File (~/.vibe/config.toml)\n\n\n\n6. Configuration\n\nSupports pattern matching (exact names, globs, regex)\nConfiguration inheritance: defaults → global config → project config → agent overrides\nTool permissions: always, never, ask\n\nThe document provides step-by-step guides for creating custom agents, skills, and tools, along with complete examples.",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#files-users-can-create-for-personalization",
    "href": "tools_study_mistral-vibe.html#files-users-can-create-for-personalization",
    "title": "Mistral vibe 2.0",
    "section": "Files Users Can Create for Personalization",
    "text": "Files Users Can Create for Personalization\n\n1. Agent Configuration Files (.toml)\n\nLocation: ~/.vibe/agents/ or .vibe/agents/ (project-specific)\nPurpose: Define custom agent profiles with specific tool permissions\nFormat: TOML with agent metadata and overrides\n\n\n\n2. Skill Definition Files (SKILL.md)\n\nLocation: ~/.vibe/skills/*/SKILL.md or .vibe/skills/*/SKILL.md\nPurpose: Define reusable skills with metadata\nFormat: Markdown with YAML frontmatter\n\n\n\n3. Tool Definition Files (.py)\n\nLocation: ~/.vibe/tools/ or .vibe/tools/ (project-specific)\nPurpose: Create custom tools by extending BaseTool\nFormat: Python files with async methods\n\n\n\n4. System Prompt Files (.md)\n\nLocation: ~/.vibe/prompts/\nPurpose: Customize the system prompt for different use cases\nFormat: Markdown files\n\n\n\n5. Main Configuration File (config.toml)\n\nLocation: ~/.vibe/config.toml or .vibe/config.toml\nPurpose: Global configuration for agents, skills, tools, and MCP servers\nFormat: TOML\n\n\n\n6. Documentation Files (Optional)\nWhile not required by the system, users can create: - AGENTS.md - Documentation for custom agents - SKILLS.md - Documentation for custom skills - TOOLS.md - Documentation for custom tools - README.md - Project-specific documentation\nThese documentation files would be in the project’s .vibe/ directory and serve as reference material for team members or future maintainers.\nThe comprehensive document I created (mistral-vibe-agents-skills-personalization.md) explains all of this in detail with examples and step-by-step guides.",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#mistral-vibe-agents-skills-subagents-and-personalization",
    "href": "tools_study_mistral-vibe.html#mistral-vibe-agents-skills-subagents-and-personalization",
    "title": "Mistral vibe 2.0",
    "section": "Mistral Vibe: Agents, Skills, Subagents, and Personalization",
    "text": "Mistral Vibe: Agents, Skills, Subagents, and Personalization\nThis document explains the core concepts of agents, skills, subagents in Mistral Vibe, how they relate to each other and to tools, and how users can personalize their Vibe experience.\n\nAgents\nSubagents\nSkills\nRelationship Between Agents, Skills, and Tools\nPersonalization Files\nCreating Custom Agents\nCreating Custom Skills\nCreating Custom Tools\nConfiguration Overview",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#agents-1",
    "href": "tools_study_mistral-vibe.html#agents-1",
    "title": "Mistral vibe 2.0",
    "section": "Agents",
    "text": "Agents\nAgents are the main entities that interact with users and execute tasks. Each agent has a profile that defines its behavior, safety level, and tool permissions.\n\nAgent Profile Structure\nAn agent profile consists of: - Name: Unique identifier for the agent - Display Name: Human-readable name - Description: What the agent does - Safety Level: One of SAFE, NEUTRAL, DESTRUCTIVE, or YOLO - Agent Type: AGENT (primary) or SUBAGENT (for delegation) - Overrides: Configuration overrides (tools, permissions, etc.)\n\n\nBuilt-in Agents\nMistral Vibe comes with several built-in agents:\n\ndefault - Standard agent requiring approval for tool executions\nplan - Read-only agent for exploration and planning (auto-approves safe tools)\naccept-edits - Auto-approves file edits only\nauto-approve - Auto-approves all tool executions (use with caution)\nexplore - Read-only subagent for codebase exploration\n\n\n\nAgent Safety Levels\n\nSAFE: Read-only operations, no destructive actions\nNEUTRAL: Default level, requires approval for most actions\nDESTRUCTIVE: Can modify files but with restrictions\nYOLO: No restrictions, auto-approves everything\n\n\n\nUsing Agents\nvibe --agent plan\nvibe --agent auto-approve",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#subagents-1",
    "href": "tools_study_mistral-vibe.html#subagents-1",
    "title": "Mistral vibe 2.0",
    "section": "Subagents",
    "text": "Subagents\nSubagents are specialized agents designed for task delegation. They run independently and can perform work without user interaction, preventing context overload.\n\nKey Characteristics\n\nRun in parallel with the main agent\nHave their own configuration and tool permissions\nCan be invoked using the task tool\nUseful for long-running or specialized tasks\n\n\n\nExample: Delegating to a Subagent\n&gt; Can you explore the codebase structure while I work on something else?\n\n🤖 I'll use the task tool to delegate this to the explore subagent.\n\n&gt; task(task=\"Analyze the project structure and architecture\", agent=\"explore\")\n\n\nCreating Custom Subagents\nTo create a custom subagent, add agent_type = \"subagent\" to your agent configuration:\n# ~/.vibe/agents/my-subagent.toml\nname = \"my-subagent\"\ndisplay_name = \"My Subagent\"\ndescription = \"Specialized subagent for my tasks\"\nsafety = \"safe\"\nagent_type = \"subagent\"\n\n[tools.read_file]\npermission = \"always\"\n\n[tools.grep]\npermission = \"always\"",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#skills-1",
    "href": "tools_study_mistral-vibe.html#skills-1",
    "title": "Mistral vibe 2.0",
    "section": "Skills",
    "text": "Skills\nSkills are reusable components that extend Vibe’s functionality. They can add new tools, slash commands, and specialized behaviors.\n\nSkill Structure\nSkills are defined in directories with a SKILL.md file containing YAML frontmatter:\n---\nname: code-review\ndescription: Perform automated code reviews\nlicense: MIT\ncompatibility: Python 3.12+\nuser-invocable: true\nallowed-tools:\n  - read_file\n  - grep\n  - ask_user_question\n---\n\n# Code Review Skill\n\nThis skill helps analyze code quality and suggest improvements.\n\n\nSkill Metadata Fields\n\nname: Skill identifier (lowercase, hyphens only)\ndescription: What the skill does\nlicense: License name or reference\ncompatibility: Environment requirements\nmetadata: Arbitrary key-value pairs\nallowed-tools: Pre-approved tools (experimental)\nuser-invocable: Whether the skill appears in slash command menu\n\n\n\nSkill Discovery Paths\nVibe discovers skills from: 1. Global skills directory: ~/.vibe/skills/ 2. Local project skills: .vibe/skills/ in your project 3. Custom paths: Configured in config.toml\n\n\nManaging Skills\n# Enable specific skills\nenabled_skills = [\"code-review\", \"test-*\"]\n\n# Disable specific skills\ndisabled_skills = [\"experimental-*\"]",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#relationship-between-agents-skills-and-tools",
    "href": "tools_study_mistral-vibe.html#relationship-between-agents-skills-and-tools",
    "title": "Mistral vibe 2.0",
    "section": "Relationship Between Agents, Skills, and Tools",
    "text": "Relationship Between Agents, Skills, and Tools\n\nThe Architecture\n┌───────────────────────────────────────────────────────────────┐\n│                        User Interface                         │\n└───────────────────────────────────────────────────────────────┘\n                            │\n                            ▼\n┌───────────────────────────────────────────────────────────────┐\n│                        Agent Manager                          │\n│  - Manages agent profiles (built-in + custom)                 │\n│  - Handles agent switching and configuration                  │\n└───────────────────────────────────────────────────────────────┘\n                            │\n                            ▼\n┌───────────────────────────────────────────────────────────────┐\n│                        Skill Manager                          │\n│  - Discovers and loads skills from multiple paths             │\n│  - Manages skill enable/disable patterns                      │\n└───────────────────────────────────────────────────────────────┘\n                            │\n                            ▼\n┌───────────────────────────────────────────────────────────────┐\n│                        Tool Manager                           │\n│  - Discovers built-in and custom tools                       │\n│  - Integrates MCP servers as tools                            │\n│  - Manages tool configuration and permissions                  │\n└───────────────────────────────────────────────────────────────┘\n                            │\n                            ▼\n┌───────────────────────────────────────────────────────────────┐\n│                        LLM Backend                            │\n│  - Executes agent conversations                               │\n│  - Handles tool calls and responses                            │\n└───────────────────────────────────────────────────────────────┘\n\n\nHow They Work Together\n\nAgent defines which tools are available and their permissions\nSkills can add new tools or modify behavior (experimental)\nTools are the actual implementations that perform actions\nSubagents are specialized agents that can be delegated to\n\n\n\nTool Permissions\nEach tool can have different permission levels: - always: Auto-approved, no user confirmation - never: Disabled for this agent - ask: Requires user approval (default)\nExample agent configuration with tool permissions:\n[tools.write_file]\npermission = \"always\"\n\n[tools.bash]\npermission = \"ask\"\n\n[tools.search_replace]\npermission = \"never\"",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#personalization-files-1",
    "href": "tools_study_mistral-vibe.html#personalization-files-1",
    "title": "Mistral vibe 2.0",
    "section": "Personalization Files",
    "text": "Personalization Files\nUsers can personalize Mistral Vibe by creating files in specific directories:\n\n1. Agent Configuration Files\nLocation: ~/.vibe/agents/ or .vibe/agents/ (project-specific)\nFormat: TOML files with .toml extension\nExample: ~/.vibe/agents/my-agent.toml\nname = \"my-agent\"\ndisplay_name = \"My Custom Agent\"\ndescription = \"Agent configured for my specific needs\"\nsafety = \"neutral\"\n\n# Override global configuration\nactive_model = \"devstral-2\"\nsystem_prompt_id = \"cli\"\n\n# Tool-specific configuration\n[tools.bash]\npermission = \"always\"\n\n[tools.write_file]\npermission = \"ask\"\n\n[tools.read_file]\npermission = \"always\"\n\n\n2. Skill Definition Files\nLocation: ~/.vibe/skills/ or .vibe/skills/ (project-specific)\nFormat: Directories with SKILL.md file\nExample: ~/.vibe/skills/my-skill/SKILL.md\n---\nname: my-skill\ndescription: My custom skill for specialized tasks\nlicense: MIT\ncompatibility: Python 3.12+\nuser-invocable: true\nallowed-tools:\n  - read_file\n  - grep\n---\n\n# My Skill Documentation\n\nThis skill provides custom functionality for my workflow.\n\n\n3. Tool Definition Files\nLocation: ~/.vibe/tools/ or .vibe/tools/ (project-specific)\nFormat: Python files with BaseTool subclasses\nExample: ~/.vibe/tools/my_tool.py\nfrom pathlib import Path\nfrom vibe.core.tools.base import BaseTool, BaseToolConfig\n\n\nclass MyToolConfig(BaseToolConfig):\n    my_option: str = \"default\"\n\n\nclass MyTool(BaseTool[MyToolConfig]):\n    @classmethod\n    def get_name(cls) -&gt; str:\n        return \"my_tool\"\n\n    @classmethod\n    def get_description(cls) -&gt; str:\n        return \"My custom tool that does something useful\"\n\n    async def run(self, my_option: str) -&gt; str:\n        \"\"\"Run the tool with the given option.\"\"\"\n        return f\"Tool executed with option: {my_option}\"\n\n\n4. System Prompt Files\nLocation: ~/.vibe/prompts/\nFormat: Markdown files with .md extension\nExample: ~/.vibe/prompts/my-prompt.md\n# Custom System Prompt\n\nYou are a helpful coding assistant...\nThen reference it in config:\nsystem_prompt_id = \"my-prompt\"\n\n\n5. Configuration File\nLocation: ~/.vibe/config.toml or .vibe/config.toml (project-specific)\nFormat: TOML\nExample:\nactive_model = \"devstral-2\"\ntextual_theme = \"terminal\"\nauto_approve = false\n\n# Agent paths\nagent_paths = [\"/path/to/custom/agents\"]\n\n# Skill paths\nskill_paths = [\"/path/to/custom/skills\"]\n\n# Tool paths\ntool_paths = [\"/path/to/custom/tools\"]\n\n# Enable/disable specific agents\nenabled_agents = [\"default\", \"plan\"]\ndisabled_agents = [\"auto-approve\"]\n\n# Enable/disable specific skills\nenabled_skills = [\"code-review\"]\ndisabled_skills = [\"experimental-*\"]\n\n# Enable/disable specific tools\nenabled_tools = [\"read_file\", \"grep\", \"bash\"]\ndisabled_tools = [\"write_file\", \"search_replace\"]\n\n# MCP server configuration\n[[mcp_servers]]\nname = \"my_server\"\ntransport = \"http\"\nurl = \"http://localhost:8000\"",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#creating-custom-agents",
    "href": "tools_study_mistral-vibe.html#creating-custom-agents",
    "title": "Mistral vibe 2.0",
    "section": "Creating Custom Agents",
    "text": "Creating Custom Agents\n\nStep-by-Step Guide\n\nCreate a new TOML file in ~/.vibe/agents/ or .vibe/agents/\nDefine the agent profile with name, description, and safety level\nConfigure tool permissions as needed\nUse the agent with the --agent flag\n\n\n\nExample: Creating a Review Agent\n# ~/.vibe/agents/reviewer.toml\nname = \"reviewer\"\ndisplay_name = \"Code Reviewer\"\ndescription = \"Specialized agent for code reviews\"\nsafety = \"safe\"\n\n# Only allow read operations\n[tools.read_file]\npermission = \"always\"\n\n[tools.grep]\npermission = \"always\"\n\n[tools.bash]\npermission = \"never\"\n\n[tools.write_file]\npermission = \"never\"\n\n[tools.search_replace]\npermission = \"never\"\n\n\nUsing the Custom Agent\nvibe --agent reviewer",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#creating-custom-skills",
    "href": "tools_study_mistral-vibe.html#creating-custom-skills",
    "title": "Mistral vibe 2.0",
    "section": "Creating Custom Skills",
    "text": "Creating Custom Skills\n\nStep-by-Step Guide\n\nCreate a directory for your skill in ~/.vibe/skills/ or .vibe/skills/\nCreate a SKILL.md file with YAML frontmatter\nDocument the skill in markdown format\nEnable the skill in your configuration (if needed)\n\n\n\nExample: Creating a Documentation Skill\n# ~/.vibe/skills/documentation/SKILL.md\n---\nname: documentation\ndescription: Generate and maintain project documentation\ndescription: MIT\ncompatibility: Python 3.12+\nuser-invocable: true\nallowed-tools:\n  - read_file\n  - grep\n  - write_file\n---\n\n# Documentation Skill\n\nThis skill helps generate and maintain project documentation by analyzing\ncode structure and creating comprehensive docs.\n\n## Features\n\n- Analyze code structure\n- Generate API documentation\n- Create README files\n- Update documentation based on code changes",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#creating-custom-tools",
    "href": "tools_study_mistral-vibe.html#creating-custom-tools",
    "title": "Mistral vibe 2.0",
    "section": "Creating Custom Tools",
    "text": "Creating Custom Tools\n\nStep-by-Step Guide\n\nCreate a Python file in ~/.vibe/tools/ or .vibe/tools/\nDefine a class that extends BaseTool\nImplement the run method asynchronously\nConfigure tool permissions in your agent profile\n\n\n\nExample: Creating a Custom Tool\n# ~/.vibe/tools/project_stats.py\nfrom pathlib import Path\nfrom vibe.core.tools.base import BaseTool, BaseToolConfig\n\n\nclass ProjectStatsConfig(BaseToolConfig):\n    include_hidden: bool = False\n    max_depth: int = 3\n\n\nclass ProjectStats(BaseTool[ProjectStatsConfig]):\n    @classmethod\n    def get_name(cls) -&gt; str:\n        return \"project_stats\"\n\n    @classmethod\n    def get_description(cls) -&gt; str:\n        return \"Generate statistics about the project structure\"\n\n    async def run(\n        self, \n        include_hidden: bool = False, \n        max_depth: int = 3\n    ) -&gt; str:\n        \"\"\"Generate project statistics.\"\"\"\n        base_path = Path.cwd()\n        \n        # Count files and directories\n        files = 0\n        dirs = 0\n        \n        for path in base_path.rglob(\"*\"):\n            if path.is_file():\n                files += 1\n            elif path.is_dir():\n                dirs += 1\n            \n            # Apply filters\n            if path.name.startswith(\".\") and not include_hidden:\n                continue\n        \n        return f\"Project Statistics:\\n- Files: {files}\\n- Directories: {dirs}\"\n\n\nUsing the Custom Tool\nOnce created, the tool will be automatically discovered and available to agents:\n&gt; project_stats()",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#configuration-overview",
    "href": "tools_study_mistral-vibe.html#configuration-overview",
    "title": "Mistral vibe 2.0",
    "section": "Configuration Overview",
    "text": "Configuration Overview\n\nConfiguration File Structure\nThe main configuration file (config.toml) controls:\n\nGlobal Settings: Model, theme, behavior\nAgent Management: Paths, enable/disable patterns\nSkill Management: Paths, enable/disable patterns\nTool Management: Paths, enable/disable patterns, permissions\nMCP Server Configuration: HTTP and stdio servers\nSession Management: Logging, save directory\n\n\n\nConfiguration Inheritance\nConfiguration follows this inheritance order (later overrides earlier):\n\nDefault values (built into Vibe)\nGlobal config (~/.vibe/config.toml)\nProject config (.vibe/config.toml)\nAgent-specific overrides (from agent profile)\n\n\n\nPattern Matching\nVibe supports three types of patterns for enabling/disabling agents, skills, and tools:\n\nExact names: \"default\" matches only “default”\nGlob patterns: \"test-*\" matches “test-1”, “test-2”, etc.\nRegex patterns: \"re:^serena_.*$\" matches regex pattern",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#summary-1",
    "href": "tools_study_mistral-vibe.html#summary-1",
    "title": "Mistral vibe 2.0",
    "section": "Summary",
    "text": "Summary\n\nAgents are the main entities that interact with users\nSubagents are specialized agents for task delegation\nSkills extend functionality through reusable components\nTools are the actual implementations that perform actions\nAll can be customized through configuration files and Python code\nConfiguration controls which agents, skills, and tools are available\n\nThis architecture provides a flexible and extensible system for personalizing Mistral Vibe to your specific workflow and needs.",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#prompt-personalization-options-in-mistral-vibe",
    "href": "tools_study_mistral-vibe.html#prompt-personalization-options-in-mistral-vibe",
    "title": "Mistral vibe 2.0",
    "section": "Prompt Personalization Options in Mistral Vibe",
    "text": "Prompt Personalization Options in Mistral Vibe\n\n1. Built-in Prompts\n\nSystem Prompts: cli (default), tests (minimal)\nUtility Prompts: compact (conversation summaries), project_context (project info), dangerous_directory (security warnings)\n\n\n\n2. System Prompt Customization\n\nCreate custom prompts in ~/.vibe/prompts/ or .vibe/prompts/ (project-specific)\nReference them via system_prompt_id in config\nDiscovery order: project → global → built-in\n\n\n\n3. Utility Prompt Customization\n\nProject Context Template: Can be customized with placeholders like {structure}, {abs_path}, {git_status}\nDangerous Directory Prompt: Can be customized for security warnings\n\n\n\n4. Project Context Configuration\n\nConfigurable parameters:\n\nmax_chars, max_depth, max_files, max_dirs_per_level\ntimeout_seconds, default_commit_count, max_doc_bytes\n\nAutomatic documentation loading from README files\nGit status integration with branch info and recent commits\n\n\n\n5. Session-Specific Prompt Control\n\nInclude/exclude layers:\n\ninclude_commit_signature - Git commit instructions\ninclude_model_info - Model name\ninclude_prompt_detail - OS/shell info and tool prompts\ninclude_project_context - Project structure and git status\n\n\n\n\n6. Advanced Customization\n\nAgent-specific prompts - Different prompts for different agents\nCustom tool prompts - Override get_tool_prompt() in custom tools\nDynamic content - Directory structure, git status, skills, subagents\n\n\n\n7. Best Practices\n\nStart with built-in prompts and modify incrementally\nKeep prompts concise (token limits)\nTest changes incrementally\nUse multiple prompts for different use cases\nDocument your custom prompts\n\nThe document provides complete examples and step-by-step guides for creating custom prompts, configuring project context, and controlling prompt layers for different scenarios.",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#understanding-the-prompt-system",
    "href": "tools_study_mistral-vibe.html#understanding-the-prompt-system",
    "title": "Mistral vibe 2.0",
    "section": "Understanding the Prompt System",
    "text": "Understanding the Prompt System\nMistral Vibe uses a multi-layered prompt architecture that combines:\n\nBase System Prompt - Defines the agent’s core behavior and guidelines\nUtility Prompts - Contextual information like project structure and git status\nTool Prompts - Specific instructions for each available tool\nSkill Prompts - Information about available skills\nSubagent Prompts - Information about available subagents\n\nThe final prompt sent to the LLM is constructed by combining these layers based on configuration settings.\n\nPrompt Construction Flow\n┌─────────────────────────────────────────────────────────────┐\n│                 System Prompt (Base)                        │\n└─────────────────────────────────────────────────────────────┘\n                            │\n                            ▼\n┌─────────────────────────────────────────────────────────────┐\n│                 Optional Layers                            │\n│  - Commit Signature (if enabled)                           │\n│  - Model Info (if enabled)                                 │\n│  - OS System Prompt (if enabled)                            │\n│  - Tool Prompts (if enabled)                                │\n│  - Skill Prompts (if enabled)                               │\n│  - Subagent Prompts (if enabled)                            │\n└─────────────────────────────────────────────────────────────┘\n                            │\n                            ▼\n┌─────────────────────────────────────────────────────────────┐\n│                 Project Context (if enabled)                │\n│  - Directory Structure                                      │\n│  - Git Status                                               │\n│  - Project Documentation (if available)                     │\n└─────────────────────────────────────────────────────────────┘\n                            │\n                            ▼\n┌─────────────────────────────────────────────────────────────┐\n│                 Final Prompt Sent to LLM                    │\n└─────────────────────────────────────────────────────────────┘",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#built-in-prompts-1",
    "href": "tools_study_mistral-vibe.html#built-in-prompts-1",
    "title": "Mistral vibe 2.0",
    "section": "Built-in Prompts",
    "text": "Built-in Prompts\nMistral Vibe comes with several built-in prompts:\n\nSystem Prompts\n\ncli - The default system prompt for CLI interactions (see vibe/core/prompts/cli.md)\n\nFocuses on tool usage, code modifications, and professional objectivity\nDesigned for coding assistance in a terminal environment\n\ntests - A minimal test prompt (see vibe/core/prompts/tests.md)\n\nSimple prompt for testing purposes\n\n\n\n\nUtility Prompts\n\ncompact - Prompt for creating conversation summaries (see vibe/core/prompts/compact.md)\n\nUsed when the conversation needs to be compacted\nRequires specific structure with 7 sections\n\nproject_context - Template for project context information (see vibe/core/prompts/project_context.md)\n\nDisplays directory structure and git status\nUsed to provide project-aware context\n\ndangerous_directory - Warning for dangerous directories (see vibe/core/prompts/dangerous_directory.md)\n\nShown when scanning is disabled for security reasons",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#system-prompt-customization-1",
    "href": "tools_study_mistral-vibe.html#system-prompt-customization-1",
    "title": "Mistral vibe 2.0",
    "section": "System Prompt Customization",
    "text": "System Prompt Customization\nThe system prompt is the foundation of the agent’s behavior and can be fully customized.\n\nMethod 1: Using Built-in Prompts\nSimply set the system_prompt_id in your configuration:\n# ~/.vibe/config.toml\nsystem_prompt_id = \"cli\"  # or \"tests\"\n\n\nMethod 2: Creating Custom System Prompts\nYou can create your own system prompts by placing markdown files in the prompts directory.\n\nStep-by-Step Guide\n\nCreate a prompts directory (if it doesn’t exist):\nmkdir -p ~/.vibe/prompts\nCreate a markdown file with your custom prompt:\nnano ~/.vibe/prompts/my-custom-prompt.md\nDefine your prompt in the markdown file:\n# My Custom Prompt\n\nYou are a helpful coding assistant specialized in [your domain].\n\n## Guidelines\n\n- Always be helpful and friendly\n- Focus on [specific requirements]\n- Avoid [certain behaviors]\n- Prefer [specific approaches]\n\n## Tool Usage\n\n- Use tools to fulfill requests\n- Always check parameters before using tools\n- Match the existing code style\nActivate your custom prompt in config:\n# ~/.vibe/config.toml\nsystem_prompt_id = \"my-custom-prompt\"\n\n\n\nProject-Specific Custom Prompts\nYou can also create prompts specific to a project:\nmkdir -p .vibe/prompts\nnano .vibe/prompts/project-prompt.md\nThen reference it:\n# .vibe/config.toml\nsystem_prompt_id = \"project-prompt\"\n\n\n\nPrompt Discovery Order\nVibe looks for custom prompts in this order: 1. Project-specific prompts: .vibe/prompts/[name].md 2. Global prompts: ~/.vibe/prompts/[name].md 3. Built-in prompts: vibe/core/prompts/[name].md",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#utility-prompt-customization-1",
    "href": "tools_study_mistral-vibe.html#utility-prompt-customization-1",
    "title": "Mistral vibe 2.0",
    "section": "Utility Prompt Customization",
    "text": "Utility Prompt Customization\nUtility prompts are used for specific contextual information and can also be customized.\n\nCustomizing Project Context\nThe project context template (project_context.md) can be customized:\n\nCreate a custom template:\nnano ~/.vibe/prompts/project_context.md\nModify the template with your preferred format: ```markdown directoryStructure: Below is a snapshot of {abs_path} at the start of the conversation.{large_repo_warning}\n\n{structure}\nAbsolute path: {abs_path}\ngitStatus: This is the git status at the start of the conversation. {git_status}\nAdditional Context: - Current timestamp: {timestamp} - User: {username}\n\n3. **Note**: The template uses placeholders that will be replaced at runtime:\n   - `{large_repo_warning}` - Warning if repository is large\n   - `{structure}` - Directory structure\n   - `{abs_path}` - Absolute path\n   - `{git_status}` - Git status information\n\n### Customizing Dangerous Directory Prompt\n\nThe dangerous directory prompt can also be customized:\n\n```bash\nnano ~/.vibe/prompts/dangerous_directory.md\nExample custom version:\n⚠️  Security Restriction Active ⚠️\n\nProject context scanning has been disabled because {reason}.\n\nThis is for your security. You can still use tools to explore the project:\n- Use `read_file` to read specific files\n- Use `bash` to run commands\n- Use `grep` to search for patterns\n\nAbsolute path: {abs_path}",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#project-context-customization",
    "href": "tools_study_mistral-vibe.html#project-context-customization",
    "title": "Mistral vibe 2.0",
    "section": "Project Context Customization",
    "text": "Project Context Customization\nThe project context provides information about the current project and can be extensively customized through configuration.\n\nConfiguration Options\n# ~/.vibe/config.toml\n\n[project_context]\n# Maximum characters in directory structure\nmax_chars = 40000\n\n# Default number of commits to show in git status\ndefault_commit_count = 5\n\n# Maximum size of documentation files to load\nmax_doc_bytes = 32768\n\n# Buffer for truncation warnings\ntruncation_buffer = 1000\n\n# Maximum depth for directory traversal\nmax_depth = 3\n\n# Maximum number of files to show\nmax_files = 1000\n\n# Maximum directories per level\nmax_dirs_per_level = 20\n\n# Timeout for git operations (seconds)\ntimeout_seconds = 2.0\n\n\nCustomizing Project Documentation\nVibe automatically loads documentation from these files in the project root: - README.md - README.rst - README.txt - README.markdown - README - readme.md - readme.rst - readme.txt - readme.markdown - readme\nThe documentation is loaded with a size limit (default: 32KB) and displayed in the prompt.\n\n\nDisabling Project Context\nYou can disable project context entirely:\n# ~/.vibe/config.toml\ninclude_project_context = false",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#session-specific-prompt-control-1",
    "href": "tools_study_mistral-vibe.html#session-specific-prompt-control-1",
    "title": "Mistral vibe 2.0",
    "section": "Session-Specific Prompt Control",
    "text": "Session-Specific Prompt Control\nYou can control which prompt layers are included in each session through configuration.\n\nAvailable Configuration Options\n# ~/.vibe/config.toml\n\n# Include commit signature instructions\ninclude_commit_signature = true\n\n# Include model information\ninclude_model_info = true\n\n# Include OS and shell information\ninclude_prompt_detail = true\n\n# Include project context\ninclude_project_context = true\n\n\nExample: Minimal Prompt Configuration\n# ~/.vibe/config.toml\ninclude_commit_signature = false\ninclude_model_info = false\ninclude_prompt_detail = false\ninclude_project_context = false\nThis would result in just the base system prompt being sent to the LLM.\n\n\nExample: Full Context Configuration\n# ~/.vibe/config.toml\ninclude_commit_signature = true\ninclude_model_info = true\ninclude_prompt_detail = true\ninclude_project_context = true\nThis would include all available context layers.",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#advanced-prompt-customization",
    "href": "tools_study_mistral-vibe.html#advanced-prompt-customization",
    "title": "Mistral vibe 2.0",
    "section": "Advanced Prompt Customization",
    "text": "Advanced Prompt Customization\n\nAgent-Specific Prompts\nYou can override the system prompt for specific agents:\n# ~/.vibe/agents/reviewer.toml\nname = \"reviewer\"\ndisplay_name = \"Code Reviewer\"\nsystem_prompt_id = \"review-prompt\"\n\n[tools.read_file]\npermission = \"always\"\nThen create the custom prompt:\nnano ~/.vibe/prompts/review-prompt.md\n\n\nDynamic Prompt Selection\nWhile Vibe doesn’t support dynamic prompt selection based on runtime conditions out of the box, you can:\n\nCreate multiple prompts and switch between them using different agents\nUse environment variables in your prompts (though they won’t be expanded automatically)\nUse MCP servers to provide dynamic context through tools\n\n\n\nCustom Tool Prompts\nEach tool can have its own prompt. To customize tool prompts:\n\nCreate a custom tool in ~/.vibe/tools/\nOverride the get_tool_prompt() method:\n\n# ~/.vibe/tools/my_tool.py\nfrom vibe.core.tools.base import BaseTool, BaseToolConfig\n\n\nclass MyTool(BaseTool[BaseToolConfig]):\n    @classmethod\n    def get_name(cls) -&gt; str:\n        return \"my_tool\"\n\n    @classmethod\n    def get_description(cls) -&gt; str:\n        return \"My custom tool\"\n\n    @classmethod\n    def get_tool_prompt(cls) -&gt; str:\n        \"\"\"Custom prompt for this tool.\"\"\"\n        return \"\"\"\n# My Tool Usage\n\nWhen using my_tool, always:\n- Provide the my_option parameter\n- Use lowercase values\n- Check the result before proceeding\n        \"\"\"\n\n    async def run(self) -&gt; str:\n        return \"Tool executed\"",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#prompt-variables-and-dynamic-content",
    "href": "tools_study_mistral-vibe.html#prompt-variables-and-dynamic-content",
    "title": "Mistral vibe 2.0",
    "section": "Prompt Variables and Dynamic Content",
    "text": "Prompt Variables and Dynamic Content\nThe prompt system supports several dynamic variables that are replaced at runtime:\n\nSystem Prompt Variables\n\n{config.active_model} - The currently active model name\n{config.textual_theme} - The current UI theme\n{platform} - Operating system platform\n{shell} - Default shell\n\n\n\nProject Context Variables\n\n{large_repo_warning} - Warning if repository is large\n{structure} - Directory structure tree\n{abs_path} - Absolute path to project root\n{git_status} - Git repository status\n\n\n\nGit Status Variables\n\nCurrent branch - Name of current git branch\nMain branch - Name of main branch (main or master)\nStatus - Repository status (clean or number of changes)\nRecent commits - List of recent commits\n\n\n\nDynamic Content Generation\nSome content is generated dynamically:\n\nDirectory Structure - Built from actual filesystem\nGit Status - Fetched from git commands\nAvailable Skills - List of loaded skills\nAvailable Subagents - List of available subagents\nTool Prompts - Prompts from each available tool",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#best-practices-for-prompt-customization",
    "href": "tools_study_mistral-vibe.html#best-practices-for-prompt-customization",
    "title": "Mistral vibe 2.0",
    "section": "Best Practices for Prompt Customization",
    "text": "Best Practices for Prompt Customization\n\n1. Start with the Built-in Prompt\nBegin by using the built-in cli prompt and make incremental changes:\ncp vibe/core/prompts/cli.md ~/.vibe/prompts/my-prompt.md\nnano ~/.vibe/prompts/my-prompt.md\n\n\n2. Keep Prompts Concise\n\nThe LLM has token limits, so keep prompts focused\nRemove guidelines that aren’t essential for your use case\nAvoid redundant information\n\n\n\n3. Be Specific About Requirements\nIf you have specific requirements, state them clearly:\n## Domain-Specific Guidelines\n\n- Always use TypeScript for frontend code\n- Prefer functional components over class components\n- Use the @mui/material library for UI components\n\n\n4. Test Incrementally\nMake small changes and test them before making larger modifications:\nsystem_prompt_id = \"test-v1\"\nTest, then:\nsystem_prompt_id = \"test-v2\"\n\n\n5. Document Your Custom Prompts\nAdd comments to explain why certain guidelines exist:\n## Code Style Guidelines\n\n- Use 2-space indentation (company standard)\n- Prefer const over let (immutability best practice)\n\n\n6. Consider Context Length\nRemember that the prompt is combined with: - Conversation history - Previous tool outputs - Current user message\nKeep the total under the model’s context window limit.\n\n\n7. Use Multiple Prompts for Different Use Cases\nCreate different prompts for different scenarios:\n# For code reviews\nnano ~/.vibe/prompts/review.md\n\n# For documentation\nnano ~/.vibe/prompts/docs.md\n\n# For testing\nnano ~/.vibe/prompts/test.md\nThen use different agents for each:\n# ~/.vibe/agents/reviewer.toml\nsystem_prompt_id = \"review\"\n\n# ~/.vibe/agents/docwriter.toml\nsystem_prompt_id = \"docs\"",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#complete-example-custom-prompt-setup",
    "href": "tools_study_mistral-vibe.html#complete-example-custom-prompt-setup",
    "title": "Mistral vibe 2.0",
    "section": "Complete Example: Custom Prompt Setup",
    "text": "Complete Example: Custom Prompt Setup\n\nStep 1: Create Custom Prompt Directory\nmkdir -p ~/.vibe/prompts\n\n\nStep 2: Create Custom System Prompt\nnano ~/.vibe/prompts/enterprise-dev.md\n# Enterprise Development Assistant\n\nYou are operating as and within Mistral Vibe, configured for enterprise development standards.\n\n## Core Principles\n\n- **Security First**: Always consider security implications\n- **Maintainability**: Write code that's easy to understand and maintain\n- **Performance**: Optimize for performance where it matters\n- **Standards Compliance**: Follow all company coding standards\n\n## Tool Usage Guidelines\n\n- Always use tools to fulfill user requests\n- Check parameters before using tools\n- Match existing code style exactly\n- Keep changes minimal and focused\n\n## Enterprise-Specific Rules\n\n- Use TypeScript for all new frontend code\n- Use React with @mui/material for UI components\n- Follow the monorepo structure\n- Use npm workspaces for package management\n- Always add proper documentation\n\n## Code Quality\n\n- Write clean, self-documenting code\n- Add JSDoc/TypeScript comments for public APIs\n- Follow SOLID principles\n- Write unit tests for all new code\n- Ensure code passes linting checks\n\n\nStep 3: Create Agent Configuration\nnano ~/.vibe/agents/enterprise.toml\nname = \"enterprise\"\ndisplay_name = \"Enterprise Developer\"\ndescription = \"Agent configured for enterprise development standards\"\nsafety = \"neutral\"\nsystem_prompt_id = \"enterprise-dev\"\n\n# Tool permissions\n[tools.write_file]\npermission = \"ask\"\n\n[tools.search_replace]\npermission = \"ask\"\n\n[tools.bash]\npermission = \"ask\"\n\n\nStep 4: Update Main Configuration\nnano ~/.vibe/config.toml\nactive_model = \"devstral-2\"\ntextual_theme = \"terminal\"\n\n# Project context settings\n[project_context]\nmax_chars = 30000\nmax_depth = 4\nmax_files = 500\n\n# Include all context\ninclude_commit_signature = true\ninclude_model_info = true\ninclude_prompt_detail = true\ninclude_project_context = true\n\n\nStep 5: Use Your Custom Setup\nvibe --agent enterprise",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#summary-2",
    "href": "tools_study_mistral-vibe.html#summary-2",
    "title": "Mistral vibe 2.0",
    "section": "Summary",
    "text": "Summary\nMistral Vibe offers extensive prompt customization options:\n\nSystem Prompts - Customize base behavior (cli, tests, or custom)\nUtility Prompts - Customize contextual information (project_context, dangerous_directory)\nPrompt Layers - Control what context is included (commit signature, model info, etc.)\nProject Context - Configure how project information is presented\nAgent-Specific Prompts - Different prompts for different agents\nTool Prompts - Custom prompts for custom tools\n\n\nKey Configuration Options\n\nsystem_prompt_id - Which system prompt to use\ninclude_commit_signature - Include git commit instructions\ninclude_model_info - Include model name in prompt\ninclude_prompt_detail - Include OS/shell info and tool prompts\ninclude_project_context - Include project structure and git status\n[project_context] - Configure project context settings\n\n\n\nCustomization Locations\n\nGlobal: ~/.vibe/prompts/\nProject: .vibe/prompts/\nConfiguration: ~/.vibe/config.toml or .vibe/config.toml\nAgent Configs: ~/.vibe/agents/*.toml or .vibe/agents/*.toml\n\nBy understanding and leveraging these options, you can fully customize Mistral Vibe’s behavior to match your specific workflow, coding standards, and requirements.",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#how-the-task-tool-works-in-mistral-vibe",
    "href": "tools_study_mistral-vibe.html#how-the-task-tool-works-in-mistral-vibe",
    "title": "Mistral vibe 2.0",
    "section": "How the Task Tool Works in Mistral Vibe",
    "text": "How the Task Tool Works in Mistral Vibe\nI’ve explored the task tool implementation and created a detailed document covering all aspects. Here’s what I found:\n\nCore Functionality\nThe task tool delegates work to subagents for independent execution, enabling: - Context management (offload work to prevent context overload) - Parallel execution (run multiple tasks independently) - Specialization (use different subagents for different tasks) - Automation (delegate repetitive or complex tasks)\n\n\nHow It Works\n\nValidation: Checks that the specified agent exists and is a subagent\nIsolated Execution: Creates a new AgentLoop with logging disabled\nConversation Loop: Runs the subagent with the task description\nEvent Streaming: Collects responses and displays tool results\nCompletion: Returns accumulated response with turn count\n\n\n\nKey Security Mechanisms\n\nAgent Type Validation: Only subagents can be spawned (prevents recursion)\nIsolated Configuration: Session logging disabled for privacy\nInherited Approval Callback: Maintains same permission model\nTurn Counting: Prevents runaway processes\nError Handling: Graceful degradation on failures\n\n\n\nTask Arguments\n\nRequired: task (str) - The task description\nOptional: agent (str) - Subagent name (default: “explore”)\n\n\n\nTask Results\nReturns a TaskResult with: - response: Accumulated text response - turns_used: Number of assistant responses - completed: Whether task finished normally\n\n\nBest Practices\n\nWrite clear, detailed task descriptions\nChoose the right subagent for the task\nUse for complex tasks, not simple operations\nTrust subagent autonomy\n\nThe document provides complete implementation details, security analysis, use cases, and a step-by-step example of creating and using custom subagents.",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#complete-guide-to-the-task-tool",
    "href": "tools_study_mistral-vibe.html#complete-guide-to-the-task-tool",
    "title": "Mistral vibe 2.0",
    "section": "Complete Guide to the Task Tool",
    "text": "Complete Guide to the Task Tool\n\nOverview of the Task Tool\nHow the Task Tool Works\nTask Execution Flow\nTask Arguments\nTask Results\nSecurity and Safety Mechanisms\nUse Cases and Best Practices\nLimitations\nImplementation Details",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#overview-of-the-task-tool",
    "href": "tools_study_mistral-vibe.html#overview-of-the-task-tool",
    "title": "Mistral vibe 2.0",
    "section": "Overview of the Task Tool",
    "text": "Overview of the Task Tool\nThe task tool is a powerful feature in Mistral Vibe that allows delegating work to subagents for independent execution. It enables parallel processing and context management by running specialized agents in the background.\n\nKey Characteristics\n\nIndependent Execution: Subagents run autonomously without user interaction\nContext Isolation: Subagent conversations don’t interfere with the main conversation\nResource Management: Limits on turns and execution to prevent runaway processes\nSecurity Constraints: Only subagents can be spawned (not regular agents)\nNo Logging: Subagent interactions are not saved to session logs",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#how-the-task-tool-works",
    "href": "tools_study_mistral-vibe.html#how-the-task-tool-works",
    "title": "Mistral vibe 2.0",
    "section": "How the Task Tool Works",
    "text": "How the Task Tool Works\nThe task tool creates a separate AgentLoop instance for the subagent, runs it with the provided task description, and returns the accumulated results.\n\nCore Process\n\nValidation: Check that the specified agent exists and is a subagent\nConfiguration: Create a new configuration with logging disabled\nExecution: Run the subagent’s conversation loop with the task\nMonitoring: Track turns and collect output\nCompletion: Return results when done or interrupted\n\n\n\nCode Flow\n# 1. Get the subagent profile\nagent_profile = agent_manager.get_agent(args.agent)\n\n# 2. Validate it's a subagent\nif agent_profile.agent_type != AgentType.SUBAGENT:\n    raise ToolError(\"Only subagents can be used\")\n\n# 3. Create isolated configuration\nbase_config = VibeConfig.load(session_logging=SessionLoggingConfig(enabled=False))\n\n# 4. Create subagent loop\nsubagent_loop = AgentLoop(config=base_config, agent_name=args.agent)\n\n# 5. Execute and collect results\nasync for event in subagent_loop.act(args.task):\n    if isinstance(event, AssistantEvent) and event.content:\n        accumulated_response.append(event.content)\n    # ... handle other events\n\n# 6. Return results\nyield TaskResult(\n    response=\"\".join(accumulated_response),\n    turns_used=turns_used,\n    completed=completed,\n)",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#task-execution-flow",
    "href": "tools_study_mistral-vibe.html#task-execution-flow",
    "title": "Mistral vibe 2.0",
    "section": "Task Execution Flow",
    "text": "Task Execution Flow\n\nStep-by-Step Execution\n\nTool Invocation\n\nUser or main agent calls task(task=\"...\", agent=\"...\")\nTool is validated and permissions are checked\n\nSubagent Selection\n\nAgent manager looks up the specified agent\nVerifies it’s a subagent (not a regular agent)\nLoads the agent’s profile and configuration\n\nIsolated Execution Environment\n\nCreates new AgentLoop instance for the subagent\nDisables session logging (SessionLoggingConfig(enabled=False))\nInherits approval callback from parent context\n\nConversation Loop\n\nSubagent processes the task description\nCan use any tools allowed in its profile\nGenerates responses and tool calls\n\nEvent Streaming\n\nAssistant messages are collected\nTool results are displayed in real-time\nInterruptions are detected\n\nCompletion\n\nCounts turns used (number of assistant responses)\nDetermines if task completed normally\nReturns accumulated response\n\n\n\n\nEvent Types Handled\nThe task tool processes these event types from the subagent:\n\nAssistantEvent: Collects the subagent’s text responses\nToolResultEvent: Displays tool execution results\nCompactStartEvent/CompactEndEvent: Handles conversation compaction\nMiddleware stop events: Detects interruptions",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#task-arguments-1",
    "href": "tools_study_mistral-vibe.html#task-arguments-1",
    "title": "Mistral vibe 2.0",
    "section": "Task Arguments",
    "text": "Task Arguments\nThe task tool accepts two parameters:\n\nRequired Parameter\n\ntask (str): The task description to delegate to the subagent\n\nShould be clear and detailed\nProvides context for autonomous execution\nExamples: “Analyze the project structure”, “Find all TODO comments”\n\n\n\n\nOptional Parameter\n\nagent (str): Name of the subagent to use\n\nDefault: “explore” (built-in exploration subagent)\nMust be a valid subagent profile\nExamples: “explore”, “my-custom-subagent”\n\n\n\n\nExample Invocations\ntask(task=\"Find all instances of the word TODO in the codebase\")\ntask(task=\"Analyze the architecture of the backend service\", agent=\"explore\")\ntask(task=\"Review the test files for missing assertions\", agent=\"reviewer\")",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#task-results-1",
    "href": "tools_study_mistral-vibe.html#task-results-1",
    "title": "Mistral vibe 2.0",
    "section": "Task Results",
    "text": "Task Results\nThe task tool returns a TaskResult object with three fields:\n\nResult Fields\n\nresponse (str): Accumulated text response from the subagent\n\nContains all assistant messages concatenated\nMay include error messages if execution failed\nUsed as the primary output of the tool\n\nturns_used (int): Number of turns the subagent used\n\nCounts assistant responses in the conversation\nHelps track resource usage\nUsed for monitoring and billing\n\ncompleted (bool): Whether the task completed normally\n\nTrue if subagent finished naturally\nFalse if interrupted by middleware or error\nHelps determine if results are reliable\n\n\n\n\nExample Results\nSuccessful completion:\n{\n  \"response\": \"Found 15 TODO comments in the codebase...\",\n  \"turns_used\": 3,\n  \"completed\": true\n}\nInterrupted:\n{\n  \"response\": \"Analyzing project structure... [Subagent error: timeout]\",\n  \"turns_used\": 5,\n  \"completed\": false\n}",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#security-and-safety-mechanisms",
    "href": "tools_study_mistral-vibe.html#security-and-safety-mechanisms",
    "title": "Mistral vibe 2.0",
    "section": "Security and Safety Mechanisms",
    "text": "Security and Safety Mechanisms\nThe task tool has multiple security layers:\n\n1. Agent Type Validation\nif agent_profile.agent_type != AgentType.SUBAGENT:\n    raise ToolError(\n        f\"Agent '{args.agent}' is a {agent_profile.agent_type.value} agent. \"\n        f\"Only subagents can be used with the task tool. \"\n        f\"This is a security constraint to prevent recursive spawning.\"\n    )\nPurpose: Prevents infinite recursion by ensuring only subagents can be spawned.\n\n\n2. Isolated Configuration\nbase_config = VibeConfig.load(\n    session_logging=SessionLoggingConfig(enabled=False)\n)\nPurpose: Subagent interactions are not logged to prevent sensitive data leakage.\n\n\n3. Inherited Approval Callback\nif ctx and ctx.approval_callback:\n    subagent_loop.set_approval_callback(ctx.approval_callback)\nPurpose: Maintains the same permission model as the parent agent.\n\n\n4. Turn Counting\nturns_used = sum(\n    msg.role == Role.assistant for msg in subagent_loop.messages\n)\nPurpose: Prevents runaway processes by tracking resource usage.\n\n\n5. Error Handling\ntry:\n    async for event in subagent_loop.act(args.task):\n        # ... process events\n        \nexcept Exception as e:\n    completed = False\n    accumulated_response.append(f\"\\n[Subagent error: {e}]\")\nPurpose: Graceful handling of subagent failures.\n\n\n6. Middleware Integration\nThe subagent inherits middleware from the main configuration: - TurnLimitMiddleware: Limits number of turns - PriceLimitMiddleware: Limits cost - AutoCompactMiddleware: Compacts long conversations - PlanAgentMiddleware: Handles plan agent logic",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#use-cases-and-best-practices",
    "href": "tools_study_mistral-vibe.html#use-cases-and-best-practices",
    "title": "Mistral vibe 2.0",
    "section": "Use Cases and Best Practices",
    "text": "Use Cases and Best Practices\n\nWhen to Use the Task Tool\n✅ Context management: Delegate tasks that would consume too much main conversation context ✅ Specialized work: Use appropriate subagents for specific task types ✅ Parallel execution: Launch multiple subagents for independent tasks ✅ Autonomous work: Tasks that don’t require back-and-forth with the user\n\n\nBest Practices\n\nWrite clear, detailed task descriptions\n\nThe subagent works autonomously\nProvide enough context for independent success\nExample: “Analyze the project structure and architecture” vs “Check stuff”\n\nChoose the right subagent\n\nMatch the subagent to the task type\nUse built-in subagents like “explore” for code analysis\nCreate custom subagents for specialized tasks\n\nPrefer direct tools for simple operations\n\nIf you know exactly which file to read: use read_file\nIf you need to search: use grep\nOnly use task for complex, multi-step work\n\nTrust the subagent’s judgment\n\nLet it explore without micromanaging\nAvoid specifying exact steps\nFocus on the goal, not the method\n\n\n\n\nExample Use Cases\nCodebase Exploration:\ntask(task=\"Analyze the project structure and identify key components\")\nPattern Searching:\ntask(task=\"Find all deprecated API usages in the codebase\")\nDocumentation Review:\ntask(task=\"Check if all public functions have proper documentation\")\nArchitecture Analysis:\ntask(task=\"Analyze the database schema and identify potential issues\")",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#limitations",
    "href": "tools_study_mistral-vibe.html#limitations",
    "title": "Mistral vibe 2.0",
    "section": "Limitations",
    "text": "Limitations\n\nFunctional Limitations\n\nNo file writing: Subagents cannot write or modify files\nNo user interaction: Subagents cannot ask user questions\nNo persistent state: Subagent sessions are not saved\nLimited turns: Controlled by middleware (turn limits)\nCost limits: Controlled by middleware (price limits)\n\n\n\nTechnical Limitations\n\nSingle subagent at a time: Only one subagent runs per task call\nNo nested tasks: Subagents cannot call the task tool themselves\nResource sharing: Subagents share the same LLM backend\nContext isolation: Subagent context doesn’t affect main conversation\n\n\n\nError Handling\n\nTimeouts: Subagents may be interrupted if they take too long\nResource limits: Subagents are constrained by middleware\nPermission errors: Subagents respect their tool permissions\nExecution errors: Errors are captured and returned in response",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#implementation-details",
    "href": "tools_study_mistral-vibe.html#implementation-details",
    "title": "Mistral vibe 2.0",
    "section": "Implementation Details",
    "text": "Implementation Details\n\nTool Class Structure\nclass Task(\n    BaseTool[TaskArgs, TaskResult, TaskToolConfig, BaseToolState],\n    ToolUIData[TaskArgs, TaskResult],\n):\nType Parameters: - TaskArgs: Input arguments (task description, agent name) - TaskResult: Output results (response, turns, completion status) - TaskToolConfig: Tool configuration (permission level) - BaseToolState: Tool execution state\n\n\nKey Methods\n\nrun(): Main execution method (async generator)\n\nCreates subagent loop\nExecutes task\nYields stream events\nReturns final result\n\nget_call_display(): UI display for tool call\n\nShows “Running {agent} agent: {task}”\nProvides feedback during execution\n\nget_result_display(): UI display for tool result\n\nShows “Agent completed in {turns_used} turns”\nIndicates success or interruption\n\nget_status_text(): Status message\n\nReturns “Running subagent”\nUsed during execution\n\n\n\n\nPermission Model\nclass TaskToolConfig(BaseToolConfig):\n    permission: ToolPermission = ToolPermission.ASK\nDefault Permission: ASK (requires user approval)\nPossible Values: - always: Auto-approve task executions - never: Disable task tool completely - ask: Require user confirmation (default)\n\n\nUI Integration\nThe task tool implements ToolUIData for rich UI display:\n@classmethod\ndef get_call_display(cls, event: ToolCallEvent) -&gt; ToolCallDisplay:\n    args = event.args\n    if isinstance(args, TaskArgs):\n        return ToolCallDisplay(\n            summary=f\"Running {args.agent} agent: {args.task}\"\n        )\n    return ToolCallDisplay(summary=\"Running subagent\")\n\n@classmethod\ndef get_result_display(cls, event: ToolResultEvent) -&gt; ToolResultDisplay:\n    result = event.result\n    if isinstance(result, TaskResult):\n        turn_word = \"turn\" if result.turns_used == 1 else \"turns\"\n        if not result.completed:\n            return ToolResultDisplay(\n                success=False,\n                message=f\"Agent interrupted after {result.turns_used} {turn_word}\",\n            )\n        return ToolResultDisplay(\n            success=True,\n            message=f\"Agent completed in {result.turns_used} {turn_word}\",\n        )\n    return ToolResultDisplay(success=True, message=\"Agent completed\")",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#complete-example",
    "href": "tools_study_mistral-vibe.html#complete-example",
    "title": "Mistral vibe 2.0",
    "section": "Complete Example",
    "text": "Complete Example\n\nCreating a Custom Subagent\nFirst, create a custom subagent profile:\n# ~/.vibe/agents/code-analyzer.toml\nname = \"code-analyzer\"\ndisplay_name = \"Code Analyzer\"\ndescription = \"Specialized subagent for code analysis tasks\"\nsafety = \"safe\"\nagent_type = \"subagent\"\n\n# Allow read-only tools\n[tools.read_file]\npermission = \"always\"\n\n[tools.grep]\npermission = \"always\"\n\n[tools.bash]\npermission = \"always\"\n\n# Disable write tools\n[tools.write_file]\npermission = \"never\"\n\n[tools.search_replace]\npermission = \"never\"\n\n\nUsing the Custom Subagent\ntask(\n    task=\"Analyze the code quality and identify potential issues\",\n    agent=\"code-analyzer\"\n)\n\n\nExpected Output\n🤖 Running code-analyzer agent: Analyze the code quality and identify potential issues\n\n🔧 code-analyzer: Agent completed in 4 turns\n\nFound the following issues:\n1. Missing type hints in 15 functions\n2. Unused imports in 3 files\n3. Potential security issues in auth module",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#summary-3",
    "href": "tools_study_mistral-vibe.html#summary-3",
    "title": "Mistral vibe 2.0",
    "section": "Summary",
    "text": "Summary\nThe task tool is a powerful feature that enables:\n\nContext Management: Offload work to prevent context overload\nParallel Execution: Run multiple tasks independently\nSpecialization: Use different subagents for different tasks\nAutomation: Delegate repetitive or complex tasks\n\n\nKey Features\n\nIndependent execution with isolated configuration\nSecurity constraints preventing recursion\nResource monitoring with turn counting\nRich UI integration with progress display\nError handling with graceful degradation\n\n\n\nBest Practices\n\nWrite detailed task descriptions\nChoose appropriate subagents\nUse for complex, multi-step tasks\nTrust subagent autonomy\nMonitor resource usage\n\nBy understanding how the task tool works, you can effectively use it to manage complex workflows, delegate specialized tasks, and improve overall productivity in Mistral Vibe.",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#how-skills-work-in-mistral-vibe",
    "href": "tools_study_mistral-vibe.html#how-skills-work-in-mistral-vibe",
    "title": "Mistral vibe 2.0",
    "section": "How Skills Work in Mistral Vibe",
    "text": "How Skills Work in Mistral Vibe\nI’ve explored the skills system implementation and created a detailed document covering all aspects. Here’s what I found:\n\nCore Functionality\nSkills are reusable components that extend Vibe’s functionality through: - Documentation in the system prompt - Slash command integration (when user_invocable=true) - Tool restrictions (experimental feature) - Metadata-driven configuration\n\n\nSkill Structure\n\nLocation: Directories with SKILL.md files\nFormat: YAML frontmatter + markdown content\nDiscovery: Automatic from multiple paths\n\n\n\nSkill Metadata Fields\nRequired: - name (str): Skill identifier (must match directory name) - description (str): What the skill does\nOptional: - license (str): License information - compatibility (str): Environment requirements - metadata (dict): Arbitrary key-value pairs - allowed-tools (list): Pre-approved tools (experimental) - user-invocable (bool): Appear in slash command menu (default: true)\n\n\nDiscovery Paths\nSkills are discovered from: 1. Custom paths (from skill_paths in config) 2. Local project skills (.vibe/skills/) 3. Global skills (~/.vibe/skills/)\n\n\nIntegration Points\n\nSystem Prompt: Skills section with XML format\nSlash Commands: User-invocable skills appear in menu\nConfiguration: Enable/disable via patterns\n\n\n\nManagement\n\nEnable/Disable: Using enabled_skills and disabled_skills in config\nPattern Matching: Exact names, globs, and regex\nDuplicate Handling: First occurrence wins\n\n\n\nSecurity Features\n\nHTML escaping in system prompt\nTool restrictions (experimental)\nValidation of metadata fields\n\nThe document provides complete implementation details, examples, and best practices for creating and using skills.",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#complete-guide-to-the-skills-system",
    "href": "tools_study_mistral-vibe.html#complete-guide-to-the-skills-system",
    "title": "Mistral vibe 2.0",
    "section": "Complete Guide to the Skills System",
    "text": "Complete Guide to the Skills System\n\nOverview of the Skills System\nSkill Structure and Format\nSkill Discovery and Loading\nSkill Metadata Fields\nSkill Integration with the System\nSkill Discovery Paths\nSkill Management and Configuration\nSkills and Slash Commands\nBest Practices for Creating Skills\nComplete Example: Creating a Custom Skill",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#overview-of-the-skills-system",
    "href": "tools_study_mistral-vibe.html#overview-of-the-skills-system",
    "title": "Mistral vibe 2.0",
    "section": "Overview of the Skills System",
    "text": "Overview of the Skills System\nThe skills system in Mistral Vibe allows extending functionality through reusable components. Skills can add specialized behaviors, documentation, and can appear as slash commands in the UI.\n\nKey Characteristics\n\nReusable Components: Skills are self-contained units of functionality\nMetadata-Driven: Skills use YAML frontmatter for configuration\nDiscoverable: Skills are automatically discovered from multiple paths\nConfigurable: Skills can be enabled/disabled via patterns\nUser-Invocable: Skills can appear in slash command menu\n\n\n\nPurpose\nSkills serve several purposes:\n\nDocumentation: Provide context about available capabilities\nSpecialization: Define specialized behaviors for different domains\nSlash Commands: Appear as user-invocable commands in the UI\nTool Restrictions: Limit which tools a skill can use (experimental)",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#skill-structure-and-format",
    "href": "tools_study_mistral-vibe.html#skill-structure-and-format",
    "title": "Mistral vibe 2.0",
    "section": "Skill Structure and Format",
    "text": "Skill Structure and Format\nSkills are defined in directories with a SKILL.md file containing YAML frontmatter followed by markdown content.\n\nFile Structure\n~/.vibe/skills/\n└── my-skill/\n    └── SKILL.md\n\n\nFormat Specification\n---\n# YAML Frontmatter (Metadata)\nname: my-skill\ndescription: What this skill does\ndescription: MIT\ncompatibility: Python 3.12+\nuser-invocable: true\nallowed-tools:\n  - read_file\n  - grep\n---\n\n# Markdown Content\n## Skill Documentation\n\nDetailed description of what the skill does and how to use it.\n\n\nFrontmatter Requirements\n\nMust start and end with --- (at least 3 dashes)\nMust be valid YAML\nMust contain at least name and description\nMust be a dictionary/mapping",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#skill-discovery-and-loading",
    "href": "tools_study_mistral-vibe.html#skill-discovery-and-loading",
    "title": "Mistral vibe 2.0",
    "section": "Skill Discovery and Loading",
    "text": "Skill Discovery and Loading\nThe skill system uses a multi-stage discovery and loading process:\n\nDiscovery Process\n\nFind Skill Directories: Look for directories in search paths\nCheck for SKILL.md: Each directory must contain a SKILL.md file\nParse Frontmatter: Extract YAML metadata from the file\nValidate Metadata: Ensure required fields are present\nCreate SkillInfo: Build internal representation\nHandle Duplicates: Skip duplicates (log warning)\n\n\n\nLoading Code\ndef _parse_skill_file(self, skill_path: Path) -&gt; SkillInfo:\n    try:\n        content = skill_path.read_text(encoding=\"utf-8\")\n    except OSError as e:\n        raise SkillParseError(f\"Cannot read file: {e}\") from e\n\n    frontmatter, _ = parse_frontmatter(content)\n    metadata = SkillMetadata.model_validate(frontmatter)\n\n    skill_name_from_dir = skill_path.parent.name\n    if metadata.name != skill_name_from_dir:\n        logger.warning(\n            \"Skill name '%s' doesn't match directory name '%s' at %s\",\n            metadata.name,\n            skill_name_from_dir,\n            skill_path,\n        )\n\n    return SkillInfo.from_metadata(metadata, skill_path)\n\n\nError Handling\n\nMissing SKILL.md: Directory is skipped\nInvalid YAML: Warning logged, skill not loaded\nMissing required fields: Validation error\nDuplicate names: Later occurrence is skipped with warning",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#skill-metadata-fields-2",
    "href": "tools_study_mistral-vibe.html#skill-metadata-fields-2",
    "title": "Mistral vibe 2.0",
    "section": "Skill Metadata Fields",
    "text": "Skill Metadata Fields\n\nRequired Fields\n\nname (str): Skill identifier\n\nMust match directory name\nLowercase letters, numbers, and hyphens only\nPattern: ^[a-z0-9]+(-[a-z0-9]+)*$\nMax length: 64 characters\nMin length: 1 character\n\ndescription (str): What the skill does\n\nMust be at least 1 character\nMax length: 1024 characters\n\n\n\n\nOptional Fields\n\nlicense (str | None): License name or reference\n\nCan reference bundled license file\nNo length restrictions\n\ncompatibility (str | None): Environment requirements\n\nCan specify product, system packages, etc.\nMax length: 500 characters\n\nmetadata (dict[str, str]): Arbitrary key-value mapping\n\nKeys and values are converted to strings\nUsed for additional metadata\n\nallowed-tools (list[str]): Pre-approved tools (experimental)\n\nSpace-delimited list in YAML\nCan be list in YAML or space-separated string\nLimits which tools the skill can use\n\nuser-invocable (bool): Controls slash command menu appearance\n\nDefault: true\nWhen true: Skill appears in slash command menu\nWhen false: Skill is documentation-only\n\n\n\n\nField Validation\nThe skill metadata goes through Pydantic validation:\nclass SkillMetadata(BaseModel):\n    model_config = {\"populate_by_name\": True}\n\n    name: str = Field(\n        ...,\n        min_length=1,\n        max_length=64,\n        pattern=r\"^[a-z0-9]+(-[a-z0-9]+)*$\",\n        description=\"Skill identifier. Lowercase letters, numbers, and hyphens only.\",\n    )\n    description: str = Field(\n        ...,\n        min_length=1,\n        max_length=1024,\n        description=\"What this skill does and when to use it.\",\n    )\n    # ... other fields with validation",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#skill-integration-with-the-system",
    "href": "tools_study_mistral-vibe.html#skill-integration-with-the-system",
    "title": "Mistral vibe 2.0",
    "section": "Skill Integration with the System",
    "text": "Skill Integration with the System\n\nSystem Prompt Integration\nSkills are included in the system prompt when include_prompt_detail is enabled:\ndef get_universal_system_prompt(\n    tool_manager: ToolManager,\n    config: VibeConfig,\n    skill_manager: SkillManager,\n    agent_manager: AgentManager,\n) -&gt; str:\n    sections = [config.system_prompt]\n    \n    # ... other sections\n    \n    if config.include_prompt_detail:\n        # ... tool prompts\n        \n        skills_section = _get_available_skills_section(skill_manager)\n        if skills_section:\n            sections.append(skills_section)\n        \n        # ... other sections\n    \n    return \"\\n\\n\".join(sections)\n\n\nSkill Section Format\n# Available Skills\n\nYou have access to the following skills. When a task matches a skill's description,\nread the full SKILL.md file to load detailed instructions.\n\n&lt;available_skills&gt;\n  &lt;skill&gt;\n    &lt;name&gt;skill-name&lt;/name&gt;\n    &lt;description&gt;Skill description&lt;/description&gt;\n    &lt;path&gt;/path/to/skill/SKILL.md&lt;/path&gt;\n  &lt;/skill&gt;\n&lt;/available_skills&gt;\n\n\nHTML Escaping\nSkill metadata is HTML-escaped before inclusion in the prompt to prevent injection:\nlines.append(f\"    &lt;name&gt;{html.escape(str(name))}&lt;/name&gt;\")\nlines.append(\n    f\"    &lt;description&gt;{html.escape(str(info.description))}&lt;/description&gt;\"\n)\nlines.append(f\"    &lt;path&gt;{html.escape(str(info.skill_path))}&lt;/path&gt;\")",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#skill-discovery-paths-1",
    "href": "tools_study_mistral-vibe.html#skill-discovery-paths-1",
    "title": "Mistral vibe 2.0",
    "section": "Skill Discovery Paths",
    "text": "Skill Discovery Paths\nThe skill system searches for skills in multiple locations:\n\nDefault Search Paths\n\nGlobal Skills Directory: ~/.vibe/skills/\nLocal Project Skills: .vibe/skills/ in current working directory\nCustom Paths: Configured in config.toml via skill_paths\n\n\n\nDiscovery Order\nSkills are discovered in this order (later paths override earlier ones): 1. Custom paths (from skill_paths) 2. Local project skills (.vibe/skills/) 3. Global skills (~/.vibe/skills/)\n\n\nPath Resolution\n@staticmethod\ndef _compute_search_paths(config: VibeConfig) -&gt; list[Path]:\n    paths: list[Path] = []\n\n    for path in config.skill_paths:\n        if path.is_dir():\n            paths.append(path)\n\n    if (skills_dir := resolve_local_skills_dir(Path.cwd())) is not None:\n        paths.append(skills_dir)\n\n    if GLOBAL_SKILLS_DIR.path.is_dir():\n        paths.append(GLOBAL_SKILLS_DIR.path)\n\n    unique: list[Path] = []\n    for p in paths:\n        rp = p.resolve()\n        if rp not in unique:\n            unique.append(rp)\n\n    return unique\n\n\nDuplicate Handling\nIf the same skill is found in multiple locations: - The first occurrence is kept - Later occurrences are skipped - A debug message is logged",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#skill-management-and-configuration",
    "href": "tools_study_mistral-vibe.html#skill-management-and-configuration",
    "title": "Mistral vibe 2.0",
    "section": "Skill Management and Configuration",
    "text": "Skill Management and Configuration\n\nEnabling and Disabling Skills\nSkills can be enabled or disabled using patterns in configuration:\n# ~/.vibe/config.toml\n\n# Enable specific skills\nenabled_skills = [\"code-review\", \"test-*\"]\n\n# Disable specific skills\ndisabled_skills = [\"experimental-*\"]\n\n\nPattern Matching\nVibe supports three types of patterns:\n\nExact names: \"code-review\" matches only “code-review”\nGlob patterns: \"test-*\" matches “test-1”, “test-2”, etc.\nRegex patterns: \"re:^serena_.*$\" matches regex pattern\n\n\n\nConfiguration Logic\n@property\ndef available_skills(self) -&gt; dict[str, SkillInfo]:\n    if self._config.enabled_skills:\n        return {\n            name: info\n            for name, info in self._available.items()\n            if name_matches(name, self._config.enabled_skills)\n        }\n    if self._config.disabled_skills:\n        return {\n            name: info\n            for name, info in self._available.items()\n            if not name_matches(name, self._config.disabled_skills)\n        }\n    return dict(self._available)\n\n\nPriority Rules\n\nIf enabled_skills is set: Only those skills are available\nIf disabled_skills is set (and enabled_skills not set): All skills except disabled ones\nIf neither is set: All discovered skills are available",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#skills-and-slash-commands",
    "href": "tools_study_mistral-vibe.html#skills-and-slash-commands",
    "title": "Mistral vibe 2.0",
    "section": "Skills and Slash Commands",
    "text": "Skills and Slash Commands\nSkills can appear as slash commands in the UI when user_invocable is true.\n\nUser-Invocable Skills\n---\nname: code-review\ndescription: Perform automated code reviews\nlicense: MIT\ncompatibility: Python 3.12+\nuser-invocable: true  # This makes it appear in slash command menu\nallowed-tools:\n  - read_file\n  - grep\n  - ask_user_question\n---\n\n\nSlash Command Format\nWhen a skill is user-invocable, it appears in the slash command menu as: /skill-name or /skill name (spaces replaced with hyphens)\n\n\nInvocation Behavior\nWhen a user invokes a skill via slash command: 1. The skill’s SKILL.md file is read 2. The description and instructions are displayed 3. The agent can use the skill’s allowed tools 4. The skill’s context is included in the conversation",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#best-practices-for-creating-skills",
    "href": "tools_study_mistral-vibe.html#best-practices-for-creating-skills",
    "title": "Mistral vibe 2.0",
    "section": "Best Practices for Creating Skills",
    "text": "Best Practices for Creating Skills\n\n1. Clear and Descriptive Names\n\nUse lowercase with hyphens: code-review, test-generator\nBe specific about the skill’s purpose\nAvoid generic names like helper or assistant\n\n\n\n2. Comprehensive Descriptions\n\nExplain what the skill does\nInclude when to use it\nMention any prerequisites or requirements\n\n\n\n3. Detailed Documentation\n\nProvide clear instructions in the markdown body\nInclude examples of usage\nDocument any limitations or constraints\n\n\n\n4. Appropriate Tool Restrictions\n\nList only the tools needed for the skill\nAvoid giving unnecessary permissions\nConsider security implications\n\n\n\n5. Versioning and Compatibility\n\nSpecify compatibility requirements\nDocument breaking changes\nConsider versioning your skills\n\n\n\n6. Testing\n\nTest your skill in different scenarios\nVerify it works with the allowed tools\nCheck that the documentation is clear",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#complete-example-creating-a-custom-skill",
    "href": "tools_study_mistral-vibe.html#complete-example-creating-a-custom-skill",
    "title": "Mistral vibe 2.0",
    "section": "Complete Example: Creating a Custom Skill",
    "text": "Complete Example: Creating a Custom Skill\n\nStep 1: Create Skill Directory\nmkdir -p ~/.vibe/skills/code-review\n\n\nStep 2: Create SKILL.md File\nnano ~/.vibe/skills/code-review/SKILL.md\n\n\nStep 3: Define Skill Metadata and Documentation\n---\nname: code-review\ndescription: Perform automated code reviews to identify issues and suggest improvements\ndescription: MIT\ncompatibility: Python 3.12+\nuser-invocable: true\nallowed-tools:\n  - read_file\n  - grep\n  - ask_user_question\n---\n\n# Code Review Skill\n\nThis skill helps analyze code quality and suggest improvements.\n\n## Features\n\n- Identifies common code smells\n- Checks for missing documentation\n- Suggests refactoring opportunities\n- Verifies coding standards compliance\n\n## Usage\n\nWhen performing a code review, this skill will:\n\n1. Analyze the code structure\n2. Check for common issues\n3. Suggest improvements\n4. Provide detailed feedback\n\n## Limitations\n\n- Only analyzes Python code\n- Requires proper file permissions\n- May miss context-specific issues\n\n## Examples\n\n/code-review\n\n🤖 I’ll perform a code review of the current project.\n\nread_file(path=“src/main.py”) grep(pattern=“TODO”, path=“src/”) ask_user_question(questions=[{ “question”: “What coding standards should I check?”, “options”: [ {“label”: “PEP 8”, “description”: “Python style guide”}, {“label”: “Custom”, “description”: “Project-specific rules”}] }])\n\n\n\n\nStep 4: Verify Skill Discovery\nvibe\nCheck that the skill appears in the system prompt or slash command menu.\n\n\nStep 5: Test the Skill\n&gt; /code-review\nOr let the agent use it automatically when appropriate.",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#advanced-project-specific-skills",
    "href": "tools_study_mistral-vibe.html#advanced-project-specific-skills",
    "title": "Mistral vibe 2.0",
    "section": "Advanced: Project-Specific Skills",
    "text": "Advanced: Project-Specific Skills\nYou can also create skills specific to a project:\n# In your project directory\nmkdir -p .vibe/skills/project-docs\nnano .vibe/skills/project-docs/SKILL.md\n---\nname: project-docs\ndescription: Generate and maintain project-specific documentation\ndescription: MIT\ncompatibility: Python 3.12+\nuser-invocable: true\nallowed-tools:\n  - read_file\n  - grep\n  - write_file\n---\n\n# Project Documentation Skill\n\nThis skill helps generate documentation specific to this project.\n\n## Features\n\n- Analyzes project structure\n- Generates API documentation\n- Creates README files\n- Updates documentation based on code changes\n\n## Project-Specific Rules\n\n- Follow the project's documentation style\n- Include project-specific examples\n- Reference project-specific components",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "tools_study_mistral-vibe.html#summary-4",
    "href": "tools_study_mistral-vibe.html#summary-4",
    "title": "Mistral vibe 2.0",
    "section": "Summary",
    "text": "Summary\nThe skills system in Mistral Vibe provides a powerful way to extend functionality:\n\nSkill Structure: Directories with SKILL.md files containing YAML frontmatter\nDiscovery: Automatic discovery from multiple paths\nMetadata: Comprehensive metadata with validation\nIntegration: System prompt and slash command integration\nConfiguration: Enable/disable via patterns\nSecurity: Tool restrictions and HTML escaping\n\n\nKey Benefits\n\nReusability: Skills can be shared across projects\nDocumentation: Built-in documentation system\nExtensibility: Easy to add new skills\nControl: Fine-grained enable/disable patterns\nUser-Friendly: Slash command integration\n\nBy understanding how skills work, you can create custom skills to extend Mistral Vibe’s capabilities for your specific needs and workflows.",
    "crumbs": [
      "Mistral vibe 2.0"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "wordslab-notebooks-lib",
    "section": "",
    "text": "wordslab-notebooks-lib is a companion library for wordslab-notebooks.\nwordslab-notebooks is a one click install of all the tools you need to learn, explore and build AI applications on your own machine. - Recent and compatible versions of the best open source tools - Optimized work environment to save disk space and memory - Options to leverage your own machines at home or to rent more powerful machines in the cloud - Documentation to guide you in your AI learning and exploration journey\n3 main applications - A rich chat interface (text, images, voice) : Open WebUI - A notebooks platform (text & code) : JupyterJab + Jupyter AI extension - A development environment (code) : Visual Studio Code + Continue.dev extension + Aider terminal agent\nFully integrated AI environment with - A visual dashboard to help you navigate all applications and manage your machine resources - Optimized inference engines to run AI models : Ollama + vLLM\nwordslab-notebooks-lib is meant to be used in Jupyterlab notebooks or python programs running inside the wordslab-notebooks environment.\nIt provides many helper functions to easily access and use all the applications and resources of the environment.",
    "crumbs": [
      "wordslab-notebooks-lib"
    ]
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "wordslab-notebooks-lib",
    "section": "",
    "text": "wordslab-notebooks-lib is a companion library for wordslab-notebooks.\nwordslab-notebooks is a one click install of all the tools you need to learn, explore and build AI applications on your own machine. - Recent and compatible versions of the best open source tools - Optimized work environment to save disk space and memory - Options to leverage your own machines at home or to rent more powerful machines in the cloud - Documentation to guide you in your AI learning and exploration journey\n3 main applications - A rich chat interface (text, images, voice) : Open WebUI - A notebooks platform (text & code) : JupyterJab + Jupyter AI extension - A development environment (code) : Visual Studio Code + Continue.dev extension + Aider terminal agent\nFully integrated AI environment with - A visual dashboard to help you navigate all applications and manage your machine resources - Optimized inference engines to run AI models : Ollama + vLLM\nwordslab-notebooks-lib is meant to be used in Jupyterlab notebooks or python programs running inside the wordslab-notebooks environment.\nIt provides many helper functions to easily access and use all the applications and resources of the environment.",
    "crumbs": [
      "wordslab-notebooks-lib"
    ]
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "wordslab-notebooks-lib",
    "section": "Getting started",
    "text": "Getting started\n\nInstallation\n$ pip install wordslab_notebooks_lib\n\n\nDocumentation\nPlease read the documentation to discover all the features of the library:\nhttps://wordslab-org.github.io/wordslab-notebooks-lib/\n\n\nFirst steps\nThe WordslabNotebooks class is the main entrypoint to discover all wordslab-notebooks resources.\n\nfrom wordslab_notebooks_lib.core import *\n\nwlnb = WordslabNotebooks()\n\nwordslab-notebooks version\n\nwlnb.version\n\n'2025-11'\n\n\nwordslab-notebooks external urls\n\nwlnb.url_dashboard\n\n'http://192.168.1.197:8888'\n\n\n\nwlnb.url_userapp1\n\n'https://192.168.1.197:8883'\n\n\nwordslab-notebooks internal ports\n\nwlnb.port_ollama\n\n11434\n\n\n\nwlnb.port_userapp1\n\n'8883'\n\n\nwordslab-notebooks install directories\n\nwlnb.dir_scripts\n\n'/home/wordslab-notebooks-2025-11'\n\n\n\nwlnb.dir_workspace\n\n'/home/workspace'\n\n\nwordslab-notebooks applications data directories\n\nwlnb.dir_openwebui_data\n\n'/home/workspace/.openwebui'\n\n\n\nwlnb.dir_jupyterlab_data\n\n'/home/workspace/.jupyter'\n\n\nwordslab-notebooks models directories\n\nwlnb.dir_models_ollama\n\n'/home/models/ollama'\n\n\n\nwlnb.dir_models_vllm\n\n'/home/models/huggingface'\n\n\nwordslab-notebooks default models\n\nwlnb.default_model_chat\n\n'gemma3:27b'\n\n\n\nwlnb.default_model_code\n\n'qwen3:30b'",
    "crumbs": [
      "wordslab-notebooks-lib"
    ]
  },
  {
    "objectID": "tools_study_answerai.html",
    "href": "tools_study_answerai.html",
    "title": "Answerai tools study",
    "section": "",
    "text": "# -- fastcore.tools --\n# Implementation of Anthropic file and text editor tools\n\ndef rg(\n    argstr:str, # All args to the command, will be split with shlex\n    disallow_re:str=None, # optional regex which, if matched on argstr, will disallow the command\n    allow_re:str=None # optional regex which, if not matched on argstr, will disallow the command\n):\n    \"Run the `rg` command with the args in `argstr`\"\n\ndef sed(\n    argstr:str, # All args to the command, will be split with shlex\n    disallow_re:str=None, # optional regex which, if matched on argstr, will disallow the command\n    allow_re:str=None # optional regex which, if not matched on argstr, will disallow the command\n):\n    \"Run the `sed` command with the args in `argstr` (e.g for reading a section of a file)\"\n\ndef view(\n    path:str, # Path to directory or file to view\n    view_range:tuple[int,int]=None, # Optional 1-indexed (start, end) line range for files, end=-1 for EOF. Do NOT use unless it's known that the file is too big to keep in context—simply view the WHOLE file when possible\n    nums:bool=False, # Whether to show line numbers\n    skip_folders:tuple[str,...]=('_proc','__pycache__') # Folder names to skip when listing directories\n):\n    'View directory or file contents with optional line range and numbers'\n\ndef create(\n    path: str, # Path where the new file should be created\n    file_text: str, # Content to write to the file\n    overwrite:bool=False # Whether to overwrite existing files\n) -&gt; str:\n    'Creates a new file with the given content at the specified path'\n\ndef insert(\n    path: str, # Path to the file to modify\n    insert_line: int, # Line number where to insert (0-based indexing)\n    new_str: str # Text to insert at the specified line\n) -&gt; str:\n    'Insert new_str at specified line number'\n\ndef str_replace(\n    path: str, # Path to the file to modify\n    old_str: str, # Text to find and replace\n    new_str: str # Text to replace with\n) -&gt; str:\n    'Replace first occurrence of old_str with new_str in file'\n\ndef strs_replace(\n    path:str, # Path to the file to modify\n    old_strs:list[str], # List of strings to find and replace\n    new_strs:list[str], # List of replacement strings (must match length of old_strs)\n):\n    \"Replace for each str pair in old_strs,new_strs\"\n    res = [str_replace(path, old, new) for (old,new) in\n\ndef replace_lines(\n    path:str, # Path to the file to modify\n    start_line:int, # Starting line number to replace (1-based indexing)\n    end_line:int, # Ending line number to replace (1-based indexing, inclusive)\n    new_content:str, # New content to replace the specified lines\n):\n    \"Replace lines in file using start and end line-numbers (index starting at 1)\"\n\ndef move_lines(\n    path: str,  # Path to the file to modify\n    start_line: int,  # Starting line number to move (1-based)\n    end_line: int,  # Ending line number to move (1-based, inclusive)\n    dest_line: int,  # Destination line number (1-based, where lines will be inserted before)\n) -&gt; str:\n    \"Move lines from start_line:end_line to before dest_line\"\n\n\n# -- claudette.text_editor --\n# Other implementation of Anthropic text editor tools\n\ndef view(path:str,  # The path to the file or directory to view\n         view_range:tuple[int,int]=None, # Optional array of two integers specifying the start and end line numbers to view. Line numbers are 1-indexed, and -1 for the end line means read to the end of the file. This parameter only applies when viewing files, not directories.\n         nums:bool=False # Optionally prefix all lines of the file with a line number\n        ) -&gt; str:\n    'Examine the contents of a file or list the contents of a directory. It can read the entire file or a specific range of lines. With or without line numbers.'\n\ndef create(path: str, # The path where the new file should be created\n           file_text: str, # The text content to write to the new file\n           overwrite:bool=False # Allows overwriting an existing file\n          ) -&gt; str:\n    'Creates a new file with the given text content at the specified path'\n\n\ndef insert(path: str,  # The path to the file to modify\n           insert_line: int, # The line number after which to insert the text (0 for beginning of file)\n           new_str: str # The text to insert\n          ) -&gt; str: \n    'Insert text at a specific line number in a file.'\n\ndef str_replace(path: str, # The path to the file to modify\n                old_str: str, # The text to replace (must match exactly, including whitespace and indentation)\n                new_str: str # The new text to insert in place of the old text\n               ) -&gt; str:\n    'Replace a specific string in a file with a new string. This is used for making precise edits.'\n\n\n# -- ipykernelhelper --\n# Get the html content of a web page using the cloudscraper library to bypass Cloudflare's anti-bot page\n# and convert it to markdown prepared for insertion in the LLM context\n\ndef read_url(url: str, as_md: bool = True, \n             extract_section: bool = True, selector: str = None,\n             math_mode: str = None):\n    \"\"\"This functions extracts a web page information for LLM ingestion\n    1. Downloads a web page\n    2. Parses HTML\n    3. Optionally extracts a specific section (fragment or CSS selector)\n    4. Converts MathML → LaTeX\n    5. Optionally converts HTML → Markdown\n    6. Convert code sections to fenced markdown blocks\n    7. Makes image URLs absolute\n    8. Returns the processed text\n    \"\"\"\n\n\n# -- dialoghelper.core --\n# Tools to edit the Jupyter notebook\n\ndef dialoghelper_explain_dialog_editing(\n)-&gt;str: # Detailed documention on dialoghelper dialog editing\n    \"Call this to get a detailed explanation of how dialog editing is done in dialoghelper. Always use if doing anything non-trivial, or if dialog editing has not previously occured in this session\"\n    return \"\"\"# dialoghelper dialog editing functionality\n\nThis guide consolidates understanding of how dialoghelper tools work together. Individual tool schemas are already in context—this adds architectural insight and usage patterns.\n\n## Core Concepts\n\n- **Dialog addressing**: All functions accepting `dname` resolve paths relative to current dialog (no leading `/`) or absolute from Solveit's runtime data path (with leading `/`). The `.ipynb` extension is never included.\n- **Message addressing**: Messages have stable `id` strings (e.g., `_a9cb5512`). The current executing message's id is in `__msg_id`. Tools use `id` for targeting; `find_msg_id()` retrieves current.\n- **Implicit state**: After `add_msg`/`update_msg`, `__msg_id` is updated to the new/modified message. This enables chaining: successive `add_msg` calls create messages in sequence.\n\n## Tool Workflow Patterns\n\n### Reading dialog state\n- `view_dlg` — fastest way to see entire dialog structure with line numbers for editing\n- `find_msgs` — search with regex, filter by type/errors/changes\n- `read_msg` — navigate relative to current message\n- `read_msgid` — direct access when you have the id\n\n**Key insight**: Messages above the current prompt are already in LLM context. Use read tools only for: (1) getting line numbers for edits, (2) accessing messages below current prompt, (3) accessing other dialogs.\n\n### Modifying dialogs\n- `add_msg` — placement can be `add_after`/`add_before` (relative to current) or `at_start`/`at_end` (absolute)\n  - **NB** When not passing a message id, it defaults to the *current* message. So if you call it multiple times with no message id, the messages will be added in REVERSE! Instead, get the return value of `add_msg` after each call, and use that for the next call\n- `update_msg` — partial updates; only pass fields to change\n- `del_msg` — use sparingly, only when explicitly requested\n`copy_msg` → `paste_msg` — for moving/duplicating messages within running dialogs.\n\n## Non-decorated Functions Worth Knowing\n\nThere are additional functions available that can be added to fenced blocks, or the user may add as tools; they are not included in schemas by default.\n\n**Browser integration:**\n- `add_html(content)` — inject HTML with `hx-swap-oob` into live browser DOM\n- `iife(code)` — execute JavaScript immediately in browser\n- `fire_event(evt, data)` / `event_get(evt)` — trigger/await browser events\n\n**Content helpers:**\n- `url2note(url, ...)` — fetch URL as markdown, add as note message\n- `mermaid(code)` / `enable_mermaid()` — render mermaid diagrams\n- `add_styles(s)` — apply solveit's MonsterUI styling to HTML\n\n**Dangerous (not exposed by default):**\n- `_add_msg_unsafe(content, run=True, ...)` — add AND execute message (code or prompt)\n- `run_msg(ids)` — queue messages for execution\n- `rm_dialog(name)` — delete entire dialog\n\n## Important Patterns\n\n### Key Principles\n\n1. **Always re-read before editing.** Past tool call results in chat history are TRUNCATED. Never rely on line numbers from earlier in the conversation—call `read_msgid(id, nums=True)` immediately before any edit operation.\n2. **Work backwards.** When making multiple edits to a message, start from the end and work towards the beginning. This prevents line number shifts from invalidating your planned edits.\n3. **Don't guess when tools fail.** If a tool call returns an error, STOP and ask for clarification. Do not retry with guessed parameters.\n4. **Verify after complex edits.** After significant changes, re-read the affected region to confirm the edit worked as expected before proceeding.\n\n### Typical Workflow\n\n```\n1. read_msgid(id, nums=True)           # Get current state with line numbers\n2. Identify lines to change\n3. msg_replace_lines(...) or msg_str_replace(...)  # Make edit\n4. If more edits needed: re-read, then repeat from step 2\n```\n\n### Tool Selection\n\n- **`msg_replace_lines`**: Best for replacing/inserting contiguous blocks. Use `view_range` on read to focus on the area.\n- **`msg_str_replace`**: Best for targeted single small string replacements when you know the exact text.\n- **`msg_strs_replace`**: Best for multiple small independent replacements in one call.\n- **`msg_insert_line`**: Best for adding new content without replacing existing lines.\n- **`msg_del_lines`**: Best for removing content.\n\n**Rough rule of thumb:** Prefer `msg_replace_lines` over `msg_str(s)_replace` unless there's &gt;1 match to change or it's just a word or two. Use the insert/delete functions for inserting/deleting; don't use `msg_str(s)_replace` for that.\n\n### Common Mistakes to Avoid\n\n- Using line numbers from a truncated earlier result\n- Making multiple edits without re-reading between them\n- Guessing line numbers when a view_range was truncated\n- Always call `read_msgid(id, nums=True)` first to get accurate line numbers\n- String-based tools (`msg_str_replace`, `msg_strs_replace`) fail if the search string appears zero or multiple times—use exact unique substrings.\"\"\"\n\ndef curr_dialog(\n    with_messages:bool=False,  # Include messages as well?\n    dname:str='' # Dialog to get info for; defaults to current dialog\n):\n    \"Get the current dialog info.\"\n\ndef view_dlg(\n    dname:str='', # Dialog to get info for; defaults to current dialog\n    msg_type:str=None, # optional limit by message type ('code', 'note', or 'prompt')\n    nums:bool=False, # Whether to show line numbers\n    include_output:bool=False, # Include output in returned dict?\n    trunc_out:bool=True, # Middle-out truncate code output to 100 characters (only applies if `include_output`)?\n    trunc_in:bool=False, # Middle-out truncate cell content to 80 characters?\n):\n    \"Concise XML view of all messages (optionally filtered by type), not including metadata. Often it is more efficient to call this to see the whole dialog at once (including line numbers if needed), instead of running `find_msgs` or `read_msg` multiple times.\"\n    return find_msgs(msg_type=msg_type, dname=dname, as_xml=True, nums=nums,\n        include_meta=False, include_output=include_output, trunc_out=trunc_out, trunc_in=trunc_in)\n\ndef msg_idx(\n    id:str=None,  # Message id to find (defaults to current message)\n    dname:str='' # Dialog to get message index from; defaults to current dialog\n):\n    \"Get absolute index of message in dialog.\"\n\ndef find_msgs(\n    re_pattern:str='', # Optional regex to search for (re.DOTALL+re.MULTILINE is used)\n    msg_type:str=None, # optional limit by message type ('code', 'note', or 'prompt')\n    use_case:bool=False, # Use case-sensitive matching?\n    use_regex:bool=True, # Use regex matching?\n    only_err:bool=False, # Only return messages that have errors?\n    only_exp:bool=False, # Only return messages that are exported?\n    only_chg:bool=False, # Only return messages that have changed vs git HEAD?\n    ids:str='', # Optionally filter by comma-separated list of message ids\n    limit:int=None, # Optionally limit number of returned items\n    include_output:bool=True, # Include output in returned dict?\n    include_meta:bool=True, # Include all additional message metadata\n    as_xml:bool=False, # Use concise unescaped XML output format\n    nums:bool=False, # Show line numbers?\n    trunc_out:bool=False, # Middle-out truncate code output to 100 characters?\n    trunc_in:bool=False, # Middle-out truncate cell content to 80 characters?\n    headers_only:bool=False, # Only return note messages that are headers (first line only); cannot be used together with `header_section`\n    header_section:str=None, # Find section starting with this header; returns it plus all children (i.e until next header of equal or more significant level)\n    dname:str='' # Dialog to get info for; defaults to current dialog\n)-&gt;list[dict]: # Messages in requested dialog that contain the given information\n    \"\"\"Often it is more efficient to call `view_dlg` to see the whole dialog at once, so you can use it all from then on, instead of using `find_msgs`.\n    {dname}\n    Message ids are identical to those in LLM chat history, so do NOT call this to view a specific message if it's in the chat history--instead use `read_msgid`.\n    Do NOT use find_msgs to view message content in the current dialog above the current prompt -- these are *already* provided in LLM context, so just read the content there directly. (NB: LLM context only includes messages *above* the current prompt, whereas `find_msgs` can access *all* messages.)\n    To refer to a found message from code or tools, use its `id` field.\"\"\"\n\ndef read_msg(\n    n:int=-1,      # Message index (if relative, +ve is downwards)\n    relative:bool=True,  # Is `n` relative to current message (True) or absolute (False)?\n    id:str=None,  # Message id to find (defaults to current message)\n    view_range:list[int,int]=None, # Optional 1-indexed (start, end) line range for files, end=-1 for EOF\n    nums:bool=False, # Whether to show line numbers\n    dname:str='' # Dialog to get info for; defaults to current dialog\n    ):\n    \"\"\"Get the message indexed in the current dialog.\n    NB: Messages in the current dialog above the current message are *already* visible; use this only when you need line numbers for editing operations, or for messages not in the current dialog or below the current message.\n    - To get the exact message use `n=0` and `relative=True` together with `id`.\n    - To get a relative message use `n` (relative position index).\n    - To get the nth message use `n` with `relative=False`, e.g `n=0` first message, `n=-1` last message.\n    {dname}\"\"\"\n\ndef read_msgid(\n    id:str,  # Message id to find\n    view_range:list[int,int]=None, # Optional 1-indexed (start, end) line range for files, end=-1 for EOF\n    nums:bool=False, # Whether to show line numbers\n    dname:str='' # Dialog to get message from; defaults to current dialog\n    ):\n    \"\"\"Get message `id`. Message IDs can be view directly in LLM chat history/context, or found in `find_msgs` results.\"\"\"\n\ndef add_msg(\n    content:str, # Content of the message (i.e the message prompt, code, or note text)\n    placement:str='add_after', # Can be 'at_start' or 'at_end', and for default dname can also be 'add_after' or 'add_before'\n    id:str=None, # id of message that placement is relative to (if None, uses current message; note: each add_msg updates \"current\" to the newly created message)\n    msg_type: str='note', # Message type, can be 'code', 'note', or 'prompt'\n    output:str='', # Prompt/code output; Code outputs must be .ipynb-compatible JSON array\n    time_run: str | None = '', # When was message executed\n    is_exported: int | None = 0, # Export message to a module?\n    skipped: int | None = 0, # Hide message from prompt?\n    i_collapsed: int | None = 0, # Collapse input?\n    o_collapsed: int | None = 0, # Collapse output?\n    heading_collapsed: int | None = 0, # Collapse heading section?\n    pinned: int | None = 0, # Pin to context?\n    dname:str='' # Dialog to get info for; defaults to current dialog. If passed, provide `id` or use `placement='at_start'`/`'at_end'`\n)-&gt;str: # Message ID of newly created message\n    \"\"\"Add/update a message to the queue to show after code execution completes.\n    **NB**: when creating multiple messages in a row, after the 1st message set `id` to the result of the last `add_msg` call,\n    otherwise messages will appear in the dialog in REVERSE order.\n    {dname}\"\"\"\n\ndef del_msg(\n    id:str=None, # id of message to delete\n    dname:str='', # Dialog to get info for; defaults to current dialog\n    log_changed:bool=False # Add a note showing the deleted content?\n):\n    \"Delete a message from the dialog. DO NOT USE THIS unless you have been explicitly instructed to delete messages.\"\n\ndef update_msg(\n    id:str=None, # id of message to update (if None, uses current message)\n    msg:Optional[Dict]=None, # Dictionary of field keys/values to update\n    dname:str='', # Dialog to get info for; defaults to current dialog\n    log_changed:bool=False, # Add a note showing the diff?\n    **kwargs):\n    \"\"\"Update an existing message. Provide either `msg` OR field key/values to update.\n    - Use `content` param to update contents.\n    - Only include parameters to update--missing ones will be left unchanged.\n    {dname}\"\"\"\n\ndef run_msg(\n    ids:str=None, # Comma-separated ids of message(s) to execute\n    dname:str='' # Running dialog to get info for; defaults to current dialog. (Note dialog *must* be running for this function)\n):\n    \"Adds a message to the run queue. Use read_msg to see the output once it runs.\"\n\ndef copy_msg(\n    ids:str=None, # Comma-separated ids of message(s) to copy\n    cut:bool=False, # Cut message(s)? (If not, copies)\n    dname:str='' # Running dialog to copy messages from; defaults to current dialog. (Note dialog *must* be running for this function)\n):\n    \"Add `ids` to clipboard.\"\n\ndef paste_msg(\n    id:str=None, # Message id to paste next to\n    after:bool=True, # Paste after id? (If not, pastes before)\n    dname:str='' # Running dialog to copy messages from; defaults to current dialog. (Note dialog *must* be running for this function)\n):\n    \"Paste clipboard msg(s) after/before the current selected msg (id).\"\n\ndef run_code_interactive(\n    code:str # Code to have user run\n):\n    \"\"\"Insert code into user's dialog and request for the user to run it. Use other tools where possible, \n    but if they can not find needed information, *ALWAYS* use this instead of guessing or giving up.\n    IMPORTANT: This tool is TERMINAL - after calling it, you MUST stop all tool usage \n    and wait for user response. Never call additional tools after this one.\"\"\"\n\ndef msg_insert_line(text, insert_line:int, new_str:str):\n    \"Insert text at specific line num in message. {besure}\\n{dname}\"\n    \ndef msg_str_replace(text, old_str:str, new_str:str):\n    \"Replace first occurrence of old_str with new_str in a message.\\n{dname}\"\n\ndef msg_strs_replace(text, old_strs:list[str], new_strs:list[str]):\n    \"Replace multiple strings simultaneously in a message.\\n{dname}\"\n\ndef msg_replace_lines(text, start_line:int, end_line:int=None, new_content:str=''):\n    \"Replace line range in msg with new content. {besure}\\n{dname}\"\n\ndef msg_del_lines(text, start_line:int, end_line:int=None):\n    \"Delete line range from a message. {besure}\\n{dname}\"\n\n\n# -- dialoghelper.core --\n# Import context and code\n\ndef url2note(\n    url:str, # URL to read\n    extract_section:bool=True, # If url has an anchor, return only that section\n    selector:str=None, # Select section(s) using BeautifulSoup.select (overrides extract_section)\n    ai_img:bool=True, # Make images visible to the AI\n    split_re:str='' # Regex to split content into multiple notes, set to '' for single note\n):\n    \"Read URL as markdown, and add note(s) below current message with the result\"\n\ndef ctx_folder(\n    path:Path='.',  # Path to collect\n    types:str|list='py,doc',  # list or comma-separated str of ext types from: py, js, java, c, cpp, rb, r, ex, sh, web, doc, cfg\n    out=False, # Include notebook cell outputs?\n    raw=True, # Add raw message, or note?\n    exts:str|list=None, # list or comma-separated str of exts to include (overrides `types`)\n    **kwargs\n):\n    \"Convert folder to XML context and place in a new message\"\n\ndef ctx_repo(\n    owner:str,  # GitHub repo owner\n    repo:str,   # GitHub repo name\n    types:str|list='py,doc',  # list or comma-separated str of ext types from: py, js, java, c, cpp, rb, r, ex, sh, web, doc, cfg\n    exts:str|list=None, # list or comma-separated str of exts to include (overrides `types`)\n    out=False, # Include notebook cell outputs?\n    raw=True, # Add raw message, or note?\n    **kwargs\n):\n    \"Convert GitHub repo to XML context and place in a new message\"\n\ndef ctx_symfile(sym):\n    \"Add note with filepath and contents for a symbol's source file\"\n\ndef ctx_symfile(sym):\n    \"Add note with filepath and contents for a symbol's source file\"\n\ndef ctx_symfolder(\n    sym, # Symbol to get folder context from\n    **kwargs):\n    \"Add raw message with folder context for a symbol's source file location\"\n\ndef ctx_sympkg(\n    sym, # Symbol to get folder context from\n    **kwargs):\n    \"Add raw message with repo context for a symbol's root package\"\n\ndef gist_file(gist_id:str):\n    \"Get the first file from a gist\"\n\ndef ast_grep(\n    pattern:str, # ast-grep pattern to search, e.g \"post($A, data=$B, $$$)\"\n    path:str=\".\", # path to recursively search for files\n    lang:str=\"python\" # language to search/scan\n): # json format from calling `ast-grep --json=compact\n    \"\"\"Use `ast-grep` to find code patterns by AST structure (not text).\n    \n    Pattern syntax:\n    - $VAR captures single nodes, $$$ captures multiple\n    - Match structure directly: `def $FUNC($$$)` finds any function; `class $CLASS` finds classes regardless of inheritance\n    - DON'T include `:` - it's concrete syntax, not AST structure\n    - Whitespace/formatting ignored - matches structural equivalence\n    \n    Examples: `import $MODULE` (find imports); `$OBJ.$METHOD($$$)` (find method calls); `await $EXPR` (find await expressions)\n    \n    Useful for: Refactoring—find all uses of deprecated APIs or changed signatures; Security review—locate SQL queries, file operations, eval calls; Code exploration—understand how libraries are used across codebase; Pattern analysis—find async functions, error handlers, decorators; Better than regex—handles multi-line code, nested structures, respects syntax\"\"\"\n\ndef import_string(\n    code:str, # Code to import as a module\n    name:str  # Name of module to create\n):\n\ndef import_gist(\n    gist_id:str, # user/id or just id of gist to import as a module\n    mod_name:str=None, # module name to create (taken from gist filename if not passed)\n    add_global:bool=True, # add module to caller's globals?\n    import_wildcard:bool=False, # import all exported symbols to caller's globals\n    create_msg:bool=False # Add a message that lists usable tools\n):\n    \"Import gist directly from string without saving to disk\"\n\ndef update_gist(gist_id:str, content:str):\n    \"Update the first file in a gist with new content\"\n\ntracetool is an LLM tool that traces function execution and captures variable snapshots after each line runs, using Python 3.12’s sys.monitoring for low-overhead tracing.\nFor each call to the target function (including recursive calls), the LLM receives a stack trace showing how that call was reached, plus a per-line trace dict mapping each executed source line to its hit count and variable snapshots. Variables that don’t change are shown as a single (type, repr) tuple, while those that evolve across iterations appear as a list of snapshots.\nThis is particularly useful when the LLM needs to understand unfamiliar code by seeing exactly what happens step-by-step, debug issues by watching how variables change, verify loop behavior by confirming iteration counts and accumulator values, or explore recursive functions where each call gets its own trace entry\n\n# -- dialoghelper.capture --\n# Screen capture\n\ndef capture_tool(timeout:int=15):\n    \"Capture the screen. Re-call this function to get the most recent screenshot, as needed. Use default timeout where possible\"\n\n\n# -- dialoghelper.tracetools --\n# Trace function execution\n\ndef tracetool(\n    sym: str,  # Dotted symbol path of callable to run\n    args: list=None,  # Positional args for callable (JSON values passed directly)\n    kwargs: dict=None,  # Keyword args for callable (JSON values passed directly)\n    target_func: str=None  # Dotted symbol path of function to trace (defaults to sym)\n)-&gt;list[tuple[str, dict[str, tuple[int, dict[str, tuple|list]]]]]:  # List of (stack_str,trace_dict); trace_dict maps source snippets to (hit_count, variables), unchanged vars collapsed to single tuple\n    \"\"\"\n    Trace execution using sys.monitoring (Python 3.12+), returning a list of per-call traces.\n\n    Return:\n    - list of length &lt;= 10\n    - one element per call to `target_func` (including recursion)\n    - each element is: (stack_str, trace_dict)\n        stack_str: call stack string (filtered so `fn` is the shallowest frame shown)\n        trace_dict: {\n        \"&lt;source snippet for AST-line&gt;\": ( hit_count, { \"var\": [ (type_name, truncated_repr), ... up to 10 ], ... } ),\n        ...}\n\n    Semantics:\n    - \"Line\" means an AST-level line: separate statements (even if on one physical line via `;`).\n    - Compound statements are keyed by their header only.\n    - Unchanged variables → `('type', 'repr')` tuple, changed variables → `[('type', 'repr'), ...]` list.\n    - Comprehensions are treated as a line node and are monitored, including inside the comprehension frame, with per-iteration snapshots.\n    - Snapshots are recorded after each line finishes, so assignments show updated values.\n    \"\"\"\n\n\n# -- dialoghelper.tmux --\n# Terminal : view tmux buffers\n\ndef pane(\n    n:int=None, # Number of scrollback lines to capture, in addition to visible area (None uses default_tmux_lines, which is 500 if not otherwise set)\n    pane:int=None,     # Pane number to capture from\n    session:str=None,  # Session name to target\n    window:int=None,   # Window number to target\n    **kwargs\n):\n    'Grab the tmux history in plain text'\n\ndef list_panes(\n    session:str=None,  # Session name to list panes from\n    window:int=None,   # Window number to list panes from\n    **kwargs\n):\n    'List panes for a session/window (or current if none specified)'\n\ndef panes(\n    session:str=None,  # Session name to target\n    window:int=None,   # Window number to target\n    n:int=None,        # Number of scrollback lines to capture\n    **kwargs\n):\n    'Grab history from all panes in a session/window'\n\ndef list_windows(\n    session:str=None,  # Session name to list windows from\n    **kwargs\n):\n    'List all windows in a session'\n\ndef windows(\n    session:str=None,  # Session name to target\n    n:int=None,        # Number of scrollback lines to capture\n    **kwargs\n):\n    'Grab history from all panes in all windows of a session'\n\ndef list_sessions(**kwargs):\n    'List all tmux sessions'\n\ndef sessions(\n    n:int=None,        # Number of scrollback lines to capture\n    **kwargs\n):\n    'Grab history from all panes in all windows of all sessions'\n\n\n# -- contextkit.read --\n\ndef read_text(url, # URL to read\n             ): # Text from page\n    \"Get text from `url`\"\n\ndef read_link(url: str,   # URL to read\n             heavy: bool = False,   # Use headless browser (requires extra setup steps before use)\n             sel: Optional[str] = None,  # Css selector to pull content from\n             useJina: bool = False, # Use Jina for the markdown conversion\n             ignore_links: bool = False, # Whether to keep links or not\n             ): \n    \"Reads a url and converts to markdown\"\n\ndef read_gist(url:str  # gist URL, of gist to read\n             ):\n    \"Returns raw gist content, or None\"\n\ndef read_gh_file(url:str # GitHub URL of the file to read\n                ):\n    \"Reads the contents of a file from its GitHub URL\"\n\ndef read_file(path:str):\n    \"returns file contents\"\n\ndef read_dir(path: str,                          # path to read\n             unicode_only: bool = True,             # ignore non-unicode files\n             included_patterns: List[str] = [\"*\"],       # glob pattern of files to include\n             excluded_patterns: List[str] = [\".git/**\"], # glob pattern of files to exclude\n             verbose: bool = False,                # log paths of files being read\n             as_dict: bool = False                  # returns dict of {path,content}\n            ) -&gt; Union[str, Dict[str, str]]:            # returns string with contents of files read\n    \"\"\"Reads files in path, returning a dict with the filenames and contents if as_dict=True, otherwise concatenating file contents into a single string. Takes optional glob patterns for files to include or exclude.\"\"\"\n\ndef read_pdf(file_path: str # path of PDF file to read\n            ) -&gt; str:\n    \"Reads the text of a PDF with PdfReader\"\n\ndef read_google_sheet(url: str # URL of a Google Sheet to read\n                     ):\n    \"Reads the contents of a Google Sheet into text\"\n\ndef read_gdoc(url: str  # URL of Google Doc to read\n             ):\n    \"Gets the text content of a Google Doc using html2text\"\n\ndef read_arxiv(url:str, # arxiv PDF URL, or arxiv abstract URL, or arxiv ID\n               save_pdf:bool=False, # True, will save the downloaded PDF\n               save_dir:str='.' # directory in which to save the PDF\n              ):\n    \"Get paper information from arxiv URL or ID, optionally saving PDF to disk\"\n\ndef read_gh_repo(path_or_url:str,    # Repo's GitHub URL, or GH SSH address, or file path\n                 as_dict:bool=True,  # if True, will return repo contents {path,content} dict\n                 verbose:bool=False  # if True, will log paths of files being read\n                ):\n    \"Repo contents from path, GH URL, or GH SSH address\"\n\n\n# -- llms-txt --\n\ndef parse_llms_file(txt):\n    \"Parse llms.txt file contents in `txt` to an `AttrDict`\"\n\ndef llms_txt2ctx(\n    fname:str, # File name to read\n    optional:bool_arg=False, # Include 'optional' section?\n    n_workers:int=None, # Number of threads to use for parallel downloading\n    save_nbdev_fname:str=None #save output to nbdev `{docs_path}` instead of emitting to stdout\n):\n    \"Print a `Project` with a `Section` for each H2 part in file read from `fname`, optionally skipping the 'optional' section.\"\n\n\n# -- playwrightnb --\n\ndef read_page(url, pause=50, timeout=5000, stealth=False, page=None):\n    \"Return contents of `url` and its iframes using Playwright\"\n\ndef url2md(url, sel=None, pause=50, timeout=5000, stealth=False, page=None):\n    \"Read `url` with `read_page`\"\n\ndef get2md(url, sel=None, **kwargs):\n    \"Read `url` with `httpx.get`\"\n\n\n# -- toolslm.xml --\n\ndef read_file(fname, max_size=None, sigs_only=False, **kwargs):\n    \"Read file content, converting notebooks to XML if needed\"\n\ndef files2ctx(\n    fnames:list[Union[str,Path]], # List of file names to add to context\n    srcs:Optional[list]=None, # Use the labels instead of `fnames`\n    max_size:int=None, # Skip files larger than this (bytes)\n    out:bool=True, # Include notebook cell outputs?\n    ids:bool=True,  # Include cell ids in notebooks?\n    nums:bool=False, # Include line numbers in notebook cell source?\n    sigs_only:bool=False, # Only include signatures and docstrings (where supported by `codesigs` lib)\n    **kwargs\n)-&gt;str: # XML for LM context\n    \"Convert files to XML context, handling notebooks\"\n\ndef folder2ctx(\n    path:Union[str,Path], # Folder to read\n    prefix:bool=False, # Include Anthropic's suggested prose intro?\n    out:bool=True, # Include notebook cell outputs?\n    include_base:bool=True, # Include full path in src?\n    title:str=None, # Optional title attr for Documents element\n    max_size:int=100_000, # Skip files larger than this (bytes)\n    max_total:int=10_000_000,  # Max total output size in bytes\n    readme_first:bool=False,  # Prioritize README files at start of context?\n    files_only:bool=False,  # Return dict of {filename: size} instead of context?\n    sigs_only:bool=False,  # Return signatures instead of full text? (where supported by `codesigs` lib)\n    ids:bool=True,  # Include cell ids in notebooks?\n    **kwargs\n)-&gt;Union[str,dict]:\n    \"Convert folder contents to XML context, handling notebooks\"\n\ndef sym2file(sym):\n    \"Return md string with filepath and contents for a symbol's source file\"\n\ndef sym2folderctx(\n    sym,\n    types:str|list='py',  # List or comma-separated str of ext types from: py, js, java, c, cpp, rb, r, ex, sh, web, doc, cfg\n    skip_file_re:str=r'^_mod', # Skip files matching regex\n    **kwargs\n):\n    \"Return folder context for a symbol's source file location\"\n\ndef sym2pkgpath(sym):\n    \"Get root package path for a symbol\"\n\ndef sym2pkgctx(\n    sym,\n    types:str|list='py', # List or comma-separated str of ext types from: py, js, java, c, cpp, rb, r, ex, sh, web, doc, cfg\n    skip_file_re:str=r'^_mod', # Skip files matching regex\n    skip_folder_re:str=r'^(\\.|__)', # Skip folders matching regex\n    **kwargs\n):\n    \"Return contents of files in a symbol's root package\"\n\ndef repo2ctx(\n    owner:str,  # GitHub repo owner or \"owner/repo\" or a full github URL\n    repo:str=None,   # GitHub repo name (leave empty if using \"owner/repo\" or URL format for owner param)\n    ref:str=None,  # Git ref (branch/tag/sha) (get from URL not provided); defaults to repo's default branch\n    folder:str=None,  # Only include files under this path (get from URL not provided)\n    show_filters:bool=True,  # Include filter info in title?\n    token:str=None,  # GitHub token (uses GITHUB_TOKEN env var if None)\n    **kwargs  # Passed to `folder2ctx`\n)-&gt;Union[str,dict]:  # XML for LM context, or dict of file sizes\n    \"Convert GitHub repo to XML context without cloning\"\n\n\n# -- toolslm.inspecttools --\n\ndef importmodule(\n    mod: str, # The module to import (e.g. 'torch.nn.functional')\n    caller_symbol:str = '__msg_id'  # The name of the special variable to find the correct caller namespace\n):\n    \"\"\"Import a module into the caller's global namespace so it's available for `symsrc`, `symval`, `symdir`, etc.\n    Use this before inspecting or using symbols from modules not yet imported.\"\"\"\n\ndef symsrc(\n    sym: str  # Dotted symbol path (e.g `Interval` or `sympy.sets.sets.Interval`) or \"_last\" for previous result\n):\n    \"\"\"Get the source code for a symbol.\n\n    Examples:\n\n    - `symsrc(\"Interval\")` -&gt; source code of Interval class if it's already imported\n    - `symsrc(\"sympy.sets.sets.Interval\")` -&gt; source code of Interval class\n    - `symsrc(\"_last\")` -&gt; source of object from previous tool call\n    - For dispatchers or registries of callables: `symnth(\"module.dispatcher.funcs\", n) then symsrc(\"_last\")`\"\"\"\n\ndef symtype(\n    syms: str  # Comma separated str list of dotted symbol paths (e.g `'Interval,a'` or `'sympy.sets.sets.Interval'`); \"_last\" for prev result\n):\n    \"\"\"Get the type of a symbol and set `_last`.\n\n    Examples:\n\n    - `symtype(\"sympy.sets.sets.Interval\")` -&gt; `&lt;class 'type'&gt;`\n    - `symtype(\"doesnotexist\")` -&gt; `'SymbolNotFound`\n    - `symtype(\"_last\")` -&gt; type of previous result\"\"\"\n    \ndef symval(\n    syms: str  # Comma separated str list of dotted symbol paths (e.g `Interval` or `sympy.sets.sets.Interval`); \"_last\" for prev result\n):\n    \"\"\"List of repr of symbols' values.\n\n    Examples:\n    \n    - `symval(\"sympy.sets.sets.Interval\")` -&gt; `[&lt;class 'sympy.sets.sets.Interval'&gt;]`\n    - `symval(\"some_dict.keys\")` -&gt; `[dict_keys([...])]`\n    - `symval(\"a,notexist\")` -&gt; `['foo','SymbolNotFound']`\"\"\"\n\ndef symdir(\n    sym: str,  # Dotted symbol path (e.g `Interval` or `sympy.sets.sets.Interval`) or \"_last\" for previous result\n    exclude_private: bool=False # Filter out attrs starting with \"_\"\n):\n    \"\"\"Get dir() listing of a symbol's attributes and set `_last`. E.g: `symdir(\"sympy.Interval\")` -&gt; `['__add__', '__and__', ...]`\"\"\"\n\n    def symnth(\n    sym: str,  # Dotted symbol path to a dict or object with .values()\n    n: int     # Index into the values (0-based)\n):\n    \"\"\"Get the nth value from a dict (or any object with .values()). Sets `_last` so you can chain with `symsrc(\"_last\")` etc.\n\n    Examples:\n    \n    - `symnth(\"dispatcher.funcs\", 12)` -&gt; 13th registered function\n    - `symnth(\"dispatcher.funcs\", 0); symsrc(\"_last\")` -&gt; source of first handler\"\"\"\n\ndef symlen(\n    sym: str  # Dotted symbol path or \"_last\" for previous result\n):\n    \"Returns the length of the given symbol\"\n\ndef symslice(\n    sym: str,   # Dotted symbol path or \"_last\" for previous result\n    start: int, # Starting index for slice\n    end: int    # Ending index for slice\n):\n    \"Returns the contents of the symbol from the given start to the end.\"\n\ndef symsearch(\n    sym:str,      # Dotted symbol path or \"_last\" for previous result\n    term:str,     # Search term (exact string or regex pattern)\n    regex:bool=True,  # If True, regex search; if False, exact match\n    flags:int=0   # Regex flags (e.g., re.IGNORECASE)\n):\n    \"\"\"Search contents of symbol, which is assumed to be str for regex, or iterable for non-regex.\n    Regex mode returns (match, start, end) tuples; otherwise returns (item, index) tuples\"\"\"\n\ndef symset(\n    val: str  # Value to assign to _ai_sym\n):\n    \"Set _ai_sym to the given value\"\n\ndef symfiles_folder(\n    sym:str,      # Dotted symbol path or \"_last\" for previous result\n    **kwargs\n):\n    \"Return XML context of files in the folder containing `sym`'s definition\"\n\ndef symfiles_package(\n    sym:str,      # Dotted symbol path or \"_last\" for previous result\n    **kwargs\n):\n    \"Return XML context of all files in `sym`'s top-level package\"\n\n\n# -- toolslm.funccall --\n\ndef get_schema(\n    f:Union[callable,dict], # Function to get schema for\n    pname='input_schema',   # Key name for parameters\n    evalable=False,  # stringify defaults that can't be literal_eval'd?\n    skip_hidden=False # skip parameters starting with '_'?\n)-&gt;dict: # {'name':..., 'description':..., pname:...}\n    \"Generate JSON schema for a class, function, or method\"\n\ndef mk_tool(dispfn, tool):\n    \"Create a callable function from a JSON schema tool definition\"\n\ndef call_func(fc_name, fc_inputs, ns, raise_on_err=True):\n    \"Call the function `fc_name` with the given `fc_inputs` using namespace `ns`.\"\n\nasync def call_func_async(fc_name, fc_inputs, ns, raise_on_err=True):\n    \"Awaits the function `fc_name` with the given `fc_inputs` using namespace `ns`.\"\n\n\n# -- toolslm.funccall --\n\ndef python(\n    code:str, # Code to execute\n    glb:Optional[dict]=None, # Globals namespace\n    loc:Optional[dict]=None, # Locals namespace\n    timeout:int=3600 # Maximum run time in seconds\n):\n    \"Executes python `code` with `timeout` and returning final expression (similar to IPython).\"\n\n\n# -- toolslm.download --\n# Fetch web content for LLMs    \n\ndef read_md(url, rm_comments=True, rm_details=True, **kwargs):\n    \"Read text from `url` and clean with `clean_docs`\"\n\ndef html2md(s:str, ignore_links=True):\n    \"Convert `s` from HTML to markdown\"\n\ndef read_html(url, # URL to read\n              sel=None, # Read only outerHTML of CSS selector `sel`\n              rm_comments=True, # Removes HTML comments\n              rm_details=True, # Removes `&lt;details&gt;` tags\n              multi=False, # Get all matches to `sel` or first one  \n              wrap_tag=None, #If multi, each selection wrapped with &lt;wrap_tag&gt;content&lt;/wrap_tag&gt;\n              ignore_links=True,\n             ): # Cleaned markdown\n    \"Get `url`, optionally selecting CSS selector `sel`, and convert to clean markdown\"\n\ndef get_llmstxt(url, optional=False, n_workers=None):\n    \"Get llms.txt file from and expand it with `llms_txt.create_ctx()`\"\n    \ndef find_docs(url):\n    \"If available, return LLM-friendly llms.txt context or markdown file location from `url`\"\n\ndef read_docs(url, optional=False, n_workers=None, rm_comments=True, rm_details=True):\n    \"If available, return LLM-friendly llms.txt context or markdown file response for `url`\"\n\n\n# -- toolslm.shell --\n# toolslm.shell Minimal IPython shell for code execution    get_shell, run_cell\n\ndef get_shell()-&gt;TerminalInteractiveShell:\n    \"Get a `TerminalInteractiveShell` with minimal functionality\"\n\ndef run_cell(self:TerminalInteractiveShell, cell, timeout=None):\n    \"Wrapper for original `run_cell` which adds timeout and output capture\"\n\n\n# -- ipykernel --\n\ndef scrape_url(url):\n    \"Get the html content of a web page using the cloudscraper library to bypass Cloudflare's anti-bot page.\"\n    return create_scraper().get(url)\n\ndef read_url(url: str, as_md: bool = True, extract_section: bool = True, selector: str = None, math_mode: str = None):\n    \"\"\"This functions extracts a web page information for LLM ingestion\n    1. Downloads a web page\n    2. Parses HTML\n    3. Optionally extracts a specific section (fragment or CSS selector)\n    4. Converts MathML → LaTeX\n    5. Optionally converts HTML → Markdown\n    6. Convert code sections to fenced markdown blocks\n    7. Makes image URLs absolute\n    8. Returns the processed text\n    \"\"\"\n    o = scrape_url(url)\n    res, ctype = o.text, o.headers.get('content-type').split(';')[0]\n    soup = BeautifulSoup(res, 'lxml')\n\n    if selector:\n        res = '\\n\\n'.join(str(s) for s in soup.select(selector))\n    elif extract_section:\n        parsed = urlparse(url)\n        if parsed.fragment:\n            section = soup.find(id=parsed.fragment)\n            if section:\n                elements = [section]\n                current = section.next_sibling\n                while current:\n                    if hasattr(current, 'name') and current.name == section.name: break\n                    elements.append(current)\n                    current = current.next_sibling\n                res = ''.join(str(el) for el in elements)\n            else:\n                res = ''\n    else:\n        res = str(soup)\n\n    if math_mode:\n        res_soup = BeautifulSoup(res, 'lxml')\n        _convert_math(res_soup, math_mode)\n        res = str(res_soup)\n\n    if as_md and ctype == 'text/html':\n        h = HTML2Text()\n        h.body_width = 0\n        # Handle code blocks\n        h.mark_code = True\n        res = h.handle(res)\n        def _f(m): return f'```\\n{dedent(m.group(1))}\\n```'\n        res = re.sub(r'\\[code]\\s*\\n(.*?)\\n\\[/code]', _f, res or '', flags=re.DOTALL).strip()\n        # Handle image urls\n        res = _absolutify_imgs(res, urljoin(url, s['href'] if (s := soup.find('base')) else ''))\n        # Handle math blocks\n        if math_mode == 'safe':\n            res = res.replace('\\\\\\\\(', '\\\\(').replace('\\\\\\\\)', '\\\\)')\n\n    return res",
    "crumbs": [
      "Answerai tools study"
    ]
  },
  {
    "objectID": "tools_study_answerai.html#summary-answerai-tools-signatures",
    "href": "tools_study_answerai.html#summary-answerai-tools-signatures",
    "title": "Answerai tools study",
    "section": "",
    "text": "# -- fastcore.tools --\n# Implementation of Anthropic file and text editor tools\n\ndef rg(\n    argstr:str, # All args to the command, will be split with shlex\n    disallow_re:str=None, # optional regex which, if matched on argstr, will disallow the command\n    allow_re:str=None # optional regex which, if not matched on argstr, will disallow the command\n):\n    \"Run the `rg` command with the args in `argstr`\"\n\ndef sed(\n    argstr:str, # All args to the command, will be split with shlex\n    disallow_re:str=None, # optional regex which, if matched on argstr, will disallow the command\n    allow_re:str=None # optional regex which, if not matched on argstr, will disallow the command\n):\n    \"Run the `sed` command with the args in `argstr` (e.g for reading a section of a file)\"\n\ndef view(\n    path:str, # Path to directory or file to view\n    view_range:tuple[int,int]=None, # Optional 1-indexed (start, end) line range for files, end=-1 for EOF. Do NOT use unless it's known that the file is too big to keep in context—simply view the WHOLE file when possible\n    nums:bool=False, # Whether to show line numbers\n    skip_folders:tuple[str,...]=('_proc','__pycache__') # Folder names to skip when listing directories\n):\n    'View directory or file contents with optional line range and numbers'\n\ndef create(\n    path: str, # Path where the new file should be created\n    file_text: str, # Content to write to the file\n    overwrite:bool=False # Whether to overwrite existing files\n) -&gt; str:\n    'Creates a new file with the given content at the specified path'\n\ndef insert(\n    path: str, # Path to the file to modify\n    insert_line: int, # Line number where to insert (0-based indexing)\n    new_str: str # Text to insert at the specified line\n) -&gt; str:\n    'Insert new_str at specified line number'\n\ndef str_replace(\n    path: str, # Path to the file to modify\n    old_str: str, # Text to find and replace\n    new_str: str # Text to replace with\n) -&gt; str:\n    'Replace first occurrence of old_str with new_str in file'\n\ndef strs_replace(\n    path:str, # Path to the file to modify\n    old_strs:list[str], # List of strings to find and replace\n    new_strs:list[str], # List of replacement strings (must match length of old_strs)\n):\n    \"Replace for each str pair in old_strs,new_strs\"\n    res = [str_replace(path, old, new) for (old,new) in\n\ndef replace_lines(\n    path:str, # Path to the file to modify\n    start_line:int, # Starting line number to replace (1-based indexing)\n    end_line:int, # Ending line number to replace (1-based indexing, inclusive)\n    new_content:str, # New content to replace the specified lines\n):\n    \"Replace lines in file using start and end line-numbers (index starting at 1)\"\n\ndef move_lines(\n    path: str,  # Path to the file to modify\n    start_line: int,  # Starting line number to move (1-based)\n    end_line: int,  # Ending line number to move (1-based, inclusive)\n    dest_line: int,  # Destination line number (1-based, where lines will be inserted before)\n) -&gt; str:\n    \"Move lines from start_line:end_line to before dest_line\"\n\n\n# -- claudette.text_editor --\n# Other implementation of Anthropic text editor tools\n\ndef view(path:str,  # The path to the file or directory to view\n         view_range:tuple[int,int]=None, # Optional array of two integers specifying the start and end line numbers to view. Line numbers are 1-indexed, and -1 for the end line means read to the end of the file. This parameter only applies when viewing files, not directories.\n         nums:bool=False # Optionally prefix all lines of the file with a line number\n        ) -&gt; str:\n    'Examine the contents of a file or list the contents of a directory. It can read the entire file or a specific range of lines. With or without line numbers.'\n\ndef create(path: str, # The path where the new file should be created\n           file_text: str, # The text content to write to the new file\n           overwrite:bool=False # Allows overwriting an existing file\n          ) -&gt; str:\n    'Creates a new file with the given text content at the specified path'\n\n\ndef insert(path: str,  # The path to the file to modify\n           insert_line: int, # The line number after which to insert the text (0 for beginning of file)\n           new_str: str # The text to insert\n          ) -&gt; str: \n    'Insert text at a specific line number in a file.'\n\ndef str_replace(path: str, # The path to the file to modify\n                old_str: str, # The text to replace (must match exactly, including whitespace and indentation)\n                new_str: str # The new text to insert in place of the old text\n               ) -&gt; str:\n    'Replace a specific string in a file with a new string. This is used for making precise edits.'\n\n\n# -- ipykernelhelper --\n# Get the html content of a web page using the cloudscraper library to bypass Cloudflare's anti-bot page\n# and convert it to markdown prepared for insertion in the LLM context\n\ndef read_url(url: str, as_md: bool = True, \n             extract_section: bool = True, selector: str = None,\n             math_mode: str = None):\n    \"\"\"This functions extracts a web page information for LLM ingestion\n    1. Downloads a web page\n    2. Parses HTML\n    3. Optionally extracts a specific section (fragment or CSS selector)\n    4. Converts MathML → LaTeX\n    5. Optionally converts HTML → Markdown\n    6. Convert code sections to fenced markdown blocks\n    7. Makes image URLs absolute\n    8. Returns the processed text\n    \"\"\"\n\n\n# -- dialoghelper.core --\n# Tools to edit the Jupyter notebook\n\ndef dialoghelper_explain_dialog_editing(\n)-&gt;str: # Detailed documention on dialoghelper dialog editing\n    \"Call this to get a detailed explanation of how dialog editing is done in dialoghelper. Always use if doing anything non-trivial, or if dialog editing has not previously occured in this session\"\n    return \"\"\"# dialoghelper dialog editing functionality\n\nThis guide consolidates understanding of how dialoghelper tools work together. Individual tool schemas are already in context—this adds architectural insight and usage patterns.\n\n## Core Concepts\n\n- **Dialog addressing**: All functions accepting `dname` resolve paths relative to current dialog (no leading `/`) or absolute from Solveit's runtime data path (with leading `/`). The `.ipynb` extension is never included.\n- **Message addressing**: Messages have stable `id` strings (e.g., `_a9cb5512`). The current executing message's id is in `__msg_id`. Tools use `id` for targeting; `find_msg_id()` retrieves current.\n- **Implicit state**: After `add_msg`/`update_msg`, `__msg_id` is updated to the new/modified message. This enables chaining: successive `add_msg` calls create messages in sequence.\n\n## Tool Workflow Patterns\n\n### Reading dialog state\n- `view_dlg` — fastest way to see entire dialog structure with line numbers for editing\n- `find_msgs` — search with regex, filter by type/errors/changes\n- `read_msg` — navigate relative to current message\n- `read_msgid` — direct access when you have the id\n\n**Key insight**: Messages above the current prompt are already in LLM context. Use read tools only for: (1) getting line numbers for edits, (2) accessing messages below current prompt, (3) accessing other dialogs.\n\n### Modifying dialogs\n- `add_msg` — placement can be `add_after`/`add_before` (relative to current) or `at_start`/`at_end` (absolute)\n  - **NB** When not passing a message id, it defaults to the *current* message. So if you call it multiple times with no message id, the messages will be added in REVERSE! Instead, get the return value of `add_msg` after each call, and use that for the next call\n- `update_msg` — partial updates; only pass fields to change\n- `del_msg` — use sparingly, only when explicitly requested\n`copy_msg` → `paste_msg` — for moving/duplicating messages within running dialogs.\n\n## Non-decorated Functions Worth Knowing\n\nThere are additional functions available that can be added to fenced blocks, or the user may add as tools; they are not included in schemas by default.\n\n**Browser integration:**\n- `add_html(content)` — inject HTML with `hx-swap-oob` into live browser DOM\n- `iife(code)` — execute JavaScript immediately in browser\n- `fire_event(evt, data)` / `event_get(evt)` — trigger/await browser events\n\n**Content helpers:**\n- `url2note(url, ...)` — fetch URL as markdown, add as note message\n- `mermaid(code)` / `enable_mermaid()` — render mermaid diagrams\n- `add_styles(s)` — apply solveit's MonsterUI styling to HTML\n\n**Dangerous (not exposed by default):**\n- `_add_msg_unsafe(content, run=True, ...)` — add AND execute message (code or prompt)\n- `run_msg(ids)` — queue messages for execution\n- `rm_dialog(name)` — delete entire dialog\n\n## Important Patterns\n\n### Key Principles\n\n1. **Always re-read before editing.** Past tool call results in chat history are TRUNCATED. Never rely on line numbers from earlier in the conversation—call `read_msgid(id, nums=True)` immediately before any edit operation.\n2. **Work backwards.** When making multiple edits to a message, start from the end and work towards the beginning. This prevents line number shifts from invalidating your planned edits.\n3. **Don't guess when tools fail.** If a tool call returns an error, STOP and ask for clarification. Do not retry with guessed parameters.\n4. **Verify after complex edits.** After significant changes, re-read the affected region to confirm the edit worked as expected before proceeding.\n\n### Typical Workflow\n\n```\n1. read_msgid(id, nums=True)           # Get current state with line numbers\n2. Identify lines to change\n3. msg_replace_lines(...) or msg_str_replace(...)  # Make edit\n4. If more edits needed: re-read, then repeat from step 2\n```\n\n### Tool Selection\n\n- **`msg_replace_lines`**: Best for replacing/inserting contiguous blocks. Use `view_range` on read to focus on the area.\n- **`msg_str_replace`**: Best for targeted single small string replacements when you know the exact text.\n- **`msg_strs_replace`**: Best for multiple small independent replacements in one call.\n- **`msg_insert_line`**: Best for adding new content without replacing existing lines.\n- **`msg_del_lines`**: Best for removing content.\n\n**Rough rule of thumb:** Prefer `msg_replace_lines` over `msg_str(s)_replace` unless there's &gt;1 match to change or it's just a word or two. Use the insert/delete functions for inserting/deleting; don't use `msg_str(s)_replace` for that.\n\n### Common Mistakes to Avoid\n\n- Using line numbers from a truncated earlier result\n- Making multiple edits without re-reading between them\n- Guessing line numbers when a view_range was truncated\n- Always call `read_msgid(id, nums=True)` first to get accurate line numbers\n- String-based tools (`msg_str_replace`, `msg_strs_replace`) fail if the search string appears zero or multiple times—use exact unique substrings.\"\"\"\n\ndef curr_dialog(\n    with_messages:bool=False,  # Include messages as well?\n    dname:str='' # Dialog to get info for; defaults to current dialog\n):\n    \"Get the current dialog info.\"\n\ndef view_dlg(\n    dname:str='', # Dialog to get info for; defaults to current dialog\n    msg_type:str=None, # optional limit by message type ('code', 'note', or 'prompt')\n    nums:bool=False, # Whether to show line numbers\n    include_output:bool=False, # Include output in returned dict?\n    trunc_out:bool=True, # Middle-out truncate code output to 100 characters (only applies if `include_output`)?\n    trunc_in:bool=False, # Middle-out truncate cell content to 80 characters?\n):\n    \"Concise XML view of all messages (optionally filtered by type), not including metadata. Often it is more efficient to call this to see the whole dialog at once (including line numbers if needed), instead of running `find_msgs` or `read_msg` multiple times.\"\n    return find_msgs(msg_type=msg_type, dname=dname, as_xml=True, nums=nums,\n        include_meta=False, include_output=include_output, trunc_out=trunc_out, trunc_in=trunc_in)\n\ndef msg_idx(\n    id:str=None,  # Message id to find (defaults to current message)\n    dname:str='' # Dialog to get message index from; defaults to current dialog\n):\n    \"Get absolute index of message in dialog.\"\n\ndef find_msgs(\n    re_pattern:str='', # Optional regex to search for (re.DOTALL+re.MULTILINE is used)\n    msg_type:str=None, # optional limit by message type ('code', 'note', or 'prompt')\n    use_case:bool=False, # Use case-sensitive matching?\n    use_regex:bool=True, # Use regex matching?\n    only_err:bool=False, # Only return messages that have errors?\n    only_exp:bool=False, # Only return messages that are exported?\n    only_chg:bool=False, # Only return messages that have changed vs git HEAD?\n    ids:str='', # Optionally filter by comma-separated list of message ids\n    limit:int=None, # Optionally limit number of returned items\n    include_output:bool=True, # Include output in returned dict?\n    include_meta:bool=True, # Include all additional message metadata\n    as_xml:bool=False, # Use concise unescaped XML output format\n    nums:bool=False, # Show line numbers?\n    trunc_out:bool=False, # Middle-out truncate code output to 100 characters?\n    trunc_in:bool=False, # Middle-out truncate cell content to 80 characters?\n    headers_only:bool=False, # Only return note messages that are headers (first line only); cannot be used together with `header_section`\n    header_section:str=None, # Find section starting with this header; returns it plus all children (i.e until next header of equal or more significant level)\n    dname:str='' # Dialog to get info for; defaults to current dialog\n)-&gt;list[dict]: # Messages in requested dialog that contain the given information\n    \"\"\"Often it is more efficient to call `view_dlg` to see the whole dialog at once, so you can use it all from then on, instead of using `find_msgs`.\n    {dname}\n    Message ids are identical to those in LLM chat history, so do NOT call this to view a specific message if it's in the chat history--instead use `read_msgid`.\n    Do NOT use find_msgs to view message content in the current dialog above the current prompt -- these are *already* provided in LLM context, so just read the content there directly. (NB: LLM context only includes messages *above* the current prompt, whereas `find_msgs` can access *all* messages.)\n    To refer to a found message from code or tools, use its `id` field.\"\"\"\n\ndef read_msg(\n    n:int=-1,      # Message index (if relative, +ve is downwards)\n    relative:bool=True,  # Is `n` relative to current message (True) or absolute (False)?\n    id:str=None,  # Message id to find (defaults to current message)\n    view_range:list[int,int]=None, # Optional 1-indexed (start, end) line range for files, end=-1 for EOF\n    nums:bool=False, # Whether to show line numbers\n    dname:str='' # Dialog to get info for; defaults to current dialog\n    ):\n    \"\"\"Get the message indexed in the current dialog.\n    NB: Messages in the current dialog above the current message are *already* visible; use this only when you need line numbers for editing operations, or for messages not in the current dialog or below the current message.\n    - To get the exact message use `n=0` and `relative=True` together with `id`.\n    - To get a relative message use `n` (relative position index).\n    - To get the nth message use `n` with `relative=False`, e.g `n=0` first message, `n=-1` last message.\n    {dname}\"\"\"\n\ndef read_msgid(\n    id:str,  # Message id to find\n    view_range:list[int,int]=None, # Optional 1-indexed (start, end) line range for files, end=-1 for EOF\n    nums:bool=False, # Whether to show line numbers\n    dname:str='' # Dialog to get message from; defaults to current dialog\n    ):\n    \"\"\"Get message `id`. Message IDs can be view directly in LLM chat history/context, or found in `find_msgs` results.\"\"\"\n\ndef add_msg(\n    content:str, # Content of the message (i.e the message prompt, code, or note text)\n    placement:str='add_after', # Can be 'at_start' or 'at_end', and for default dname can also be 'add_after' or 'add_before'\n    id:str=None, # id of message that placement is relative to (if None, uses current message; note: each add_msg updates \"current\" to the newly created message)\n    msg_type: str='note', # Message type, can be 'code', 'note', or 'prompt'\n    output:str='', # Prompt/code output; Code outputs must be .ipynb-compatible JSON array\n    time_run: str | None = '', # When was message executed\n    is_exported: int | None = 0, # Export message to a module?\n    skipped: int | None = 0, # Hide message from prompt?\n    i_collapsed: int | None = 0, # Collapse input?\n    o_collapsed: int | None = 0, # Collapse output?\n    heading_collapsed: int | None = 0, # Collapse heading section?\n    pinned: int | None = 0, # Pin to context?\n    dname:str='' # Dialog to get info for; defaults to current dialog. If passed, provide `id` or use `placement='at_start'`/`'at_end'`\n)-&gt;str: # Message ID of newly created message\n    \"\"\"Add/update a message to the queue to show after code execution completes.\n    **NB**: when creating multiple messages in a row, after the 1st message set `id` to the result of the last `add_msg` call,\n    otherwise messages will appear in the dialog in REVERSE order.\n    {dname}\"\"\"\n\ndef del_msg(\n    id:str=None, # id of message to delete\n    dname:str='', # Dialog to get info for; defaults to current dialog\n    log_changed:bool=False # Add a note showing the deleted content?\n):\n    \"Delete a message from the dialog. DO NOT USE THIS unless you have been explicitly instructed to delete messages.\"\n\ndef update_msg(\n    id:str=None, # id of message to update (if None, uses current message)\n    msg:Optional[Dict]=None, # Dictionary of field keys/values to update\n    dname:str='', # Dialog to get info for; defaults to current dialog\n    log_changed:bool=False, # Add a note showing the diff?\n    **kwargs):\n    \"\"\"Update an existing message. Provide either `msg` OR field key/values to update.\n    - Use `content` param to update contents.\n    - Only include parameters to update--missing ones will be left unchanged.\n    {dname}\"\"\"\n\ndef run_msg(\n    ids:str=None, # Comma-separated ids of message(s) to execute\n    dname:str='' # Running dialog to get info for; defaults to current dialog. (Note dialog *must* be running for this function)\n):\n    \"Adds a message to the run queue. Use read_msg to see the output once it runs.\"\n\ndef copy_msg(\n    ids:str=None, # Comma-separated ids of message(s) to copy\n    cut:bool=False, # Cut message(s)? (If not, copies)\n    dname:str='' # Running dialog to copy messages from; defaults to current dialog. (Note dialog *must* be running for this function)\n):\n    \"Add `ids` to clipboard.\"\n\ndef paste_msg(\n    id:str=None, # Message id to paste next to\n    after:bool=True, # Paste after id? (If not, pastes before)\n    dname:str='' # Running dialog to copy messages from; defaults to current dialog. (Note dialog *must* be running for this function)\n):\n    \"Paste clipboard msg(s) after/before the current selected msg (id).\"\n\ndef run_code_interactive(\n    code:str # Code to have user run\n):\n    \"\"\"Insert code into user's dialog and request for the user to run it. Use other tools where possible, \n    but if they can not find needed information, *ALWAYS* use this instead of guessing or giving up.\n    IMPORTANT: This tool is TERMINAL - after calling it, you MUST stop all tool usage \n    and wait for user response. Never call additional tools after this one.\"\"\"\n\ndef msg_insert_line(text, insert_line:int, new_str:str):\n    \"Insert text at specific line num in message. {besure}\\n{dname}\"\n    \ndef msg_str_replace(text, old_str:str, new_str:str):\n    \"Replace first occurrence of old_str with new_str in a message.\\n{dname}\"\n\ndef msg_strs_replace(text, old_strs:list[str], new_strs:list[str]):\n    \"Replace multiple strings simultaneously in a message.\\n{dname}\"\n\ndef msg_replace_lines(text, start_line:int, end_line:int=None, new_content:str=''):\n    \"Replace line range in msg with new content. {besure}\\n{dname}\"\n\ndef msg_del_lines(text, start_line:int, end_line:int=None):\n    \"Delete line range from a message. {besure}\\n{dname}\"\n\n\n# -- dialoghelper.core --\n# Import context and code\n\ndef url2note(\n    url:str, # URL to read\n    extract_section:bool=True, # If url has an anchor, return only that section\n    selector:str=None, # Select section(s) using BeautifulSoup.select (overrides extract_section)\n    ai_img:bool=True, # Make images visible to the AI\n    split_re:str='' # Regex to split content into multiple notes, set to '' for single note\n):\n    \"Read URL as markdown, and add note(s) below current message with the result\"\n\ndef ctx_folder(\n    path:Path='.',  # Path to collect\n    types:str|list='py,doc',  # list or comma-separated str of ext types from: py, js, java, c, cpp, rb, r, ex, sh, web, doc, cfg\n    out=False, # Include notebook cell outputs?\n    raw=True, # Add raw message, or note?\n    exts:str|list=None, # list or comma-separated str of exts to include (overrides `types`)\n    **kwargs\n):\n    \"Convert folder to XML context and place in a new message\"\n\ndef ctx_repo(\n    owner:str,  # GitHub repo owner\n    repo:str,   # GitHub repo name\n    types:str|list='py,doc',  # list or comma-separated str of ext types from: py, js, java, c, cpp, rb, r, ex, sh, web, doc, cfg\n    exts:str|list=None, # list or comma-separated str of exts to include (overrides `types`)\n    out=False, # Include notebook cell outputs?\n    raw=True, # Add raw message, or note?\n    **kwargs\n):\n    \"Convert GitHub repo to XML context and place in a new message\"\n\ndef ctx_symfile(sym):\n    \"Add note with filepath and contents for a symbol's source file\"\n\ndef ctx_symfile(sym):\n    \"Add note with filepath and contents for a symbol's source file\"\n\ndef ctx_symfolder(\n    sym, # Symbol to get folder context from\n    **kwargs):\n    \"Add raw message with folder context for a symbol's source file location\"\n\ndef ctx_sympkg(\n    sym, # Symbol to get folder context from\n    **kwargs):\n    \"Add raw message with repo context for a symbol's root package\"\n\ndef gist_file(gist_id:str):\n    \"Get the first file from a gist\"\n\ndef ast_grep(\n    pattern:str, # ast-grep pattern to search, e.g \"post($A, data=$B, $$$)\"\n    path:str=\".\", # path to recursively search for files\n    lang:str=\"python\" # language to search/scan\n): # json format from calling `ast-grep --json=compact\n    \"\"\"Use `ast-grep` to find code patterns by AST structure (not text).\n    \n    Pattern syntax:\n    - $VAR captures single nodes, $$$ captures multiple\n    - Match structure directly: `def $FUNC($$$)` finds any function; `class $CLASS` finds classes regardless of inheritance\n    - DON'T include `:` - it's concrete syntax, not AST structure\n    - Whitespace/formatting ignored - matches structural equivalence\n    \n    Examples: `import $MODULE` (find imports); `$OBJ.$METHOD($$$)` (find method calls); `await $EXPR` (find await expressions)\n    \n    Useful for: Refactoring—find all uses of deprecated APIs or changed signatures; Security review—locate SQL queries, file operations, eval calls; Code exploration—understand how libraries are used across codebase; Pattern analysis—find async functions, error handlers, decorators; Better than regex—handles multi-line code, nested structures, respects syntax\"\"\"\n\ndef import_string(\n    code:str, # Code to import as a module\n    name:str  # Name of module to create\n):\n\ndef import_gist(\n    gist_id:str, # user/id or just id of gist to import as a module\n    mod_name:str=None, # module name to create (taken from gist filename if not passed)\n    add_global:bool=True, # add module to caller's globals?\n    import_wildcard:bool=False, # import all exported symbols to caller's globals\n    create_msg:bool=False # Add a message that lists usable tools\n):\n    \"Import gist directly from string without saving to disk\"\n\ndef update_gist(gist_id:str, content:str):\n    \"Update the first file in a gist with new content\"\n\ntracetool is an LLM tool that traces function execution and captures variable snapshots after each line runs, using Python 3.12’s sys.monitoring for low-overhead tracing.\nFor each call to the target function (including recursive calls), the LLM receives a stack trace showing how that call was reached, plus a per-line trace dict mapping each executed source line to its hit count and variable snapshots. Variables that don’t change are shown as a single (type, repr) tuple, while those that evolve across iterations appear as a list of snapshots.\nThis is particularly useful when the LLM needs to understand unfamiliar code by seeing exactly what happens step-by-step, debug issues by watching how variables change, verify loop behavior by confirming iteration counts and accumulator values, or explore recursive functions where each call gets its own trace entry\n\n# -- dialoghelper.capture --\n# Screen capture\n\ndef capture_tool(timeout:int=15):\n    \"Capture the screen. Re-call this function to get the most recent screenshot, as needed. Use default timeout where possible\"\n\n\n# -- dialoghelper.tracetools --\n# Trace function execution\n\ndef tracetool(\n    sym: str,  # Dotted symbol path of callable to run\n    args: list=None,  # Positional args for callable (JSON values passed directly)\n    kwargs: dict=None,  # Keyword args for callable (JSON values passed directly)\n    target_func: str=None  # Dotted symbol path of function to trace (defaults to sym)\n)-&gt;list[tuple[str, dict[str, tuple[int, dict[str, tuple|list]]]]]:  # List of (stack_str,trace_dict); trace_dict maps source snippets to (hit_count, variables), unchanged vars collapsed to single tuple\n    \"\"\"\n    Trace execution using sys.monitoring (Python 3.12+), returning a list of per-call traces.\n\n    Return:\n    - list of length &lt;= 10\n    - one element per call to `target_func` (including recursion)\n    - each element is: (stack_str, trace_dict)\n        stack_str: call stack string (filtered so `fn` is the shallowest frame shown)\n        trace_dict: {\n        \"&lt;source snippet for AST-line&gt;\": ( hit_count, { \"var\": [ (type_name, truncated_repr), ... up to 10 ], ... } ),\n        ...}\n\n    Semantics:\n    - \"Line\" means an AST-level line: separate statements (even if on one physical line via `;`).\n    - Compound statements are keyed by their header only.\n    - Unchanged variables → `('type', 'repr')` tuple, changed variables → `[('type', 'repr'), ...]` list.\n    - Comprehensions are treated as a line node and are monitored, including inside the comprehension frame, with per-iteration snapshots.\n    - Snapshots are recorded after each line finishes, so assignments show updated values.\n    \"\"\"\n\n\n# -- dialoghelper.tmux --\n# Terminal : view tmux buffers\n\ndef pane(\n    n:int=None, # Number of scrollback lines to capture, in addition to visible area (None uses default_tmux_lines, which is 500 if not otherwise set)\n    pane:int=None,     # Pane number to capture from\n    session:str=None,  # Session name to target\n    window:int=None,   # Window number to target\n    **kwargs\n):\n    'Grab the tmux history in plain text'\n\ndef list_panes(\n    session:str=None,  # Session name to list panes from\n    window:int=None,   # Window number to list panes from\n    **kwargs\n):\n    'List panes for a session/window (or current if none specified)'\n\ndef panes(\n    session:str=None,  # Session name to target\n    window:int=None,   # Window number to target\n    n:int=None,        # Number of scrollback lines to capture\n    **kwargs\n):\n    'Grab history from all panes in a session/window'\n\ndef list_windows(\n    session:str=None,  # Session name to list windows from\n    **kwargs\n):\n    'List all windows in a session'\n\ndef windows(\n    session:str=None,  # Session name to target\n    n:int=None,        # Number of scrollback lines to capture\n    **kwargs\n):\n    'Grab history from all panes in all windows of a session'\n\ndef list_sessions(**kwargs):\n    'List all tmux sessions'\n\ndef sessions(\n    n:int=None,        # Number of scrollback lines to capture\n    **kwargs\n):\n    'Grab history from all panes in all windows of all sessions'\n\n\n# -- contextkit.read --\n\ndef read_text(url, # URL to read\n             ): # Text from page\n    \"Get text from `url`\"\n\ndef read_link(url: str,   # URL to read\n             heavy: bool = False,   # Use headless browser (requires extra setup steps before use)\n             sel: Optional[str] = None,  # Css selector to pull content from\n             useJina: bool = False, # Use Jina for the markdown conversion\n             ignore_links: bool = False, # Whether to keep links or not\n             ): \n    \"Reads a url and converts to markdown\"\n\ndef read_gist(url:str  # gist URL, of gist to read\n             ):\n    \"Returns raw gist content, or None\"\n\ndef read_gh_file(url:str # GitHub URL of the file to read\n                ):\n    \"Reads the contents of a file from its GitHub URL\"\n\ndef read_file(path:str):\n    \"returns file contents\"\n\ndef read_dir(path: str,                          # path to read\n             unicode_only: bool = True,             # ignore non-unicode files\n             included_patterns: List[str] = [\"*\"],       # glob pattern of files to include\n             excluded_patterns: List[str] = [\".git/**\"], # glob pattern of files to exclude\n             verbose: bool = False,                # log paths of files being read\n             as_dict: bool = False                  # returns dict of {path,content}\n            ) -&gt; Union[str, Dict[str, str]]:            # returns string with contents of files read\n    \"\"\"Reads files in path, returning a dict with the filenames and contents if as_dict=True, otherwise concatenating file contents into a single string. Takes optional glob patterns for files to include or exclude.\"\"\"\n\ndef read_pdf(file_path: str # path of PDF file to read\n            ) -&gt; str:\n    \"Reads the text of a PDF with PdfReader\"\n\ndef read_google_sheet(url: str # URL of a Google Sheet to read\n                     ):\n    \"Reads the contents of a Google Sheet into text\"\n\ndef read_gdoc(url: str  # URL of Google Doc to read\n             ):\n    \"Gets the text content of a Google Doc using html2text\"\n\ndef read_arxiv(url:str, # arxiv PDF URL, or arxiv abstract URL, or arxiv ID\n               save_pdf:bool=False, # True, will save the downloaded PDF\n               save_dir:str='.' # directory in which to save the PDF\n              ):\n    \"Get paper information from arxiv URL or ID, optionally saving PDF to disk\"\n\ndef read_gh_repo(path_or_url:str,    # Repo's GitHub URL, or GH SSH address, or file path\n                 as_dict:bool=True,  # if True, will return repo contents {path,content} dict\n                 verbose:bool=False  # if True, will log paths of files being read\n                ):\n    \"Repo contents from path, GH URL, or GH SSH address\"\n\n\n# -- llms-txt --\n\ndef parse_llms_file(txt):\n    \"Parse llms.txt file contents in `txt` to an `AttrDict`\"\n\ndef llms_txt2ctx(\n    fname:str, # File name to read\n    optional:bool_arg=False, # Include 'optional' section?\n    n_workers:int=None, # Number of threads to use for parallel downloading\n    save_nbdev_fname:str=None #save output to nbdev `{docs_path}` instead of emitting to stdout\n):\n    \"Print a `Project` with a `Section` for each H2 part in file read from `fname`, optionally skipping the 'optional' section.\"\n\n\n# -- playwrightnb --\n\ndef read_page(url, pause=50, timeout=5000, stealth=False, page=None):\n    \"Return contents of `url` and its iframes using Playwright\"\n\ndef url2md(url, sel=None, pause=50, timeout=5000, stealth=False, page=None):\n    \"Read `url` with `read_page`\"\n\ndef get2md(url, sel=None, **kwargs):\n    \"Read `url` with `httpx.get`\"\n\n\n# -- toolslm.xml --\n\ndef read_file(fname, max_size=None, sigs_only=False, **kwargs):\n    \"Read file content, converting notebooks to XML if needed\"\n\ndef files2ctx(\n    fnames:list[Union[str,Path]], # List of file names to add to context\n    srcs:Optional[list]=None, # Use the labels instead of `fnames`\n    max_size:int=None, # Skip files larger than this (bytes)\n    out:bool=True, # Include notebook cell outputs?\n    ids:bool=True,  # Include cell ids in notebooks?\n    nums:bool=False, # Include line numbers in notebook cell source?\n    sigs_only:bool=False, # Only include signatures and docstrings (where supported by `codesigs` lib)\n    **kwargs\n)-&gt;str: # XML for LM context\n    \"Convert files to XML context, handling notebooks\"\n\ndef folder2ctx(\n    path:Union[str,Path], # Folder to read\n    prefix:bool=False, # Include Anthropic's suggested prose intro?\n    out:bool=True, # Include notebook cell outputs?\n    include_base:bool=True, # Include full path in src?\n    title:str=None, # Optional title attr for Documents element\n    max_size:int=100_000, # Skip files larger than this (bytes)\n    max_total:int=10_000_000,  # Max total output size in bytes\n    readme_first:bool=False,  # Prioritize README files at start of context?\n    files_only:bool=False,  # Return dict of {filename: size} instead of context?\n    sigs_only:bool=False,  # Return signatures instead of full text? (where supported by `codesigs` lib)\n    ids:bool=True,  # Include cell ids in notebooks?\n    **kwargs\n)-&gt;Union[str,dict]:\n    \"Convert folder contents to XML context, handling notebooks\"\n\ndef sym2file(sym):\n    \"Return md string with filepath and contents for a symbol's source file\"\n\ndef sym2folderctx(\n    sym,\n    types:str|list='py',  # List or comma-separated str of ext types from: py, js, java, c, cpp, rb, r, ex, sh, web, doc, cfg\n    skip_file_re:str=r'^_mod', # Skip files matching regex\n    **kwargs\n):\n    \"Return folder context for a symbol's source file location\"\n\ndef sym2pkgpath(sym):\n    \"Get root package path for a symbol\"\n\ndef sym2pkgctx(\n    sym,\n    types:str|list='py', # List or comma-separated str of ext types from: py, js, java, c, cpp, rb, r, ex, sh, web, doc, cfg\n    skip_file_re:str=r'^_mod', # Skip files matching regex\n    skip_folder_re:str=r'^(\\.|__)', # Skip folders matching regex\n    **kwargs\n):\n    \"Return contents of files in a symbol's root package\"\n\ndef repo2ctx(\n    owner:str,  # GitHub repo owner or \"owner/repo\" or a full github URL\n    repo:str=None,   # GitHub repo name (leave empty if using \"owner/repo\" or URL format for owner param)\n    ref:str=None,  # Git ref (branch/tag/sha) (get from URL not provided); defaults to repo's default branch\n    folder:str=None,  # Only include files under this path (get from URL not provided)\n    show_filters:bool=True,  # Include filter info in title?\n    token:str=None,  # GitHub token (uses GITHUB_TOKEN env var if None)\n    **kwargs  # Passed to `folder2ctx`\n)-&gt;Union[str,dict]:  # XML for LM context, or dict of file sizes\n    \"Convert GitHub repo to XML context without cloning\"\n\n\n# -- toolslm.inspecttools --\n\ndef importmodule(\n    mod: str, # The module to import (e.g. 'torch.nn.functional')\n    caller_symbol:str = '__msg_id'  # The name of the special variable to find the correct caller namespace\n):\n    \"\"\"Import a module into the caller's global namespace so it's available for `symsrc`, `symval`, `symdir`, etc.\n    Use this before inspecting or using symbols from modules not yet imported.\"\"\"\n\ndef symsrc(\n    sym: str  # Dotted symbol path (e.g `Interval` or `sympy.sets.sets.Interval`) or \"_last\" for previous result\n):\n    \"\"\"Get the source code for a symbol.\n\n    Examples:\n\n    - `symsrc(\"Interval\")` -&gt; source code of Interval class if it's already imported\n    - `symsrc(\"sympy.sets.sets.Interval\")` -&gt; source code of Interval class\n    - `symsrc(\"_last\")` -&gt; source of object from previous tool call\n    - For dispatchers or registries of callables: `symnth(\"module.dispatcher.funcs\", n) then symsrc(\"_last\")`\"\"\"\n\ndef symtype(\n    syms: str  # Comma separated str list of dotted symbol paths (e.g `'Interval,a'` or `'sympy.sets.sets.Interval'`); \"_last\" for prev result\n):\n    \"\"\"Get the type of a symbol and set `_last`.\n\n    Examples:\n\n    - `symtype(\"sympy.sets.sets.Interval\")` -&gt; `&lt;class 'type'&gt;`\n    - `symtype(\"doesnotexist\")` -&gt; `'SymbolNotFound`\n    - `symtype(\"_last\")` -&gt; type of previous result\"\"\"\n    \ndef symval(\n    syms: str  # Comma separated str list of dotted symbol paths (e.g `Interval` or `sympy.sets.sets.Interval`); \"_last\" for prev result\n):\n    \"\"\"List of repr of symbols' values.\n\n    Examples:\n    \n    - `symval(\"sympy.sets.sets.Interval\")` -&gt; `[&lt;class 'sympy.sets.sets.Interval'&gt;]`\n    - `symval(\"some_dict.keys\")` -&gt; `[dict_keys([...])]`\n    - `symval(\"a,notexist\")` -&gt; `['foo','SymbolNotFound']`\"\"\"\n\ndef symdir(\n    sym: str,  # Dotted symbol path (e.g `Interval` or `sympy.sets.sets.Interval`) or \"_last\" for previous result\n    exclude_private: bool=False # Filter out attrs starting with \"_\"\n):\n    \"\"\"Get dir() listing of a symbol's attributes and set `_last`. E.g: `symdir(\"sympy.Interval\")` -&gt; `['__add__', '__and__', ...]`\"\"\"\n\n    def symnth(\n    sym: str,  # Dotted symbol path to a dict or object with .values()\n    n: int     # Index into the values (0-based)\n):\n    \"\"\"Get the nth value from a dict (or any object with .values()). Sets `_last` so you can chain with `symsrc(\"_last\")` etc.\n\n    Examples:\n    \n    - `symnth(\"dispatcher.funcs\", 12)` -&gt; 13th registered function\n    - `symnth(\"dispatcher.funcs\", 0); symsrc(\"_last\")` -&gt; source of first handler\"\"\"\n\ndef symlen(\n    sym: str  # Dotted symbol path or \"_last\" for previous result\n):\n    \"Returns the length of the given symbol\"\n\ndef symslice(\n    sym: str,   # Dotted symbol path or \"_last\" for previous result\n    start: int, # Starting index for slice\n    end: int    # Ending index for slice\n):\n    \"Returns the contents of the symbol from the given start to the end.\"\n\ndef symsearch(\n    sym:str,      # Dotted symbol path or \"_last\" for previous result\n    term:str,     # Search term (exact string or regex pattern)\n    regex:bool=True,  # If True, regex search; if False, exact match\n    flags:int=0   # Regex flags (e.g., re.IGNORECASE)\n):\n    \"\"\"Search contents of symbol, which is assumed to be str for regex, or iterable for non-regex.\n    Regex mode returns (match, start, end) tuples; otherwise returns (item, index) tuples\"\"\"\n\ndef symset(\n    val: str  # Value to assign to _ai_sym\n):\n    \"Set _ai_sym to the given value\"\n\ndef symfiles_folder(\n    sym:str,      # Dotted symbol path or \"_last\" for previous result\n    **kwargs\n):\n    \"Return XML context of files in the folder containing `sym`'s definition\"\n\ndef symfiles_package(\n    sym:str,      # Dotted symbol path or \"_last\" for previous result\n    **kwargs\n):\n    \"Return XML context of all files in `sym`'s top-level package\"\n\n\n# -- toolslm.funccall --\n\ndef get_schema(\n    f:Union[callable,dict], # Function to get schema for\n    pname='input_schema',   # Key name for parameters\n    evalable=False,  # stringify defaults that can't be literal_eval'd?\n    skip_hidden=False # skip parameters starting with '_'?\n)-&gt;dict: # {'name':..., 'description':..., pname:...}\n    \"Generate JSON schema for a class, function, or method\"\n\ndef mk_tool(dispfn, tool):\n    \"Create a callable function from a JSON schema tool definition\"\n\ndef call_func(fc_name, fc_inputs, ns, raise_on_err=True):\n    \"Call the function `fc_name` with the given `fc_inputs` using namespace `ns`.\"\n\nasync def call_func_async(fc_name, fc_inputs, ns, raise_on_err=True):\n    \"Awaits the function `fc_name` with the given `fc_inputs` using namespace `ns`.\"\n\n\n# -- toolslm.funccall --\n\ndef python(\n    code:str, # Code to execute\n    glb:Optional[dict]=None, # Globals namespace\n    loc:Optional[dict]=None, # Locals namespace\n    timeout:int=3600 # Maximum run time in seconds\n):\n    \"Executes python `code` with `timeout` and returning final expression (similar to IPython).\"\n\n\n# -- toolslm.download --\n# Fetch web content for LLMs    \n\ndef read_md(url, rm_comments=True, rm_details=True, **kwargs):\n    \"Read text from `url` and clean with `clean_docs`\"\n\ndef html2md(s:str, ignore_links=True):\n    \"Convert `s` from HTML to markdown\"\n\ndef read_html(url, # URL to read\n              sel=None, # Read only outerHTML of CSS selector `sel`\n              rm_comments=True, # Removes HTML comments\n              rm_details=True, # Removes `&lt;details&gt;` tags\n              multi=False, # Get all matches to `sel` or first one  \n              wrap_tag=None, #If multi, each selection wrapped with &lt;wrap_tag&gt;content&lt;/wrap_tag&gt;\n              ignore_links=True,\n             ): # Cleaned markdown\n    \"Get `url`, optionally selecting CSS selector `sel`, and convert to clean markdown\"\n\ndef get_llmstxt(url, optional=False, n_workers=None):\n    \"Get llms.txt file from and expand it with `llms_txt.create_ctx()`\"\n    \ndef find_docs(url):\n    \"If available, return LLM-friendly llms.txt context or markdown file location from `url`\"\n\ndef read_docs(url, optional=False, n_workers=None, rm_comments=True, rm_details=True):\n    \"If available, return LLM-friendly llms.txt context or markdown file response for `url`\"\n\n\n# -- toolslm.shell --\n# toolslm.shell Minimal IPython shell for code execution    get_shell, run_cell\n\ndef get_shell()-&gt;TerminalInteractiveShell:\n    \"Get a `TerminalInteractiveShell` with minimal functionality\"\n\ndef run_cell(self:TerminalInteractiveShell, cell, timeout=None):\n    \"Wrapper for original `run_cell` which adds timeout and output capture\"\n\n\n# -- ipykernel --\n\ndef scrape_url(url):\n    \"Get the html content of a web page using the cloudscraper library to bypass Cloudflare's anti-bot page.\"\n    return create_scraper().get(url)\n\ndef read_url(url: str, as_md: bool = True, extract_section: bool = True, selector: str = None, math_mode: str = None):\n    \"\"\"This functions extracts a web page information for LLM ingestion\n    1. Downloads a web page\n    2. Parses HTML\n    3. Optionally extracts a specific section (fragment or CSS selector)\n    4. Converts MathML → LaTeX\n    5. Optionally converts HTML → Markdown\n    6. Convert code sections to fenced markdown blocks\n    7. Makes image URLs absolute\n    8. Returns the processed text\n    \"\"\"\n    o = scrape_url(url)\n    res, ctype = o.text, o.headers.get('content-type').split(';')[0]\n    soup = BeautifulSoup(res, 'lxml')\n\n    if selector:\n        res = '\\n\\n'.join(str(s) for s in soup.select(selector))\n    elif extract_section:\n        parsed = urlparse(url)\n        if parsed.fragment:\n            section = soup.find(id=parsed.fragment)\n            if section:\n                elements = [section]\n                current = section.next_sibling\n                while current:\n                    if hasattr(current, 'name') and current.name == section.name: break\n                    elements.append(current)\n                    current = current.next_sibling\n                res = ''.join(str(el) for el in elements)\n            else:\n                res = ''\n    else:\n        res = str(soup)\n\n    if math_mode:\n        res_soup = BeautifulSoup(res, 'lxml')\n        _convert_math(res_soup, math_mode)\n        res = str(res_soup)\n\n    if as_md and ctype == 'text/html':\n        h = HTML2Text()\n        h.body_width = 0\n        # Handle code blocks\n        h.mark_code = True\n        res = h.handle(res)\n        def _f(m): return f'```\\n{dedent(m.group(1))}\\n```'\n        res = re.sub(r'\\[code]\\s*\\n(.*?)\\n\\[/code]', _f, res or '', flags=re.DOTALL).strip()\n        # Handle image urls\n        res = _absolutify_imgs(res, urljoin(url, s['href'] if (s := soup.find('base')) else ''))\n        # Handle math blocks\n        if math_mode == 'safe':\n            res = res.replace('\\\\\\\\(', '\\\\(').replace('\\\\\\\\)', '\\\\)')\n\n    return res",
    "crumbs": [
      "Answerai tools study"
    ]
  },
  {
    "objectID": "tools_study_answerai.html#solveit-tool-exploration",
    "href": "tools_study_answerai.html#solveit-tool-exploration",
    "title": "Answerai tools study",
    "section": "Solveit tool exploration",
    "text": "Solveit tool exploration\nHere is the list of unique Answerai library names sorted in alphabetical order:\n\nLibrairies providing tools\n\n\n\nLibrary\nGitHub Repo URL\nLibrary Description\n\n\n\n\ncontextkit\n* https://github.com/AnswerDotAI/ContextKit\n* Useful LLM contexts ready to be used in AIMagic. Library for gathering context from various sources (URLs, GitHub repos, Google Docs, PDFs, arXiv) for feeding to LLMs.\n\n\ndialoghelper\n* https://github.com/AnswerDotAI/dialoghelper\n* Helper functions for solveit dialogs. Provides functions for message manipulation, gist management, screen capture, and tool integration in the solveit environment.\n\n\nexecnb\n* https://github.com/AnswerDotAI/execnb\n* Execute a jupyter notebook, fast, without needing jupyter. Provides CaptureShell for running code and capturing outputs without a Jupyter server.\n\n\nfastcore\n* https://github.com/AnswerDotAI/fastcore\nPython utilities and enhancements used across fast.ai projects. Includes foundational tools like @patch, @delegates, type dispatch, and LLM file/bash tools.\n\n\nipykernel_helper\nPRIVATE REPO\nHelper utilities for IPython/Jupyter kernels, providing read_url, transient display, enhanced completion, and namespace inspection for dialog environments.\n\n\nllms_txt\n* https://github.com/AnswerDotAI/llms-txt\n* The llms.txt specification is open for community input. A GitHub repository hosts this informal overview. Tools for the /llms.txt standard helping language models use websites.\n\n\nplaywrightnb\nhttps://github.com/AnswerDotAI/playwrightnb\nPlaywright browser automation integration for Jupyter notebooks, enabling headless browser control and web scraping.\n\n\ntoolslm\n* https://github.com/AnswerDotAI/toolslm\n* Tools to make language models a bit easier to use. Provides XML context creation, function schema generation, shell execution, and web content downloading for LLMs.\n\n\n\n\n\nSummary: Answer.AI Tools for LLM Development\nLLM Tool Utilities\n\n\n\n\n\n\n\n\nLibrary\nPurpose\nKey Functions\n\n\n\n\nfastcore.tools\nFile/bash operations designed for LLM tool loops\nview, create, insert, str_replace, strs_replace, replace_lines, run_cmd, rg, sed\n\n\ntoolslm.funccall\nFunction calling / tool use\nget_schema, call_func, call_func_async, python, mk_ns\n\n\n\nContext Preparation\n\n\n\n\n\n\n\n\nLibrary\nPurpose\nKey Functions\n\n\n\n\ncontextkit\nGather context from various sources\nread_link, read_gh_repo, read_gh_file, read_gist, read_google_sheet, read_gdoc, read_dir, read_pdf, read_arxiv\n\n\ntoolslm.xml\nConvert files/folders to XML for LLMs\nfolder2ctx, files2ctx, json_to_xml, docs_xml, nb2xml\n\n\ntoolslm.download\nFetch web content for LLMs\nread_html, read_md, html2md, get_llmstxt, find_docs, read_docs\n\n\ntoolslm.md_hier\nParse markdown hierarchically\ncreate_heading_dict, HeadingDict\n\n\n\nMessage Formatting\n\n\n\n\n\n\n\n\nLibrary\nPurpose\nKey Functions\n\n\n\n\nmsglm\nCreate properly formatted messages for LLM APIs\nmk_msg, mk_msgs, mk_msg_anthropic, mk_msg_openai, mk_ant_doc (supports text, images, PDFs, caching)\n\n\n\nCode Execution\n\n\n\n\n\n\n\n\nLibrary\nPurpose\nKey Functions\n\n\n\n\nexecnb\nExecute Jupyter notebooks without Jupyter\nCaptureShell, CaptureShell.run(), CaptureShell.execute(), read_nb, write_nb, new_nb\n\n\nnbformat\nNotebook format handling & validation\nread, write, validate, format conversion\n\n\ntoolslm.shell\nMinimal IPython shell for code execution\nget_shell, run_cell\n\n\n\nDialog/Session Management\n\n\n\n\n\n\n\n\nLibrary\nPurpose\nKey Functions\n\n\n\n\ndialoghelper.core\nInteract with solveit dialogs\nread_msg, add_msg, update_msg, del_msg, run_msg, find_msgs, import_gist, url2note, ast_grep\n\n\ndialoghelper.capture\nScreen sharing with AI\nsetup_share, start_share, capture_screen, capture_tool\n\n\n\nVisual Overview\n┌─────────────────────────────────────────────────────────────────┐\n│                    LLM Application Stack                        │\n├─────────────────────────────────────────────────────────────────┤\n│  USER INTERFACE                                                 │\n│    dialoghelper (dialog management, screen capture)             │\n├─────────────────────────────────────────────────────────────────┤\n│  MESSAGE LAYER                                                  │\n│    msglm (format messages for Claude/OpenAI/etc)                │\n├─────────────────────────────────────────────────────────────────┤\n│  CONTEXT LAYER                                                  │\n│    contextkit (read from URLs, GitHub, Google, PDFs, arXiv)     │\n│    toolslm.xml (convert files/folders to XML)                   │\n│    toolslm.download (fetch & clean web content)                 │\n│    toolslm.md_hier (parse markdown structure)                   │\n├─────────────────────────────────────────────────────────────────┤\n│  TOOL EXECUTION LAYER                                           │\n│    fastcore.tools (file editing, bash commands)                 │\n│    toolslm.funccall (function schemas, safe execution)          │\n│    toolslm.shell (IPython execution)                            │\n│    execnb (notebook execution)                                  │\n├─────────────────────────────────────────────────────────────────┤\n│  FORMAT LAYER                                                   │\n│    nbformat (notebook format handling)                          │\n└─────────────────────────────────────────────────────────────────┘\nKey Themes\n\nSafety: Tools return errors as strings for LLMs to debug; file operations have safeguards\nLLM-Optimized Formats: XML for Claude, proper message structures for each API\nSingle-Function Access: Most operations are one function call (e.g., read_gh_repo, folder2ctx)\nComposability: Libraries work together - contextkit gathers content, toolslm formats it, msglm packages it for APIs\n\n\n\nDetails of the tools available in these libraries\n\ncontextkit\nAll functions follow th same design pattern: single required argument (location), optional parameters for customization, and return text or dict suitable for LLM consumption.\nWeb Content Functions\nread_link(url, heavy=False, sel=None, useJina=False, ignore_links=False) - Reads a URL and converts to markdown - The heavy argument allows you to do a heavy scrape with a contactless browser using playwrightnb - sel - CSS selector to extract specific content - useJina=True - uses Jina.ai service for markdown conversion - ignore_links - whether to strip out links\nread_url(...) - Deprecated alias for read_link()\nread_text(url) - Get raw text from a URL (no markdown conversion)\nread_html(url, sel=None, ...) - Fetch URL, optionally select CSS elements, convert to clean markdown\nGitHub Functions\nread_gist(url) - Returns raw gist content from a GitHub gist URL\nread_gh_file(url) - Reads contents of a single file from its GitHub URL\nread_gh_repo(path_or_url, as_dict=False, verbose=False) - Reads entire repo contents from GitHub URL, SSH address, or local path - Clones/caches repos automatically - Returns dict of {filepath: content} if as_dict=True, otherwise concatenated string\nGoogle Services Functions\nread_google_sheet(url) - Reads a Google Sheet into CSV text\nread_gdoc(url) - Gets text content of a Google Doc converted to markdown\nFile System Functions\nread_file(path) - Returns file contents as string\nread_dir(path, unicode_only=True, included_patterns=[\"*\"], excluded_patterns=[\".git/**\"], verbose=False, as_dict=False) - Reads files in a directory with glob pattern filtering - unicode_only=True - skip binary files - included_patterns - glob patterns to include - excluded_patterns - glob patterns to exclude - Returns dict or concatenated string with file markers\nread_pdf(file_path) - Extracts text from PDF using pypdf\nAcademic Functions\nread_arxiv(url, save_pdf=False, save_dir='.') - Get paper info from arXiv URL or ID - Returns dict with title, authors, summary, published date, links - Optionally downloads PDF and extracts LaTeX source - Returns structured metadata perfect for LLM context\n\n\ndialoghelper\ndialoghelper is a library from Answer.AI providing helper functions for the solveit dialog environment. It enables programmatic interaction with dialog cells, message editing, gist management, and screen capture.\nModule 1: core (dialoghelper.core)\nThe main module with functions for interacting with solveit dialogs:\nBasics - Variable/Context Management\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nfind_var(var)\nSearch for variable in all frames of the call stack\n\n\nset_var(var, val)\nSet variable value after finding it in call stack frames\n\n\nfind_msg_id()\nGet current message ID from call stack (__msg_id)\n\n\nfind_dname()\nGet current dialog name from call stack (__dialog_id)\n\n\ncall_endp(path, ...)\nCall a solveit API endpoint\n\n\ncurr_dialog(with_messages, dname)\nGet current dialog info\n\n\nmsg_idx(msgid, dname)\nGet absolute index of message in dialog\n\n\n\nJavaScript/HTML Injection\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nadd_scr(scr, oob)\nSwap a script element into the DOM\n\n\niife(code)\nWrap JavaScript code in IIFE and execute via add_html\n\n\nadd_html(content, dname)\nSend HTML to browser to be swapped into DOM (uses hx-swap-oob)\n\n\n\nEvent System\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\npop_data(idx, timeout)\nPop data from a queue\n\n\nfire_event(evt, data)\nFire a browser event\n\n\nevent_get(evt, timeout, data)\nFire event and wait for response data\n\n\n\nView/Edit Dialog Messages\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nfind_msgs(re_pattern, msg_type, limit, ...)\nFind messages matching regex/type in current dialog\n\n\nread_msg(n, relative, msgid, view_range, nums, ...)\nGet message by index (absolute or relative) with optional line range\n\n\nadd_msg(content, placement, msgid, msg_type, ...)\nAdd a new message (note/code/prompt) at specified position\n\n\ndel_msg(msgid, dname)\nDelete a message\n\n\nupdate_msg(msgid, msg, content, ...)\nUpdate an existing message’s content/properties\n\n\nrun_msg(msgid, dname)\nQueue a message for execution\n\n\nurl2note(url, ...)\nRead URL as markdown and add as note(s) below current message\n\n\n\nAST/Code Search\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nast_py(code)\nGet an ast-grep SgRoot node for Python code\n\n\nast_grep(pattern, path, lang)\nUse ast-grep to find pattern in files\n\n\n\nText Edit Functions (for editing message content)\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nmsg_insert_line(msgid, insert_line, new_str, ...)\nInsert text at specific line number\n\n\nmsg_str_replace(msgid, old_str, new_str, ...)\nReplace first occurrence of string\n\n\nmsg_strs_replace(msgid, old_strs, new_strs, ...)\nReplace multiple strings simultaneously\n\n\nmsg_replace_lines(msgid, start_line, end_line, new_content, ...)\nReplace a range of lines\n\n\n\nGist Management\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nload_gist(gist_id)\nRetrieve a GitHub gist\n\n\ngist_file(gist_id)\nGet the first file from a gist\n\n\nimport_string(code, name)\nImport code string as a module\n\n\nis_usable_tool(func)\nCheck if function can be used as LLM tool (has docstring + typed params)\n\n\nmk_toollist(syms)\nCreate markdown list of tools with & notation\n\n\nimport_gist(gist_id, mod_name, ...)\nImport gist directly as module, optionally wildcard import\n\n\n\nTool Info\n\n\n\nFunction\nDescription\n\n\n\n\ntool_info()\nGet information about available tools\n\n\nfc_tool_info()\nGet function-calling tool information\n\n\n\nModule 2: capture (dialoghelper.capture)\nScreen capture functionality for sharing screens with the AI:\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nsetup_share()\nSetup screen sharing (initializes the capture system)\n\n\nstart_share()\nStart screen sharing session\n\n\ncapture_screen(timeout=15)\nCapture the screen as a PIL image\n\n\ncapture_tool(timeout=15)\nCapture screen for LLM tool use - returns image data suitable for vision models\n\n\n\ndialoghelper is the infrastructure that powers the solveit interactive environment, enabling: - Dialog manipulation: Add, edit, delete, search messages programmatically - Code execution: Queue and run code cells - Text editing: Line-based editing operations on message content - Gist integration: Import tools/modules directly from GitHub gists - Screen capture: Share your screen with the AI for visual understanding - AST search: Search code using ast-grep patterns\n\n\nexecnb\nexecnb is a fast.ai library for * * executing Jupyter notebooks without needing a Jupyter server or even having Jupyter installed.\nCore execution:\n\n\nCaptureShell - runs Jupyter code and captures notebook outputs\n\n\nCaptureShell.run() - executes code strings and returns outputs\n\n\nCaptureShell.execute() - executes a notebook and saves it with outputs filled in\n\n\n\n\n\nfastcore\nfastcore provides helpful tools for running CLI commands and reading, modifying, and creating files in Python, primarily for AI’s in tool loops for automating tasks involving the filesystem.\nBash Tools\nrun_cmd(cmd, argstr='', disallow_re=None, allow_re=None) - Run cmd passing split argstr, optionally checking for allowed argstr - Can include regex patterns to allow/disallow certain argument patterns (e.g., blocking parent directories)\nrg(argstr, disallow_re=None, allow_re=None) - Runs the ripgrep command for fast text searching across files - Supports regex-based argument filtering for safety\nsed(argstr, disallow_re=None, allow_re=None) - Runs sed command for reading sections of files - Useful for viewing specific line ranges with or without line numbers\nText Edit Tools\nPython implementations of the text editor tools from Anthropic. These tools are especially useful in an AI’s tool loop.\nview(path, view_range=None, nums=False) - View directory or file contents with optional line range and numbers - Can specify 1-indexed line ranges and toggle line numbers\ncreate(path, file_text, overwrite=False) - Creates a new file with the given content at the specified path - Safety feature: won’t overwrite by default\ninsert(path, insert_line, new_str) - Insert new_str at specified line number - Uses 0-based indexing\nstr_replace(path, old_str, new_str) - Replace first occurrence of old_str with new_str in file\nstrs_replace(path, old_strs, new_strs) - Replace for each str pair in old_strs, new_strs - Batch replacement of multiple string pairs\nreplace_lines(path, start_line, end_line, new_content) - Replace lines in file using start and end line-numbers (index starting at 1)\nSpecial LLM-Friendly Features\nThese tools have special behavior around errors. Since these have been specifically designed for work with LLMs, any exceptions created from their use is returned as a string to help them debug their work. These tools are designed to be safe, predictable, and easy for LLMs to use in agentic workflows.\n\nllms_txt\nThe llms-txt library provides a CLI and Python API to parse llms.txt files and create XML context files from them.\n* * The llms_txt Python module provides the source code and helpers needed to create and use llms.txt files.\nThe llms-txt library provides: 1. Parsing - Convert llms.txt markdown files into structured Python data 2. XML Generation - Transform parsed data into LLM-friendly XML context (especially for Claude) 3. CLI Tool - Command-line utility for batch processing 4. URL Fetching - Automatically fetch and include content from linked URLs Core Parsing Functions\nparse_llms_file(txt, optional=True) - * * Parse llms.txt file contents to create a data structure with the sections of an llms.txt file - * Returns an AttrDict with keys: title, summary, info, sections - optional=True - includes the optional section in the parsed output - Example: parsed.sections.Examples returns list of links in the Examples section\nparse_link(txt) - * Extracts the title, URL, and optional description from a markdown link - Returns dict with title, url, and desc keys\nInternal: _parse_llms(txt) - Lower-level parser that splits the file into start section and sectioned links - Returns tuple of (start_text, sections_dict)\nXML Context Generation\nmk_ctx(d, optional=True, n_workers=None) - * * Create a Project with a Section for each H2 part in the parsed data, optionally skipping the ‘optional’ section - * For LLMs such as Claude, XML format is preferred - Takes parsed llms.txt data structure and converts to XML - n_workers - parallel workers for fetching URLs\ncreate_ctx(txt, optional=True, n_workers=None) - * Create an LLM context file with XML sections, suitable for systems such as Claude (this is what the CLI calls behind the scenes) - High-level function that parses and converts in one step\nget_doc_content(url) - * * Fetch content from local file if in nbdev repo - Helper for retrieving document content from URLs or local paths\nCommand-Line Interface\nllms_txt2ctx (CLI command) - * * After installation, llms_txt2ctx is available in your terminal - Usage: llms_txt2ctx llms.txt &gt; llms.md - * * Pass –optional True to add the ‘optional’ section of the input file - * Uses the parsing helpers to process an llms.txt file and output an XML context file\nFile Format Specification\n* * The llms.txt file spec contains: an H1 with the name of the project (the only required section), a blockquote with a short summary, zero or more markdown sections containing detailed information, and zero or more H2-delimited sections containing “file lists” of URLs\n\n\nplaywrightnb\n* * playwrightnb provides quality-of-life helpers for interactive use of Playwright, particularly useful in Jupyter notebooks. It handles JavaScript rendering and other web complexities automatically.\nKey Features\n\nSync mode in Jupyter - Use Playwright synchronously for interactive exploration\nAutomatic iframe handling - Returns both main content and iframe contents in a dict\nJavaScript support - Handles dynamically loaded content automatically\nStealth mode - Optional bot detection avoidance\nCSS selectors - Extract specific page sections easily\nMarkdown conversion - Built-in HTML to markdown conversion\nFlexible timeouts - Configurable pause and timeout parameters\n\nUse cases: - Scraping JavaScript-heavy sites (Discord docs, dynamic help pages) - Extracting content from iframes - Interactive web exploration in notebooks - Quick conversion of web pages to markdown for LLM context\nCore Page Functions\nget_page(*args, stealth=False, **kwargs) - Get a Playwright page object - stealth=True - uses playwright-stealth to avoid bot detection - Returns an async page that can navigate to URLs\npage_ready(page, pause=50, timeout=5000) - Wait until main content of page is ready - pause - milliseconds to wait (default: 50ms) - timeout - maximum wait time in milliseconds (default: 5000ms)\nframes_ready(page, pause=50, timeout=5000) - Wait until all visible frames (if any) on page are ready - Handles dynamically loaded iframes\nwait_page(page, pause=50, timeout=5000) - Wait until page and visible frames (if any) are ready - Combines page_ready and frames_ready\nget_full_content(page) - Returns tuple of (page_content, iframes_dict) - iframes_dict maps iframe IDs to their HTML contents\nHigh-Level Reading Functions\nread_page_async(url, pause=50, timeout=5000, stealth=False, page=None) - * Return contents of url and its iframes using Playwright async - Handles JavaScript and dynamic content automatically - Returns tuple of (main_content, iframes_dict)\nread_page(url, pause=50, timeout=5000, stealth=False, page=None) - * Return contents of url and its iframes using Playwright (sync version) - Same as read_page_async but synchronous for easier interactive use - Perfect for Jupyter notebooks\nHTML to Markdown Conversion\nh2md(h) - Convert HTML string to markdown using HTML2Text - Simple utility for converting any HTML to markdown\nurl2md_async(url, sel=None, pause=50, timeout=5000, stealth=False, page=None) - Read URL with read_page_async, optionally selecting CSS selector - Converts result to markdown - sel - CSS selector to extract specific content\nurl2md(url, sel=None, pause=50, timeout=5000, stealth=False, page=None) - Read URL with read_page (sync version) - Optionally select CSS selector and convert to markdown - * Great for accessing Discord’s JS-rendered docs or other dynamic content\nget2md(url, sel=None, params=None, headers=None, cookies=None, auth=None, proxy=None, follow_redirects=False, verify=True, timeout=5.0, trust_env=True) - * Read URL with httpx.get (no JavaScript rendering) - Faster alternative when you don’t need JS rendering - Supports all standard HTTP options (headers, auth, proxies, etc.) - Optionally select CSS content with sel\n\n\ntoolslm\ntoolslm provides tools to make language models easier to use, focused on context creation and XML handling:\nContext Creation Functions: - folder2ctx - generates XML context from files, useful for providing file contents to LLMs - json_to_xml - converts JSON/dict structures to XML format\nKey modules (based on the repo structure): - xml - XML handling utilities - funccall - Function calling support - shell - Shell command integration - download - Download utilities - md_hier - Markdown hierarchy handling\nThe library is designed to help structure information (especially from codebases and files) into XML format that works well with Claude and other LLMs.\n\nxml (toolslm.xml)\n\nProvides functions for converting content to XML format optimized for LLMs (especially Claude/Anthropic):\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\njson_to_xml(d, rnm)\nConvert a JSON/dict to XML with a root name\n\n\nmk_doctype(content, src)\nCreate a named tuple with source and content\n\n\nmk_doc(index, content, src)\nCreate a single document in Anthropic’s recommended XML format\n\n\ndocs_xml(docs, srcs, prefix, details)\nCreate XML string with multiple documents in Anthropic’s format\n\n\nread_file(fname)\nRead file content, converting notebooks to XML if needed\n\n\nfiles2ctx(fnames, prefix)\nConvert multiple files to XML context\n\n\nfolder2ctx(folder, prefix, **kwargs)\nGenerate XML context from all files in a folder\n\n\nfolder2ctx_cli\nCommand-line interface for folder2ctx\n\n\nnb2xml(fname)\nConvert a Jupyter notebook to XML\n\n\ncell2xml(cell)\nConvert a notebook cell to XML format\n\n\ncell2out(o)\nConvert notebook output to XML format\n\n\n\n\nfunccall (toolslm.funccall)\n\nTools for function calling / tool use with LLMs:\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nget_schema(f)\nGenerate JSON schema for a class, function, or method (for tool definitions)\n\n\ncall_func(fc_name, fc_inputs, ns)\nCall a function by name with inputs using a namespace\n\n\ncall_func_async(fc_name, fc_inputs, ns)\nAsync version of call_func\n\n\npython(code, glb, loc, timeout)\nExecute Python code with timeout, returning final expression (like IPython)\n\n\nmk_ns(fs)\nCreate a namespace dict from functions or dicts\n\n\nPathArg(path)\nHelper for filesystem path arguments\n\n\n\n\nshell (toolslm.shell)\n\nProvides a minimal IPython shell for code execution:\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nget_shell()\nGet a TerminalInteractiveShell with minimal functionality (no logging, history, automagic)\n\n\nrun_cell(cell, timeout)\nPatched method to run cells with timeout and output capture\n\n\n\n\ndownload (toolslm.download)\n\nFunctions for fetching and processing web content for LLM consumption:\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nclean_md(text, rm_comments, rm_details)\nRemove HTML comments and &lt;details&gt; sections from markdown\n\n\nread_md(url, ...)\nRead markdown from URL and clean it\n\n\nhtml2md(s, ignore_links)\nConvert HTML string to markdown\n\n\nread_html(url, sel, multi, wrap_tag, ...)\nFetch URL, optionally select CSS elements, convert to clean markdown\n\n\nget_llmstxt(url, optional, n_workers)\nGet and expand an llms.txt file (LLM-friendly site documentation)\n\n\nsplit_url(url)\nSplit URL into base, path, and filename\n\n\nfind_docs(url)\nFind LLM-friendly docs (llms.txt or markdown) from a URL\n\n\nread_docs(url, ...)\nRead and return LLM-friendly documentation for a URL\n\n\n\n\nmd_hier (toolslm.md_hier)\n\nParse markdown into a hierarchical dictionary structure:\n\n\n\n\n\n\n\nFunction/Class\nDescription\n\n\n\n\nHeadingDict\nA dict subclass that also stores the original markdown text in .text\n\n\ncreate_heading_dict(text, rm_fenced)\nParse markdown into nested dict based on heading hierarchy (e.g., result['Section']['Subsection'].text)\n\n\n\nThis is useful for navigating and extracting specific sections from markdown documents programmatically.\nSummary: toolslm provides a complete toolkit for preparing context for LLMs - converting files/folders to XML, generating function schemas for tool calling, executing code safely, fetching web documentation, and parsing markdown hierarchically.\n\n\nmsglm - Message Creation for LLMs\nmsglm makes it easier to create messages for language models like Claude and OpenAI GPTs.\nText messages: - mk_msgs - takes a list of strings and an api format (e.g. “openai”) and generates the correct format - mk_msg - creates a single message\nAPI-specific wrappers: - mk_msg_anthropic / mk_msgs_anthropic - for Anthropic - mk_msg_openai / mk_msgs_openai - for OpenAI\nAdvanced features: - Image chats - pass raw image bytes in a list with your question - Prompt caching - pass cache=True to mk_msg or mk_msgs (Anthropic) - PDF support - pass raw pdf bytes just like image chats (Anthropic beta) - mk_ant_doc - creates document objects for Anthropic citations feature\nSummary: toolslm helps structure context (files, XML), while msglm helps format the actual message payloads for different LLM APIs, handling text, images, PDFs, and advanced features like caching and citations.\n\n\n\nLibrairies using tools\n\n\n\nLibrary\nGitHub Repo URL\nLibrary Description\n\n\n\n\nclaudette\n* * * https://github.com/AnswerDotAI/claudette\n* Wrapper for Anthropic’s Python SDK. The SDK works well, but is quite low level. Claudette automates pretty much everything that can be automated, whilst providing full control.\n\n\ncosette\n* https://github.com/AnswerDotAI/cosette\n* Claudette’s sister, a helper for OpenAI GPT. High-level wrapper for OpenAI’s SDK with stateful chat, tool calling, and streaming support.\n\n\nlisette\nhttps://github.com/AnswerDotAI/lisette\nLiteLLM helper providing unified access to 100+ LLM providers using the OpenAI API format, with stateful chat, tools, web search, and reasoning support.\n\n\nmsglm\nhttps://github.com/AnswerDotAI/msglm\nMakes it easier to create messages for LLMs (Claude, OpenAI). Handles text, images, PDFs, prompt caching, and API-specific formatting.\n\n\n\n\n\nDevelopment tools\n\n\n\nLibrary\nGitHub Repo URL\nLibrary Description\n\n\n\n\nfastcaddy\nhttps://github.com/AnswerDotAI/fastcaddy\nPython wrapper for Caddy web server integration, simplifying HTTPS setup and reverse proxy configuration for FastHTML apps.\n\n\nfastlite\n* https://github.com/AnswerDotAI/fastlite\nA thin, expressive wrapper around SQLite for Python, providing easy database operations with minimal boilerplate.\n\n\nfastlucide\nhttps://github.com/AnswerDotAI/fastlucide\nLucide icons integration for FastHTML applications, providing SVG icons as Python components.\n\n\nfasthtml\nhttps://github.com/AnswerDotAI/fasthtml\nModern Python web framework for building HTML applications with HTMX and Starlette, using pure Python for HTML generation.\n\n\nghapi\nhttps://github.com/AnswerDotAI/ghapi\n* A delightful and complete interface to GitHub’s amazing API. Auto-generated Python client covering all GitHub API endpoints.\n\n\nmonsterui\n* https://github.com/AnswerDotAI/MonsterUI\n* MonsterUI is a FastHTML Tailwind-powered UI framework for building beautiful web interfaces with minimal code.\n\n\nnbdev\n* https://github.com/AnswerDotAI/nbdev\nLiterate programming framework that allows writing library code, tests, and documentation in Jupyter Notebooks, then exporting to Python modules.\n\n\nshell_sage\nhttps://github.com/AnswerDotAI/shell_sage\n* ShellSage saves sysadmins’ sanity by solving shell script snafus super swiftly. AI-powered shell command helper.",
    "crumbs": [
      "Answerai tools study"
    ]
  },
  {
    "objectID": "tools_study_answerai.html#tools-catalog",
    "href": "tools_study_answerai.html#tools-catalog",
    "title": "Answerai tools study",
    "section": "Tools catalog",
    "text": "Tools catalog\nHere is the list of tools we want to develop.\n\nEnhance IPython kernel\n\nipykernel_helper\nipykernel_helper is a library that provides helper utilities for working with IPython/Jupyter kernels.\nIt appears to be part of the infrastructure that makes the Dialog Engineering environment work smoothly, providing convenient functions that are useful in an interactive coding environment.\nIPython InteractiveShell - Complete Feature Summary\nInteractiveShell is the core class of IPython, providing an enhanced, interactive Python environment. It’s a singleton class that manages everything from code execution to history, completion, and output formatting.\n\nCode Execution\n\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\nrun_cell(raw_cell, store_history, silent, shell_futures, cell_id)\nRun a complete IPython cell including magics\n\n\nrun_cell_async(...)\nAsync version of run_cell\n\n\nrun_code(code_obj, result, async_)\nExecute a compiled code object\n\n\nrun_ast_nodes(nodelist, cell_name, interactivity, compiler, result)\nRun a sequence of AST nodes\n\n\nrun_line_magic(magic_name, line)\nExecute a line magic like %timeit\n\n\nrun_cell_magic(magic_name, line, cell)\nExecute a cell magic like %%bash\n\n\nsafe_execfile(fname, *where, ...)\nSafely execute a .py file\n\n\nsafe_execfile_ipy(fname, ...)\nExecute .ipy or .ipynb files with IPython syntax\n\n\nsafe_run_module(mod_name, where)\nSafe version of runpy.run_module()\n\n\n\n\nNamespace Management\n\n\n\n\n\n\n\n\nMethod/Property\nDescription\n\n\n\n\nuser_ns\nThe user’s namespace dictionary\n\n\nuser_ns_hidden\nHidden namespace items (not shown to user)\n\n\nall_ns_refs\nList of all namespace dictionaries where objects may be stored\n\n\npush(variables, interactive)\nInject variables into user namespace\n\n\ndel_var(varname, by_name)\nDelete a variable from namespaces\n\n\ndrop_by_id(variables)\nRemove variables if they match given values\n\n\nev(expr)\nEvaluate Python expression in user namespace\n\n\nex(cmd)\nExecute Python statement in user namespace\n\n\ninit_user_ns()\nInitialize user-visible namespaces to defaults\n\n\nreset(new_session, aggressive)\nClear all internal namespaces\n\n\nreset_selective(regex)\nClear variables matching a regex pattern\n\n\n\n\nCode Completion\n\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\ninit_completer()\nInitialize completion machinery\n\n\ncomplete(text, line, cursor_pos)\nReturn completed text and list of completions\n\n\nset_completer_frame(frame)\nSet the frame for completion context\n\n\nset_custom_completer(completer, pos)\nAdd a custom completer function\n\n\n\n\nCode Transformation\n\n\n\n\n\n\n\n\nMethod/Property\nDescription\n\n\n\n\ntransform_cell(raw_cell)\nTransform input cell before parsing (handles %magic, !system, etc.)\n\n\ntransform_ast(node)\nApply AST transformations from ast_transformers\n\n\ninput_transformers_post\nList of string transformers applied after IPython’s own\n\n\nast_transformers\nList of ast.NodeTransformer instances for code modification\n\n\ncheck_complete(code)\nCheck if code is ready to execute or needs continuation\n\n\n\n\nHistory Management\n\n\n\n\n\n\n\n\nMethod/Property\nDescription\n\n\n\n\ninit_history()\nSetup command history and autosaves\n\n\nextract_input_lines(range_str, raw)\nReturn input history slices as string\n\n\nhistory_length\nTotal length of command history\n\n\nhistory_load_length\nNumber of entries loaded at startup\n\n\n\n\nMagic System\n\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\nfind_magic(magic_name, magic_kind)\nFind and return a magic by name\n\n\nfind_line_magic(magic_name)\nFind a line magic\n\n\nfind_cell_magic(magic_name)\nFind a cell magic\n\n\nregister_magic_function(func, magic_kind, magic_name)\nExpose a function as a magic\n\n\ndefine_macro(name, themacro)\nDefine a new macro\n\n\n\n\nObject Inspection\n\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\nobject_inspect(oname, detail_level)\nGet object info\n\n\nobject_inspect_mime(oname, detail_level, omit_sections)\nGet object info as mimebundle\n\n\nobject_inspect_text(oname, detail_level)\nGet object info as formatted text\n\n\nfind_user_code(target, raw, py_only, ...)\nGet code from history, file, URL, or string\n\n\n\n\nError Handling & Debugging\n\n\n\n\n\n\n\n\nMethod/Property\nDescription\n\n\n\n\nshowtraceback(exc_tuple, filename, tb_offset, ...)\nDisplay exception that just occurred\n\n\nshowsyntaxerror(filename, running_compiled_code)\nDisplay syntax error\n\n\nshowindentationerror()\nHandle IndentationError\n\n\nget_exception_only(exc_tuple)\nGet exception string without traceback\n\n\nset_custom_exc(exc_tuple, handler)\nSet custom exception handler\n\n\nexcepthook(etype, value, tb)\nCustom sys.excepthook\n\n\ndebugger(force)\nCall pdb debugger\n\n\ncall_pdb\nControl auto-activation of pdb at exceptions\n\n\npdb\nAutomatically call pdb after every exception\n\n\nxmode\nSwitch modes for exception handlers\n\n\n\n\nSystem Interaction\n\n\n\n\nMethod\nDescription\n\n\n\n\nsystem(cmd)\nCall cmd in subprocess, piping stdout/err\n\n\nsystem_piped(cmd)\nCall cmd with piped output\n\n\nsystem_raw(cmd)\nCall cmd using os.system or subprocess\n\n\ngetoutput(cmd, split, depth)\nGet output from subprocess\n\n\n\n\nExpression Evaluation\n\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\nuser_expressions(expressions)\nEvaluate dict of expressions, return rich mime-typed display_data\n\n\nvar_expand(cmd, depth, formatter)\nExpand Python variables in a string\n\n\n\n\nAsync Support\n\n\n\n\n\n\n\n\nProperty/Method\nDescription\n\n\n\n\nautoawait\nAutomatically run await in top-level REPL\n\n\nshould_run_async(raw_cell, ...)\nDetermine if cell should run via coroutine runner\n\n\nloop_runner\nSelect the loop runner for async code\n\n\n\n\nConfiguration Properties\n\n\n\n\n\n\n\n\nProperty\nDescription\n\n\n\n\nautocall\nAuto-call callable objects (0=off, 1=smart, 2=full)\n\n\nautoindent\nAuto-indent code\n\n\nautomagic\nCall magics without leading %\n\n\ncolors\nColor scheme (nocolor, neutral, linux, lightbg)\n\n\ncache_size\nOutput cache size (default 1000)\n\n\nast_node_interactivity\nWhich nodes display output (‘all’, ‘last’, ‘last_expr’, ‘none’, ‘last_expr_or_assign’)\n\n\nbanner1 / banner2\nBanner text before/after profile\n\n\ndisplay_page\nShow pager content as regular output\n\n\nenable_html_pager\nEnable HTML in pagers\n\n\nenable_tip\nShow tip on IPython start\n\n\nshow_rewritten_input\nShow rewritten input for autocall\n\n\nsphinxify_docstring\nEnable rich HTML docstrings\n\n\nwarn_venv\nWarn if running in venv without IPython installed\n\n\n\nArchitecture Summary\n┌─────────────────────────────────────────────────────────────────────┐\n│                    InteractiveShell (Singleton)                     │\n├─────────────────────────────────────────────────────────────────────┤\n│  NAMESPACES                                                         │\n│    user_ns, user_ns_hidden, all_ns_refs                            │\n├─────────────────────────────────────────────────────────────────────┤\n│  CODE EXECUTION PIPELINE                                            │\n│    raw_cell → transform_cell → parse → transform_ast → run_code    │\n├─────────────────────────────────────────────────────────────────────┤\n│  SUBSYSTEMS                                                         │\n│    • Completer (tab completion)                                     │\n│    • History Manager                                                │\n│    • Magic System (line/cell magics)                               │\n│    • Display Publisher                                              │\n│    • Exception Handler / Debugger                                   │\n│    • Input Transformers (magics, shell commands)                   │\n│    • AST Transformers                                               │\n├─────────────────────────────────────────────────────────────────────┤\n│  EXTENSIONS                                                         │\n│    • Hooks (set_hook)                                               │\n│    • Custom completers (set_custom_completer)                      │\n│    • Custom exception handlers (set_custom_exc)                    │\n│    • Magic functions (register_magic_function)                     │\n└─────────────────────────────────────────────────────────────────────┘\nHow ipykernel_helper Extends InteractiveShell\nThe ipykernel_helper library uses fastcore’s @patch to add methods to InteractiveShell:\n\n\n\nAdded Method\nPurpose\n\n\n\n\nuser_items()\nGet user-defined vars & functions (filtered)\n\n\nget_vars(vs)\nGet specific variable values\n\n\nget_schemas(fs)\nGet JSON schemas for functions (tool calling)\n\n\nranked_complete(code, ...)\nEnhanced completion with ranking\n\n\nsig_help(code, ...)\nSignature help using Jedi\n\n\nxpush(**kw)\nPush with kwargs\n\n\npublish(data, ...)\nEnhanced display publishing\n\n\n\nThis is how solveit extends IPython’s core to support AI-assisted coding!\n\n\nExtract information from the user’s namespace\nThese are methods patched onto IPython’s InteractiveShell by ipykernel_helper.\nGetting an InteractiveShell instance from a notebook code cell is straightforward.\nfrom IPython import get_ipython\nshell = get_ipython()\nThey help extract information from the user’s namespace for use in dialog/AI contexts.\nInteractiveShell.user_items(self, max_len=200, xtra_skip=())\nPurpose: Get user-defined variables and functions from the IPython namespace, filtering out system/internal items.\nReturns: Tuple of (user_vars, user_fns) - user_vars: Dict of {var_name: repr_string} - variable names and their string representations (truncated to max_len) - user_fns: Dict of {func_name: signature_string} - user-defined function names and their signatures\nWhat it filters out: - Hidden namespace items (user_ns_hidden) - Items starting with _ - Types, modules, methods, builtins, typing constructs - Specific names like nbmeta, receive_nbmeta\nUse case: Let the AI know what variables and functions the user has defined in their session.\nInteractiveShell.get_vars(self:InteractiveShell, vs:list, literal=True)\nPurpose: Retrieve specific variable values from the namespace\nParameters: - vs: List of variable names to retrieve - literal: If True, try to return actual Python literals; if False, return string representation\nReturns: - Dict of {var_name: value} for variables that exist in the namespace\nThe literal=True behavior: - Tries to repr() the value, then literal_eval() it back - If that succeeds, returns the actual value (works for dicts, lists, strings, numbers, etc.) - If it fails (complex objects), returns the string representation instead\nUse case: When the user shares specific variables with the AI using $varname notation, this function retrieves their current values.\nInteractiveShell.get_schemas(self:InteractiveShell, fs:list)\nPurpose: Get JSON schemas for functions (for LLM tool calling).\nParameters: - fs: List of function names to get schemas for\nReturns: Dict of {func_name: schema_or_error} - If successful: {‘type’: ‘function’, ‘function’: {JSON schema}} - If not found: “funcname not found. Did you run it?” - If schema error: “funcname: error message”\nThe schema format (via toolslm.funccall.get_schema):\n{\n    'type': 'function',\n    'function': {\n        'name': 'my_func',\n        'description': 'Docstring here',\n        'parameters': {\n            'type': 'object',\n            'properties': {\n                'param1': {'type': 'string', 'descmription': '...'},\n                'param2': {'type': 'integer', 'description': '...'}\n            },\n            'required': ['param1']\n        }\n    }\n}\nUse case: When the user shares tools with the AI using &toolname notation, this function generates the JSON schemas that tell the AI how to call those functions.\n\n\nSolveit use of get_vars() and get_schema()\nThe $varname and &toolname Notation System\nOverview\nWhen you write a prompt in solveit containing $myvar or &mytool, the system: 1. Parses your prompt for these special notations 2. Extracts the referenced names 3. Retrieves their values/schemas from the Python namespace 4. Injects this information into the context sent to the AI\nVariable Sharing: $varname\nWhat Happens\nUser writes prompt:\n┌─────────────────────────────────────────────┐\n│ \"Analyze the data in `$df` and tell me     │\n│  the average of the 'sales' column\"         │\n└─────────────────────────────────────────────┘\n                    │\n                    ▼\n            Parse for $-prefixed names\n                    │\n                    ▼\n            Found: ['df']\n                    │\n                    ▼\n            Call: shell.get_vars(['df'])\n                    │\n                    ▼\n            Retrieve current value of df\n                    │\n                    ▼\n┌─────────────────────────────────────────────┐\n│ Context sent to AI includes:                │\n│                                             │\n│ &lt;variables&gt;                                 │\n│   df = DataFrame with 100 rows, columns:    │\n│        ['date', 'sales', 'region', ...]     │\n│ &lt;/variables&gt;                                │\n│                                             │\n│ User prompt: \"Analyze the data in df...\"    │\n└─────────────────────────────────────────────┘\nKey Behavior\n\nCurrent value: The variable’s value at the time the prompt is sent (not when it was first defined)\nRetroactive updates: If you change the variable and re-run, the context updates\nSafe representation: Uses _safe_repr() to truncate large values (default 200 chars)\nLiteral conversion: Tries to preserve actual Python types when possible via literal_eval\n\nExample Flow\n# Cell 1: Define data\ndf = pd.DataFrame({'a': [1,2,3], 'b': [4,5,6]})\n\n# Cell 2: Prompt with $df\n# \"What's the sum of column 'a' in `$df`?\"\n\n# Behind the scenes:\nshell.get_vars(['df'])\n# Returns: {'df': \"   a  b\\n0  1  4\\n1  2  5\\n2  3  6\"}\nTool Sharing: &toolname\nWhat Happens\nUser writes prompt:\n┌─────────────────────────────────────────────┐\n│ \"Use `&calculate` to add 15 and 27\"         │\n└─────────────────────────────────────────────┘\n                    │\n                    ▼\n            Parse for &-prefixed names\n                    │\n                    ▼\n            Found: ['calculate']\n                    │\n                    ▼\n            Call: shell.get_schemas(['calculate'])\n                    │\n                    ▼\n            Generate JSON schema from function\n                    │\n                    ▼\n┌─────────────────────────────────────────────┐\n│ AI receives tool definition:                │\n│                                             │\n│ {                                           │\n│   \"type\": \"function\",                       │\n│   \"function\": {                             │\n│     \"name\": \"calculate\",                    │\n│     \"description\": \"Add two numbers\",       │\n│     \"parameters\": {                         │\n│       \"type\": \"object\",                     │\n│       \"properties\": {                       │\n│         \"a\": {\"type\": \"integer\"},           │\n│         \"b\": {\"type\": \"integer\"}            │\n│       },                                    │\n│       \"required\": [\"a\", \"b\"]                │\n│     }                                       │\n│   }                                         │\n│ }                                           │\n└─────────────────────────────────────────────┘\nRequirements for Tools\nFrom dialoghelper.core.is_usable_tool(), a function must have: 1. Type annotations for all parameters 2. A docstring (becomes the tool description)\n# ✅ Valid tool\ndef calculate(a: int, b: int) -&gt; int:\n    \"Add two numbers together\"\n    return a + b\n\n# ❌ Not usable - no docstring\ndef bad_tool(a: int, b: int) -&gt; int:\n    return a + b\n\n# ❌ Not usable - missing type hints\ndef another_bad(a, b):\n    \"Add numbers\"\n    return a + b\nTool Execution Flow\nWhen the AI decides to use a tool:\n┌─────────────────────────────────────────────┐\n│ AI Response:                                │\n│ \"I'll use calculate to add those numbers\"   │\n│                                             │\n│ Tool Call:                                  │\n│   name: \"calculate\"                         │\n│   arguments: {\"a\": 15, \"b\": 27}             │\n└─────────────────────────────────────────────┘\n                    │\n                    ▼\n            solveit receives tool call\n                    │\n                    ▼\n            Look up 'calculate' in namespace\n                    │\n                    ▼\n            Execute: calculate(a=15, b=27)\n                    │\n                    ▼\n            Result: 42\n                    │\n                    ▼\n┌─────────────────────────────────────────────┐\n│ Tool result sent back to AI                 │\n│                                             │\n│ AI continues: \"The sum of 15 and 27 is 42\"  │\n└─────────────────────────────────────────────┘\nThe Parsing Mechanism\nThe parsing likely uses regex to find these patterns:\nimport re\n\ndef parse_prompt(prompt):\n    # Find $varname in backticks\n    var_pattern = r'`\\$(\\w+)`'\n    vars_found = re.findall(var_pattern, prompt)\n    \n    # Find &toolname in backticks\n    tool_pattern = r'`&(\\w+)`'\n    tools_found = re.findall(tool_pattern, prompt)\n    \n    return vars_found, tools_found\n\n# Example\nprompt = \"Use `&calculate` on `$x` and `$y`\"\nparse_prompt(prompt)\n# Returns: (['x', 'y'], ['calculate'])\nComplete Architecture\n┌─────────────────────────────────────────────────────────────────────┐\n│                      USER WRITES PROMPT                             │\n│  \"Use `&my_tool` to process `$my_data` and return the result\"      │\n└─────────────────────────────────────────────────────────────────────┘\n                              │\n                              ▼\n┌─────────────────────────────────────────────────────────────────────┐\n│                      SOLVEIT FRONTEND                               │\n│  1. Parse prompt for `$name` and `&name` patterns                  │\n│  2. Extract: vars=['my_data'], tools=['my_tool']                   │\n│  3. Request from kernel: get_vars(), get_schemas()                 │\n└─────────────────────────────────────────────────────────────────────┘\n                              │\n                              ▼\n┌─────────────────────────────────────────────────────────────────────┐\n│                      IPYKERNEL (Python)                             │\n│  shell.get_vars(['my_data'])                                       │\n│    → {'my_data': [1, 2, 3, 4, 5]}                                  │\n│                                                                     │\n│  shell.get_schemas(['my_tool'])                                    │\n│    → {'my_tool': {'type': 'function', 'function': {...}}}          │\n└─────────────────────────────────────────────────────────────────────┘\n                              │\n                              ▼\n┌─────────────────────────────────────────────────────────────────────┐\n│                      CONTEXT ASSEMBLY                               │\n│  &lt;variables&gt;                                                        │\n│    my_data = [1, 2, 3, 4, 5]                                       │\n│  &lt;/variables&gt;                                                       │\n│                                                                     │\n│  Tools: [my_tool schema]                                           │\n│                                                                     │\n│  Prompt: \"Use my_tool to process my_data...\"                       │\n└─────────────────────────────────────────────────────────────────────┘\n                              │\n                              ▼\n┌─────────────────────────────────────────────────────────────────────┐\n│                      AI (Claude/etc)                                │\n│  - Sees variable values in context                                 │\n│  - Has tool schemas available                                      │\n│  - Can call tools, results executed in user's kernel               │\n└─────────────────────────────────────────────────────────────────────┘\nKey Points\n\n\n\n\n\n\n\n\nAspect\n$varname\n&toolname\n\n\n\n\nPurpose\nShare variable value\nShare function as callable tool\n\n\nRetrieval\nget_vars()\nget_schemas()\n\n\nWhat AI sees\nCurrent value (as string/literal)\nJSON schema for tool calling\n\n\nRequirements\nVariable must exist\nFunction needs types + docstring\n\n\nAI can modify?\nNo (read-only snapshot)\nYes (can call the function)\n\n\n\nPractical Tips\n\nVariables are snapshots: The AI sees the value when you send the prompt\nTools execute in your kernel: Side effects happen in your session\nUse backticks: The notation is `$var` and `&tool`, not bare $var\nCheck tool validity: Use is_usable_tool(func) to verify a function can be used as a tool\nList tools: Use mk_toollist(symbols) from dialoghelper to see available tools formatted for prompts\n\n\n\nCustomize Jupyterlab default behavior to enhance variables display\nLet me explain how JupyterLab’s Debugger panel works and whether you could hook into it.\nHow JupyterLab’s Debugger Panel Gets Variables\nThe Debugger panel in JupyterLab uses a completely different mechanism than these ipykernel_helper functions: 1. Debug Adapter Protocol (DAP) - JupyterLab’s debugger communicates via the DAP standard 2. debugpy - The Python debugger backend 3. Kernel debug messages - Special debug_request/debug_reply messages over ZMQ The flow looks like:\nJupyterLab Debugger UI\n        │\n        ▼ (DAP protocol)\n    debugpy \n        │\n        ▼ (inspects frames)\n    Python runtime\nThe Challenge\nThe user_items(), get_vars(), get_schemas() functions are: - Pull-based: Called on demand by the solveit frontend - Custom protocol: Use transient display messages or custom endpoints - Namespace-focused: Look at InteractiveShell.user_ns\nJupyterLab’s Debugger is: - DAP-based: Uses standardized debug adapter protocol - Frame-focused: Inspects stack frames, not just the namespace - debugpy-controlled: The variable inspection happens inside debugpy\nOption: Custom Variable Inspector Extension\nInstead of modifying the Debugger, create a JupyterLab extension that: - Sends custom comm messages to the kernel - Calls user_items() or similar - Displays results in a custom panel\n# Kernel side\nfrom ipykernel.comm import Comm\n\ndef send_variables():\n    shell = get_ipython()\n    user_vars, user_fns = shell.user_items()\n    comm = Comm(target_name='variable_inspector')\n    comm.send({'vars': user_vars, 'fns': user_fns})\nPractical Recommendation\nThe easiest approach that gives you similar functionality to solveit’s variable display in standard JupyterLab:\n# In your notebook or startup file\nfrom IPython import get_ipython\nfrom IPython.display import display, HTML\nimport json\n\ndef show_user_items(max_len=100):\n    \"\"\"Display user variables and functions like solveit does\"\"\"\n    shell = get_ipython()\n    user_vars, user_fns = shell.user_items(max_len=max_len)\n    \n    html = \"&lt;details open&gt;&lt;summary&gt;&lt;b&gt;Variables&lt;/b&gt;&lt;/summary&gt;&lt;ul&gt;\"\n    for k, v in user_vars.items():\n        html += f\"&lt;li&gt;&lt;code&gt;{k}&lt;/code&gt;: {v}&lt;/li&gt;\"\n    html += \"&lt;/ul&gt;&lt;/details&gt;\"\n    \n    html += \"&lt;details open&gt;&lt;summary&gt;&lt;b&gt;Functions&lt;/b&gt;&lt;/summary&gt;&lt;ul&gt;\"\n    for k, v in user_fns.items():\n        html += f\"&lt;li&gt;&lt;code&gt;{k}{v}&lt;/code&gt;&lt;/li&gt;\"\n    html += \"&lt;/ul&gt;&lt;/details&gt;\"\n    \n    display(HTML(html))\n\n# Auto-show after each cell (optional)\n# get_ipython().events.register('post_run_cell', lambda r: show_user_items())\n\n\nProvide smarter code completion and signature help\nWhy These Are Better Than Default IPython\nDefault IPython completion: - No custom ranking (alphabetical or frequency-based) - Includes all dunder methods always - Doesn’t prioritize user code\nThese custom versions: - Smart ranking: user code &gt; builtins &gt; everything else - Filters dunder unless explicitly requested - Context-aware (knows about your namespace) - Provides structured data for rich UI display\nInteractiveShell.ranked_complete(self:InteractiveShell, code, line_no=None, col_no=None)\nPurpose: Provide intelligent, ranked code completions using Jedi, with custom ranking logic.\nParameters: - code: The code string to complete - line_no: Optional line number (1-indexed) - col_no: Optional column number (1-indexed)\nReturns: List of completion objects with these attributes: - text - The completion text - type - Type of completion (param, function, module, etc.) - mod - Module where the item is defined - rank - Numeric rank (lower = higher priority)\nRanking logic (lower is better): - Rank 1: Parameters (function arguments being filled) - Rank 2: Local variables/functions (from main) - Rank 3: Module members (when completing module.something) - Rank 4: Builtins (like print, len, etc.) - Rank 5: Everything else (imported modules, third-party) - Rank +0.1: Private items (starting with _) - slightly lower priority\nSpecial handling: - Filters out dunder methods unless the user explicitly types __ (so they’re not cluttering normal completion) - Deprioritizes private _methods slightly but doesn’t remove them\nUse case: Provide better autocomplete suggestions in the solveit dialog environment, prioritizing user-defined items and parameters over stdlib/third-party items.\nInteractiveShell.sig_help(self:InteractiveShell, code, line_no=None, col_no=None)\nPurpose: Get function signature information at the cursor position (like when you type func( and want to see the parameters).\nParameters: - code The code string - line_no Line number where cursor is (1-indexed) - col_no Column number where cursor is (1-indexed)\nReturns: List of signature objects, each containing: - label - Full signature description (e.g., “print(value, …, sep=’ ‘, end=’’)”) - typ - Type of callable (function, method, class, etc.) - mod - Module name where it’s defined d- oc - Full docstring - idx - Current parameter index (which parameter the cursor is on) - params - List of parameter dicts with name and desc\nHow it works: - Uses Jedi’s Interpreter with the current namespace to get context-aware signatures - Falls back to Script (static analysis) if Interpreter doesn’t find anything - Extracts detailed information about each signature - Returns structured data about parameters and documentation\nUse case: Power the signature help tooltip in the solveit editor, showing: - What parameters a function takes - Which parameter you’re currently typing - Documentation for each parameter - Full docstring\nInspector._get_info(self:Inspector, obj, oname='', formatter=None, info=None, detail_level=0, omit_sections=())\nPurpose: Customizes the ?? (double question mark) output to display source code as formatted Markdown.\nNote: This is patched onto Inspector, not InteractiveShell.\nParameters: - obj: The object being inspected - oname Object name (string) - formatter: Optional formatter - info: Pre-computed info dict - detail_level: 0 = basic (?), 1 = detailed (??) - omit_sections: Sections to skip\nHow it works: - Calls the original _get_info method first (stored as _orig__get_info) - If detail_level == 0 (single ?), returns original output unchanged - If detail_level == 1 (double ??), creates enhanced Markdown output: - Source code in a Python fenced code block - File path in bold with backticks\nUse case: Makes the ?? inspection output much more readable in environments that support Markdown rendering\n\n\nOverride IPython’s default behavior to enhance completions\nJupyterLab communicates with the kernel via the Jupyter messaging protocol:\n┌─────────────────┐                    ┌─────────────────┐\n│   JupyterLab    │  complete_request  │    IPython      │\n│   Frontend      │ ─────────────────► │    Kernel       │\n│                 │                    │                 │\n│                 │ ◄───────────────── │                 │\n│                 │  complete_reply    │                 │\n└─────────────────┘                    └─────────────────┘\nThe relevant message types are: - complete_request / complete_reply - for autocompletion - inspect_request / inspect_reply - for signature/documentation help\nPatch do_complete on the Kernel\nThe kernel’s do_complete method handles complete_request messages. You can patch it:\nfrom ipykernel.ipkernel import IPythonKernel\nfrom functools import wraps\n\n# Store original\n_original_do_complete = IPythonKernel.do_complete\n\n@wraps(_original_do_complete)\ndef custom_do_complete(self, code, cursor_pos):\n    \"\"\"Enhanced completion with ranking.\"\"\"\n    # Get original reply\n    reply = _original_do_complete(self, code, cursor_pos)\n    \n    if reply['status'] == 'ok':\n        matches = reply['matches']\n        \n        # Custom ranking logic\n        def rank(name):\n            ...\n        \n        # Filter __dunder__ unless explicitly typing them\n        text = code[:cursor_pos].split()[-1] if code[:cursor_pos].split() else ''\n        if '__' not in text:\n            matches = [m for m in matches if not m.startswith('__')]\n        \n        # Sort by rank\n        reply['matches'] = sorted(matches, key=rank)\n    \n    return reply\n\n# Apply patch\nIPythonKernel.do_complete = custom_do_complete\nPatch do_inspect for Signature Help\nFor enhanced signature/documentation help:\nfrom ipykernel.ipkernel import IPythonKernel\nfrom jedi import Interpreter\nfrom functools import wraps\n\n_original_do_inspect = IPythonKernel.do_inspect\n\n@wraps(_original_do_inspect)\ndef custom_do_inspect(self, code, cursor_pos, detail_level=0):\n    \"\"\"Enhanced inspect with Jedi signatures.\"\"\"\n    # Try Jedi first for better signature info\n    try:\n        ns = self.shell.user_ns\n        \n        # Get signatures at cursor\n        ...\n            \n            return {\n                'status': 'ok',\n                'found': True,\n                'data': {'text/markdown': text, 'text/plain': sig.docstring()},\n                'metadata': {}\n            }\n    except Exception:\n        pass\n    \n    # Fall back to original\n    return _original_do_inspect(self, code, cursor_pos, detail_level)\n\nIPythonKernel.do_inspect = custom_do_inspect\nCreate an IPython Extension\nPackage everything as a proper IPython extension:\ndef load_ipython_extension(ip):\n    \"\"\"Called when extension is loaded via %load_ext\"\"\"\n    from ipykernel.ipkernel import IPythonKernel\n    from functools import wraps\n    \n    # Patch do_complete\n    _orig = IPythonKernel.do_complete\n    \n    @wraps(_orig)\n    def patched_do_complete(self, code, cursor_pos):\n        ...\n    \n    IPythonKernel.do_complete = patched_do_complete\n    print(\"Enhanced completions loaded!\")\n\ndef unload_ipython_extension(ip):\n    \"\"\"Called when extension is unloaded\"\"\"\n    # Restore original if needed\n    pass\nUsage:\n%load_ext my_completer_extension\nOr add to ipython_config.py:\nc.InteractiveShellApp.extensions = ['my_completer_extension']\n\n\nProgrammatically inject variables into the user namespace\nInteractiveShell.xpush(self:InteractiveShell, interactive=False, **kw):\n    \"Like `push`, but with kwargs\"\n    self.push(kw, interactive=interactive)\nPurpose: A convenience wrapper around InteractiveShell.push() that accepts keyword arguments instead of a dictionary.\nParameters: - interactive: If True, the variables are treated as if typed interactively (affects display) - **kw: Variables to inject as keyword arguments\nComparison:\nshell = get_ipython()\n\n# Standard push() - requires a dictionary\nshell.push({'x': 42, 'name': 'Alice'})\n\n# xpush() - cleaner kwargs syntax\nshell.xpush(x=42, name='Alice')\nUse case: Cleaner API when programmatically injecting variables into the user namespace.\nWhat InteractiveShell.push() does\nPurpose: Programmatically add variables to the user’s namespace - as if the user had typed them directly in the notebook.\nThe interactive Parameter controls how the variables are treated: - interactive=True (default): - Variables treated as if user typed them - Subject to display hooks (e.g., last expression displays) - Triggers post_execute hooks - Can show output - interactive=False: - Variables injected silently - No display output - Minimal side effects - Used for “behind the scenes” injection\nUse Cases in solveit\n\nTool Results Injection\n\nWhen the AI calls a tool, the result needs to be available to the user:\n# AI calls a tool: calculate(5, 3)\ndef calculate(a: int, b: int) -&gt; int:\n    return a + b\n\nresult = calculate(5, 3)  # Tool executes\n\n# Inject result into user namespace\nshell.xpush(tool_result=result, interactive=False)\n\n# User can now access it:\nprint(tool_result)  # 8\nWhere: claudette.toolloop / cosette.toolloop / lisette.core\nFile & Function:\n# claudette/toolloop.py\n@patch\ndef toolloop(self:Chat, pr, max_steps=10, ...):\n    # After tool execution, results are automatically added to chat history\n    # The tool result becomes part of the conversation context\nImplementation: Tool results are typically kept in the chat history rather than pushed to namespace. However, if you wanted to inject them:\n# dialoghelper/core.py (hypothetical)\ndef execute_tool_and_inject(tool_name, args, ns):\n    result = call_func(tool_name, args, ns)\n    shell = get_ipython()\n    shell.xpush(**{f'{tool_name}_result': result}, interactive=False)\n    return result\nActual location: Not directly implemented in the libraries we explored - this would be custom solveit backend code.\n\nLoading Context from External Sources\n\nWhen importing from a gist or URL:\n# User runs: import_gist('abc123')\n# Behind the scenes:\ngist_code = fetch_gist('abc123')\nexec(gist_code, globals_dict := {})\n\n# Inject all functions/classes from gist\nshell.xpush(interactive=False, **globals_dict)\n\n# Now user has access to everything from the gist\nWhere: dialoghelper.core\nFile & Function:\n# dialoghelper/core.py\ndef import_gist(gist_id:str, mod_name:str='gist', run:bool=False)\nKey line: shell.user_ns[k] = v - directly modifies namespace (equivalent to push)\n\nAI-Generated Variables\n\nWhen AI generates code that creates variables:\n# AI generates this code:\ncode = \"\"\"\nimport pandas as pd\ndf = pd.DataFrame({'a': [1,2,3], 'b': [4,5,6]})\n\"\"\"\n\n# Execute and capture namespace\nexec_namespace = {}\nexec(code, exec_namespace)\n\n# Push the dataframe to user\nshell.xpush(df=exec_namespace['df'])\n\n# User now has 'df' available\ndf.head()\nWhere: execnb.shell / toolslm.shell\nFile & Function:\n# toolslm/shell.py\ndef get_shell():\n    \"Get a `TerminalInteractiveShell` with minimal functionality\"\n    # Shell maintains its own namespace\n    # Variables from executed code live in shell.user_ns\n# execnb/shell.py (from execnb library)\nclass CaptureShell:\n    def run(self, code):\n        # Executes code and captures outputs\n        # Variables stay in shell's namespace\nFor injecting into main namespace: Custom solveit code would do:\nshell = get_ipython()\nshell.xpush(**exec_namespace, interactive=False)\n\nSharing Variables Between Messages\n\nWhen running code in one message and needing results in another:\n# Message 1 (code cell):\nx = expensive_computation()\n\n# Behind the scenes, solveit might:\nshell.xpush(_last_result=x, interactive=False)\n\n# Message 2 (prompt to AI):\n# AI can reference _last_result\nWhere: dialoghelper.core\nFile & Function:\n# dialoghelper/core.py\n\ndef add_msg(content, placement='afterCurrent', msgid=None, msg_type='note', ...):\n    \"Add a message to dialog\"\n    # Messages can contain code that executes\n    # Results are automatically in namespace\n\ndef run_msg(msgid, dname=None):\n    \"Queue a message for execution\"\n    # Executes code cell, results go to namespace automatically\nImplementation: When code cells execute in solveit, they naturally share the same namespace. No explicit push() needed - it’s the default behavior.\n\nPre-loading Helper Functions\n\nWhen starting a dialog, inject helper utilities:\n# On dialog start\nfrom dialoghelper import read_msg, add_msg, update_msg\n\nshell.xpush(\n    read_msg=read_msg,\n    add_msg=add_msg,\n    update_msg=update_msg,\n    interactive=False\n)\n\n# Now available everywhere in the dialog\nWhere: ipykernel_helper.core\nFile & Function:\n# ipykernel_helper/core.py\ndef load_ipython_extension(ip):\n    \"Load extension and inject helper functions\"\n    from ipykernel_helper import transient, run_cmd\n    ns = ip.user_ns\n    ns['read_url'] = read_url\n    ns['transient'] = transient\n    ns['run_cmd'] = run_cmd\nKey line: ns['read_url'] = read_url - directly modifies namespace dictionary\nAlso:\n# dialoghelper/core.py (hypothetical startup)\ndef initialize_dialog_helpers():\n    shell = get_ipython()\n    shell.xpush(\n        read_msg=read_msg,\n        add_msg=add_msg,\n        update_msg=update_msg,\n        find_msgs=find_msgs,\n        interactive=False\n    )\n\nRestoring Session State\n\nWhen loading a saved dialog:\n# Load saved state\nsaved_vars = load_dialog_state('dialog_123')\n# saved_vars = {'x': 42, 'data': [...], 'model': &lt;object&gt;}\n\n# Restore to namespace\nshell.xpush(interactive=False, **saved_vars)\n\n# User's variables are back\nWhere: Not explicitly implemented in the libraries we explored\nWould be in: Custom solveit backend (not open source)\nHypothetical implementation:\n# solveit/session.py (hypothetical)\n\ndef restore_dialog_state(dialog_id):\n    \"Restore variables from saved dialog\"\n    import pickle\n    \n    # Load saved state\n    with open(f'dialogs/{dialog_id}/state.pkl', 'rb') as f:\n        saved_vars = pickle.load(f)\n    \n    # Restore to namespace\n    shell = get_ipython()\n    shell.xpush(interactive=False, **saved_vars)\n    \n    return saved_vars\nKey Insight\nMost of these behaviors don’t explicitly call push() or xpush() - they use alternative approaches:\n\nDirect namespace modification: shell.user_ns['key'] = value\nNatural code execution: Code cells share namespace automatically\nExtension loading: Inject at startup via load_ipython_extension()\n\nThe xpush() convenience wrapper is available but many implementations use the underlying mechanisms directly. The actual solveit backend (which orchestrates dialogs, AI interactions, and message execution) likely uses xpush() more extensively, but that code isn’t in the open-source libraries we explored.\nArchitecture: How solveit Uses push()\n┌─────────────────────────────────────────────────────────────────────┐\n│                         USER ACTION                                 │\n│   • Runs a tool                                                     │\n│   • Imports a gist                                                  │\n│   • AI generates code                                               │\n│   • Loads dialog state                                              │\n└────────────────────────────┬────────────────────────────────────────┘\n                             │\n                             ▼\n┌─────────────────────────────────────────────────────────────────────┐\n│                    SOLVEIT BACKEND                                  │\n│   • Executes code in isolated namespace                             │\n│   • Captures results                                                │\n│   • Determines what to share with user                              │\n└────────────────────────────┬────────────────────────────────────────┘\n                             │\n                             ▼\n┌─────────────────────────────────────────────────────────────────────┐\n│                    shell.xpush()                                    │\n│   • Injects variables into user namespace                           │\n│   • Makes results accessible in subsequent cells                    │\n└────────────────────────────┬────────────────────────────────────────┘\n                             │\n                             ▼\n┌─────────────────────────────────────────────────────────────────────┐\n│                    USER NAMESPACE                                   │\n│   Variables now available:                                          │\n│   • print(tool_result)                                              │\n│   • df.head()                                                       │\n│   • my_imported_function()                                          │\n└─────────────────────────────────────────────────────────────────────┘\nExample: Complete Tool Execution Flow\n# 1. User shares a tool with AI\ndef fetch_data(url: str) -&gt; dict:\n    \"\"\"Fetch JSON from URL\"\"\"\n    import httpx\n    return httpx.get(url).json()\n\n# 2. AI decides to call it\n# Behind the scenes in solveit:\ntool_name = \"fetch_data\"\ntool_args = {\"url\": \"https://api.github.com/users/torvalds\"}\n\n# 3. Execute tool\nfrom toolslm.funccall import call_func\nresult = call_func(tool_name, tool_args, namespace={'fetch_data': fetch_data})\n\n# 4. Inject result into user namespace\nshell = get_ipython()\nshell.xpush(\n    github_data=result,  # Make result accessible\n    interactive=False     # Silent injection\n)\n\n# 5. User can now use it:\nprint(github_data['name'])  # 'Linus Torvalds'\nWhy Not Just Execute Directly?\nYou might wonder: why use push() instead of just executing code directly?\nProblems: - Harder to control scope - Difficult to inject complex objects - No clean separation between execution and namespace injection - Can’t easily inject from external sources\nBenefits of using push: - Clean API for namespace manipulation - Works with any Python object (including unpicklable ones) - Respects IPython’s namespace management - Can control interactive behavior - Integrates with IPython’s hooks and events\nSummary\n\n\n\nFeature\nPurpose in solveit\n\n\n\n\nTool results\nMake AI tool outputs available to user\n\n\nGist imports\nInject functions from GitHub gists\n\n\nAI code execution\nShare variables from AI-generated code\n\n\nSession restoration\nReload saved dialog state\n\n\nHelper injection\nPre-load utility functions\n\n\nContext sharing\nPass data between messages/cells\n\n\n\n\n\n\n\n\n\n\n\n\nUse Case\nLibrary\nFile\nFunction/Method\n\n\n\n\n1. Tool Results\n(custom solveit)\nN/A\nNot in open source libs\n\n\n2. Gist Imports\ndialoghelper\ncore.py\nimport_gist()\n\n\n3. AI-Generated Vars\nexecnb / toolslm\nshell.py\nCaptureShell.run() / get_shell()\n\n\n4. Message Variables\ndialoghelper\ncore.py\nrun_msg(), add_msg()\n\n\n5. Helper Pre-loading\nipykernel_helper\ncore.py\nload_ipython_extension()\n\n\n6. Session Restoration\n(custom solveit)\nN/A\nNot in open source libs\n\n\n\nKey insight: push() / xpush() is the bridge between solveit’s backend execution and the user’s interactive namespace. It’s how results from AI actions become available for the user to work with.\n\n\nSend commands and display data to the Javascript frontend\nInteractiveShell.run_cmd(cmd, data='', meta=None, update=False, **kw)\nPurpose: a convenience wrapper that sends a command to the frontend via the transient mechanism.\nInternally using: InteractiveShell.transient().\nIn Jupyter, when you output something, it gets sent to the frontend via a display_data or execute_result message. These messages have three main parts:\n{\n    \"data\": {\"text/plain\": \"Hello\", \"text/html\": \"&lt;b&gt;Hello&lt;/b&gt;\"},  # The content\n    \"metadata\": {},  # Extra info about the content\n    \"transient\": {}  # Data that should NOT be persisted in the notebook\n}\nSends display data to the frontend where the actual payload is in the transient field, not the main data field.\nThe key insight: transient data is displayed but not saved when the notebook is saved. It’s ephemeral.\nWhy use transient? - Not saved to notebook - Commands and temporary UI updates don’t clutter the saved .ipynb file - Custom frontend communication - The solveit frontend watches for specific transient keys - Ephemeral state - Progress indicators, status updates, commands that shouldn’t persist\nExample usage: run_cmd(\"scroll_to\", msg_id=\"abc123\")\nInteractiveShell.publish(self:InteractiveShell, data='', subtype='plain', mimetype='text', meta=None, update=False, **kw)\n\nInteractiveShell.transient(data='', subtype='plain', mimetype='text', meta=None, update=False, **kw)\nPurpose: A flexible method to publish display data to the frontend, with support for transient data.\nParameters: - data: Content to display (string, DisplayObject, or dict) - subtype: MIME subtype (default: ‘plain’) - mimetype: MIME type (default: ‘text’) → combined as text/plain - meta: Metadata dictionary - update: If True, updates a previous display with same -display_id - **kw: Extra kwargs go into the transient field\nHow it works: - If data is a DisplayObject (like HTML, Markdown), it formats it properly - If data is not a dict/mapping, it wraps it as {mimetype/subtype: data} - Publishes via display_pub.publish() with transient data in **kw\nExamples:\nshell = get_ipython()\n\n# Publish plain text\nshell.publish(\"Hello, world!\")\n\n# Publish HTML\nshell.publish(\"&lt;b&gt;Bold text&lt;/b&gt;\", subtype='html')\n\n# Publish with transient data (for frontend commands)\nshell.publish(\"status\", cmd=\"update_status\", id=\"123\")\n\n# Publish a DisplayObject\nfrom IPython.display import HTML\nshell.publish(HTML(\"&lt;h1&gt;Title&lt;/h1&gt;\"))\n\n# Update an existing display\nshell.publish(\"Updated content\", update=True, display_id=\"my_display\")\npublish() is the lower-level method that transient() essentially wraps.\n\n\n\nWeb access\nI have access to two main tools:\n\nWeb Search - I can search for current information, recent events, technical documentation, news, and anything where up-to-date data would be helpful. I use this when:\n\n\nYou need recent information (after my March 2025 knowledge cutoff)\nYou’re looking for specific facts or current conditions\nReal-time data is important (weather, news, stock prices, etc.)\n\n\nRead URL - I can fetch and read content from specific web pages you provide, which is useful for analyzing articles, documentation, or other online content.\n\nI use these tools strategically - I don’t search for things I already know well (like general programming concepts, historical facts, or established knowledge), but I will search when current or specific information would genuinely help answer your question.\n\nModel inference service\nThe web_search function isn’t defined in your Python environment - it’s a tool that’s available to me (the AI assistant) but not directly accessible to you in your Python code.\nWhen I use web_search, I’m calling it through my own tool-calling mechanism, not through your Python interpreter. It’s part of my capabilities, separate from the Python environment you’re working in.\nweb_search(query: str)\nParameters - Takes a search query string and returns web search results\n\n\nipykernel_helper\nFull-featured URL reader that can extract sections, convert to markdown, handle math (KaTeX/MathJax), absolutify image URLs, and optionally tag images for AI processing\nread_url(\n    url: str,\n    as_md: bool = True,\n    extract_section: bool = True,\n    selector: str = None\n)\nParameters - url: The URL to read (required) - as_md: Whether to convert HTML to Markdown (default: True) - extract_section: If URL has an anchor, return only that section (default: True) - selector: CSS selector to extract specific sections using BeautifulSoup.select syntax (optional)\nUses internally - scrape_url() - Fetch URL content using cloudscraper (handles anti-bot protections) - get_md() - Convert HTML to clean markdown",
    "crumbs": [
      "Answerai tools study"
    ]
  },
  {
    "objectID": "notebook.html",
    "href": "notebook.html",
    "title": "wordslab-notebooks-lib.notebook",
    "section": "",
    "text": "A Jupyter notebook is a convenient way to build context for a LLM one cell after the other: you are working in a fully editable conversation, while interacting with AI and with code.\nJeremy Howard and his team at Answer.ai explored how to work efficiently in this kind of conversation: they developed a method and platform called Solveit.\nhttps://solve.it.com/\nWe would like to replicate this approach to working with AI in a wordslab notebooks environment.\nHere is how we chose to do it: - in a jupyterlab notebook, there are two types of cells: markdown and code - we want to simulate a third type of cell: a “prompt” cell - the content of this cell is a prompt (text in markdown format) which is sent to an llm when the cell is executed, along with the text of all the cells situated above in the notebook (context) - the llm response is streamed just below and formatted as markdown.\nTo simulate this “prompt” cell we need to develop a Jupyterlab frontend extension which implements the following behaviors : - three buttons are added to the cell toolbar: “note”, “prompt”, “code” - a click on one of these buttons changes the type of the cell - “note” selects a classic markdown cell - “prompt” selects a code cell, modified with the special “prompt behavior” defined below - “code” selects a classic code cell - a “prompt” cell is distinguished from a regular code cell by a metadata property registered in the ipynb file - each cell type is visualized by a specific color in the left border of the cell - “note” cell has a green border - “prompt” cell a red border - “code” cell has a blue border - the “prompt” cell is a code cell with the specific modified behaviors - the code syntax highlighting is replaced by markdown syntax highlighting when the user types text in this cell - when the user executes this cell, the frontend extension does the following - calls the kernel to inject the following variables - __notebook_path with the path and name of the notebook in the workspace - __notebook_content with the full json representation of the notebook - __cell_id with the id of the current cell - then calls the kernel to execute a specific chat(message) python function - where the message parameter is the content of the cell - and the content of the notebook above the current cell is inluded as context - the python chat() function streams the response tokens from the llm to the output section of the code cell, with markdown rendering\nSee the section “Develop a Jupyterlab frontend extension” at the bottom of this page to understand how the extension was developed.",
    "crumbs": [
      "wordslab-notebooks-lib.notebook"
    ]
  },
  {
    "objectID": "notebook.html#work-with-ai-in-a-jupyterlab-notebook---the-solveit-method",
    "href": "notebook.html#work-with-ai-in-a-jupyterlab-notebook---the-solveit-method",
    "title": "wordslab-notebooks-lib.notebook",
    "section": "",
    "text": "A Jupyter notebook is a convenient way to build context for a LLM one cell after the other: you are working in a fully editable conversation, while interacting with AI and with code.\nJeremy Howard and his team at Answer.ai explored how to work efficiently in this kind of conversation: they developed a method and platform called Solveit.\nhttps://solve.it.com/\nWe would like to replicate this approach to working with AI in a wordslab notebooks environment.\nHere is how we chose to do it: - in a jupyterlab notebook, there are two types of cells: markdown and code - we want to simulate a third type of cell: a “prompt” cell - the content of this cell is a prompt (text in markdown format) which is sent to an llm when the cell is executed, along with the text of all the cells situated above in the notebook (context) - the llm response is streamed just below and formatted as markdown.\nTo simulate this “prompt” cell we need to develop a Jupyterlab frontend extension which implements the following behaviors : - three buttons are added to the cell toolbar: “note”, “prompt”, “code” - a click on one of these buttons changes the type of the cell - “note” selects a classic markdown cell - “prompt” selects a code cell, modified with the special “prompt behavior” defined below - “code” selects a classic code cell - a “prompt” cell is distinguished from a regular code cell by a metadata property registered in the ipynb file - each cell type is visualized by a specific color in the left border of the cell - “note” cell has a green border - “prompt” cell a red border - “code” cell has a blue border - the “prompt” cell is a code cell with the specific modified behaviors - the code syntax highlighting is replaced by markdown syntax highlighting when the user types text in this cell - when the user executes this cell, the frontend extension does the following - calls the kernel to inject the following variables - __notebook_path with the path and name of the notebook in the workspace - __notebook_content with the full json representation of the notebook - __cell_id with the id of the current cell - then calls the kernel to execute a specific chat(message) python function - where the message parameter is the content of the cell - and the content of the notebook above the current cell is inluded as context - the python chat() function streams the response tokens from the llm to the output section of the code cell, with markdown rendering\nSee the section “Develop a Jupyterlab frontend extension” at the bottom of this page to understand how the extension was developed.",
    "crumbs": [
      "wordslab-notebooks-lib.notebook"
    ]
  },
  {
    "objectID": "notebook.html#install-the-jupyterlab-extension---wordslab-notebooks-lib",
    "href": "notebook.html#install-the-jupyterlab-extension---wordslab-notebooks-lib",
    "title": "wordslab-notebooks-lib.notebook",
    "section": "Install the Jupyterlab extension - wordslab-notebooks-lib",
    "text": "Install the Jupyterlab extension - wordslab-notebooks-lib\nIf you want to use “prompt” cells, you will first need to install the Jupyterlab frontend extension: - activate your Jupyterlab python virtual environment - pip install wordslab-notebooks-lib - restart your Jupyterlab server\nThe extension is already pre-installed in the wordslab-notebooks environment.\nTo be clear: the wordslab-notebooks-lib package contains both: the Javascript Jupyterlab frontend extension AND the python library wich is loaded in the python kernel.\nThe Jupyterlab frontend extension is reloaded and re-initialized each time you refresh your browser page: - to check is the extension is installed and running, look at the browser console and llok for the message ‘Wordslab notebooks extension vx.y.z activated’ - hit the refresh button if you encounter a bug and the extension stops working",
    "crumbs": [
      "wordslab-notebooks-lib.notebook"
    ]
  },
  {
    "objectID": "notebook.html#extend-the-ipython-kernel-with-useful-utilities",
    "href": "notebook.html#extend-the-ipython-kernel-with-useful-utilities",
    "title": "wordslab-notebooks-lib.notebook",
    "section": "Extend the ipython kernel with useful utilities",
    "text": "Extend the ipython kernel with useful utilities\nYou also need to install the wordslab-notebooks-lib library in the virtual environnement used by the ipython kernel which runs each notebook in which you want to use the Solveit method.\nIt is the client of the Jupyterlab extension, and provides many utilties and tools which support this new way of working with AI.\nThe main python object used to interact with an ipython kernel is the InteractiveShell. You get an instance of it with the get_ipython() method.\n\nshell = get_ipython()\ntype(shell)\n\nipykernel.zmqshell.ZMQInteractiveShell\n\n\nWe will start by extending this shell with capabilities useful to work in a notebook with the Solveit method. These extensions are inspired by the library ipykernel_helper from Answer.ai. As of december 2025, this library is not open source, but it is available to users in the solve.it.com environment and is a dependency of other Apache 2.0 libraries, so I think it is OK to use it as an inspiration.\n\nescape??\n\ndef escape(s, quote=True):\n    \"\"\"\n    Replace special characters \"&\", \"&lt;\" and \"&gt;\" to HTML-safe sequences.\n    If the optional flag quote is true (the default), the quotation mark\n    characters, both double quote (\") and single quote (') characters are also\n    translated.\n    \"\"\"\n    s = s.replace(\"&\", \"&amp;\") # Must be done first!\n    s = s.replace(\"&lt;\", \"&lt;\")\n    s = s.replace(\"&gt;\", \"&gt;\")\n    if quote:\n        s = s.replace('\"', \"&quot;\")\n        s = s.replace('\\'', \"&#x27;\")\n    return s\nFile: /home/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/html/__init__.py\n\n\n\n\nInteractiveShell.user_items\n\ndef user_items(\n    max_str_len:int=200, xtra_ignore:tuple=()\n):\n\nGet an overview of the variables & functions defined by the user so far in the notebook. The value addded by this function is to filter out all internal ipython and wordslab variables. Returns a tuple of dictionaries (user_variables, user_functions): - the keys are the variables or function names - the value is a truncated string representation of the variable value or the function signature The max_str_len parameter is used to truncate the string representation of the variables. The xtra_ignore parameter is used to hide additional names from the result.\n\nvariables, functions = shell.user_items()\nvariables, functions\n\n({'Code': \"functools.partial(&lt;function ft&gt;, 'code', void_=False)\",\n  'Source': \"functools.partial(&lt;function ft&gt;, 'source', void_=True)\",\n  'shell': '&lt;ipykernel.zmqshell.ZMQInteractiveShell object&gt;',\n  'user_items': 'None'},\n {'_safe_str': '(obj, max_str_len=200)'})\n\n\n\n\n\nInteractiveShell.get_variables_values\n\ndef get_variables_values(\n    var_names:list, literal:bool=True\n):\n\nGet a safe and serializable representation of variables values from the user namespace. This method preserves real Python values when they are safe literals, otherwise it falls back to strings. You can call it in two modes: - literal = True : Preserve actual Python values when safe, best for internal tools - literal = False : Force everything to strings, best for logging / UI display / debug output\n\nshell.get_variables_values(var_names=[\"variables\", \"functions\"])\n\n{'variables': {'Code': \"functools.partial(&lt;function ft&gt;, 'code', void_=False)\",\n  'Source': \"functools.partial(&lt;function ft&gt;, 'source', void_=True)\",\n  'shell': '&lt;ipykernel.zmqshell.ZMQInteractiveShell object&gt;',\n  'user_items': 'None'},\n 'functions': {'_safe_str': '(obj, max_str_len=200)'}}\n\n\n\n\n\nInteractiveShell.get_tools_schemas_and_functions\n\ndef get_tools_schemas_and_functions(\n    func_names:list\n):\n\nGet a json schema and a function object for the functions defined in the user namespace which can be used as tools.\n\n# Example tool definition\ndef example_sum(\n    a: int,  # First thing to sum\n    b: int = 1,  # Second thing to sum\n) -&gt; int:  # The sum of the inputs\n    \"Adds a + b.\"\n    return a + b\n\n\nshell.get_tools_schemas_and_functions([\"example_sum\"])\n\n{'example_sum': ({'type': 'function',\n   'function': {'name': 'example_sum',\n    'description': 'Adds a + b.\\n\\nReturns:\\n- type: integer',\n    'parameters': {'type': 'object',\n     'properties': {'a': {'type': 'integer',\n       'description': 'First thing to sum'},\n      'b': {'type': 'integer',\n       'description': 'Second thing to sum',\n       'default': 1}},\n     'required': ['a']}}},\n  &lt;function __main__.example_sum(a: int, b: int = 1) -&gt; int&gt;)}",
    "crumbs": [
      "wordslab-notebooks-lib.notebook"
    ]
  },
  {
    "objectID": "notebook.html#access-the-notebook-path-cells-content-and-current-cell-id",
    "href": "notebook.html#access-the-notebook-path-cells-content-and-current-cell-id",
    "title": "wordslab-notebooks-lib.notebook",
    "section": "Access the notebook path, cells content and current cell id",
    "text": "Access the notebook path, cells content and current cell id\nThe 4 notebook properties below are silently injected by the Jupyterlab frontend extension before each code cell is executed.\nThis will not work if the wordslab-notebooks-lib Jupyterlab extension is not installed.\n\n\nfind_var\n\ndef find_var(\n    var:str\n):\n\nSearch for var in all frames of the call stack\n\n\n\nWordslabNotebook\n\ndef WordslabNotebook(\n    \n):\n\nJupyterlab notebook introspection and metaprogramming.\n\nnotebook = WordslabNotebook()\n\n\nnotebook.jupyterlab_extension_version\n\n'0.0.13'\n\n\n\nnotebook.path\n\n'wordslab-notebooks-lib/nbs/04_notebook.ipynb'\n\n\n\nnotebook.content.metadata\n\n{'kernelspec': {'display_name': 'wordslab-notebooks-lib',\n  'language': 'python',\n  'name': 'wordslab-notebooks-lib'},\n 'language_info': {'codemirror_mode': {'name': 'ipython', 'version': 3},\n  'file_extension': '.py',\n  'mimetype': 'text/x-python',\n  'name': 'python',\n  'nbconvert_exporter': 'python',\n  'pygments_lexer': 'ipython3',\n  'version': '3.12.12'}}\n\n\n\nnotebook.cell_id\n\n'd16ad869-d651-40bd-af2c-623d82b4edf0'\n\n\n\nnotebook.cell_id\n\n'59347d6e-d1f7-4d23-b041-143e42887f6d'",
    "crumbs": [
      "wordslab-notebooks-lib.notebook"
    ]
  },
  {
    "objectID": "notebook.html#create-update-delete-and-run-notebook-cells",
    "href": "notebook.html#create-update-delete-and-run-notebook-cells",
    "title": "wordslab-notebooks-lib.notebook",
    "section": "Create, update, delete and run notebook cells",
    "text": "Create, update, delete and run notebook cells\nThese methods can be used to manipulate the notebook cells when the wordslab-notebooks-lib Jupyterlab frontend extension is installed.\nThey are inspired by the library dialoghelper from Answer.ai, but are adapted to the standard Jupyterlab context.\n\n\nWordslabNotebook.add_cell\n\ndef add_cell(\n    content:str, # Content of the cell (i.e the prompt, code, or note cell text)\n    placement:str='add_after', # Can be 'add_after', 'add_before', 'at_start', 'at_end'\n    cell_id:str=None, # id of the cell that placement is relative to (if None, uses current cell)\n    cell_type:str='note', # Cell type, can be 'code', 'note', or 'prompt'\n    notebook_path:str='', # Notebook to update, defaults to current notebook\n):\n\nAdd a cell to the current notebook or any other opened notebook (notebook_path), at the start/end of the notebook or before/after any cell (placementand cell_id), with a cell_type (note|prompt|code) and content (text). Returns the new cell id.\n\nawait notebook.add_cell(\"Test note\")\n\n'a1cc961e-e918-4ac7-9398-7c6fa574f6f2'\n\n\nTest note\n\nawait notebook.add_cell(\"Test note 2\", placement=\"add_after\")\n\n'984dea0c-450c-488f-8115-82e92d56c0ea'\n\n\nTest note 2\nTest note 3\n\nawait notebook.add_cell(\"Test note 3\", placement=\"add_before\")\n\n'1e24f157-66dc-4200-8b12-523c92bb447d'\n\n\n\nawait notebook.add_cell(\"Test note 4\", placement=\"at_start\")\n\n'01d8ebd2-7a4d-479a-939a-d345a9660211'\n\n\n\nawait notebook.add_cell(\"Test note 5\", placement=\"at_end\")\n\n'b135d91b-63c0-4a73-8172-d7e73e3a5cf3'\n\n\n\n# Creates a new cell at the end of the notebook\nawait notebook.add_cell(\"Test note somewhere\", placement=\"somewhere\")\n\n'28690fe7-bf10-4781-8067-432f45d18c7f'\n\n\n\nawait notebook.add_cell(\"Test prompt\", cell_type='prompt')\n\n'c41b364b-74da-4886-a974-1683e8143864'\n\n\n\nTest prompt\n\n\nawait notebook.add_cell(\"Test code\", cell_type='code')\n\n'cd02e818-4989-40a8-90ce-8e92a99bcd59'\n\n\n\nTest code\n\n\n# Create a new cell of type note\nawait notebook.add_cell(\"Test sometype\", cell_type='sometype')\n\n'e80dbc6b-0832-423f-8e33-aacc0ea52039'\n\n\nTest sometype\n\nawait notebook.add_cell(\"Test bad cell id\", cell_id=\"bad_cell_id\")\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[69], line 1\n----&gt; 1 await notebook.add_cell(\"Test bad cell id\", cell_id=\"bad_cell_id\")\n\nCell In[65], line 25, in add_cell(self, content, placement, cell_id, cell_type, notebook_path)\n     23     return result['cell_id']\n     24 elif 'error' in result:\n---&gt; 25     raise RuntimeError(result['error'])\n\nRuntimeError: Cell not found: bad_cell_id\n\n\n\n\nawait notebook.add_cell(\"Test other notebook (bad notebook name)\", placement=\"at_start\", notebook_path=\"wordslab-notebooks-lib/nbs/temp.ipynb\")\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[73], line 1\n----&gt; 1 await notebook.add_cell(\"Test other notebook (bad notebook name)\", placement=\"at_start\", notebook_path=\"wordslab-notebooks-lib/nbs/temp.ipynb\")\n\nCell In[65], line 25, in add_cell(self, content, placement, cell_id, cell_type, notebook_path)\n     23     return result['cell_id']\n     24 elif 'error' in result:\n---&gt; 25     raise RuntimeError(result['error'])\n\nRuntimeError: Notebook not found: wordslab-notebooks-lib/nbs/temp.ipynb. Make sure the notebook is opened in Jupyterlab.\n\n\n\n\nawait notebook.add_cell(\"Test other notebook (open)\", placement=\"at_start\", notebook_path=\"wordslab-notebooks-lib/nbs/test.ipynb\")\n\n'637eb2a9-421c-4a7c-99aa-e2426b9eab64'\n\n\n\nawait notebook.add_cell(\"Test other notebook (closed)\", placement=\"at_start\", notebook_path=\"wordslab-notebooks-lib/nbs/01_env.ipynb\")\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[75], line 1\n----&gt; 1 await notebook.add_cell(\"Test other notebook (closed)\", placement=\"at_start\", notebook_path=\"wordslab-notebooks-lib/nbs/01_env.ipynb\")\n\nCell In[65], line 25, in add_cell(self, content, placement, cell_id, cell_type, notebook_path)\n     23     return result['cell_id']\n     24 elif 'error' in result:\n---&gt; 25     raise RuntimeError(result['error'])\n\nRuntimeError: Notebook not found: wordslab-notebooks-lib/nbs/01_env.ipynb. Make sure the notebook is opened in Jupyterlab.\n\n\n\n\n\n\nWordslabNotebook.update_cell\n\ndef update_cell(\n    cell_id:str=None, # id of the cell to update (if None, uses current cell)\n    content:str=None, # Content of the cell (i.e the prompt, code, or note cell text)\n    notebook_path:str='', # Notebook to update, defaults to current notebook\n):\n\nUpdate the cell identified by cell_id, in the current notebook or any other opened notebook (notebook_path), with a new content. Returns the updated cell id.\n\norig_cell_id = await notebook.add_cell(\"original cell content\")\n\nupdated cell content\n\nawait notebook.update_cell(cell_id=orig_cell_id, content=\"updated cell content\")\n\n'293c77e8-5449-407a-8b4f-0d628c45bfc7'\n\n\n\nawait notebook.update_cell(content=\"no id\")\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[95], line 1\n----&gt; 1 await notebook.update_cell(content=\"no id\")\n\nCell In[94], line 13, in update_cell(self, cell_id, content, notebook_path)\n     11 self._ensure_jupyterlab_extension()\n     12 if not cell_id:\n---&gt; 13      raise ValueError(\"`cell_id` parameter is mandatory\")\n     14 result = await self._comm.send({'action': 'update_cell', 'cell_id': cell_id, 'content': content, 'notebook_path': notebook_path })\n     15 if 'success' in result and result['success']:\n\nValueError: `cell_id` parameter is mandatory\n\n\n\n\nawait notebook.update_cell(cell_id=\"bad_cell_id\", content=\"bad id\")\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[96], line 1\n----&gt; 1 await notebook.update_cell(cell_id=\"bad_cell_id\", content=\"bad id\")\n\nCell In[94], line 18, in update_cell(self, cell_id, content, notebook_path)\n     16     return result['cell_id']\n     17 elif 'error' in result:\n---&gt; 18     raise RuntimeError(result['error'])\n\nRuntimeError: Cell not found: bad_cell_id\n\n\n\n\norig_cell_id = await notebook.add_cell(\"original cell content in other notebook\", placement=\"at_start\", notebook_path=\"wordslab-notebooks-lib/nbs/test.ipynb\")\n\n\nawait notebook.update_cell(cell_id=orig_cell_id, content=\"updated cell content in other notebook\", notebook_path=\"wordslab-notebooks-lib/nbs/test.ipynb\")\n\n'55d9b34d-7e39-4cb6-9c95-eb0b9e0176f3'\n\n\n\n\n\nWordslabNotebook.delete_cell\n\ndef delete_cell(\n    cell_id:str=None, # id of cell to delete\n    notebook_path:str='', # Notebook to update, defaults to current notebook\n):\n\n“Update the cell identified by cell_id, in the current notebook or any other opened notebook (notebook_path). Returns the deleted cell id.\n\nawait notebook.delete_cell(cell_id=\"01d8ebd2-7a4d-479a-939a-d345a9660211\")\n\n'01d8ebd2-7a4d-479a-939a-d345a9660211'\n\n\n\nawait notebook.delete_cell(cell_id=\"55d9b34d-7e39-4cb6-9c95-eb0b9e0176f3\", notebook_path=\"wordslab-notebooks-lib/nbs/test.ipynb\")\n\n'55d9b34d-7e39-4cb6-9c95-eb0b9e0176f3'\n\n\n\n\n\nWordslabNotebook.run_cell\n\ndef run_cell(\n    cell_id:str=None, # id of cell to execute\n):\n\n“Adds the cell identified by cell_id to the run queue, only in the current notebook (jupyterlab ‘run-cell’ command limitation). Returns the cell id. DOES NOT return the result of the execution: the target cell will only be run after the current cell execution finishes. Use the read_cell method later with the same cell_id to get the result of the execution.\n\ncode_cell_id = await notebook.add_cell(cell_type=\"code\", content=\"1+1\")\n\n\n1+1\n\n2\n\n\n\nawait notebook.run_cell(cell_id=code_cell_id)\n\n'2a7bc090-ef99-4a21-bf9e-475d24de51c5'\n\n\n\n\n\nWordslabNotebook.read_cell\n\ndef read_cell(\n    cell_id:str=None, # id of cell to delete\n):\n\n“Read the text content of the cell identified by cell_id, only in the current notebook. Returns the text content of the cell as a single multiline string.\n\nabove_cell_id = \"df396dbb-8ecf-41df-b4cf-1f484dabb0fb\"\n\n\nnotebook.read_cell(above_cell_id)\n\n'#|export\\n@patch\\ndef read_cell(\\n    self: WordslabNotebook,\\n    cell_id: str = None,  # id of cell to delete\\n):\\n    \"\"\"\"Read the text content of the cell identified by `cell_id`, only in the current notebook.\\n    Returns the text content of the cell as a single multiline string.\"\"\"\\n    self._ensure_jupyterlab_extension()\\n    if not cell_id:\\n        raise ValueError(\"`cell_id` parameter is mandatory\")\\n    cell = next((c for c in self.content.cells if c.id == cell_id), None)\\n    if not cell:\\n        raise ValueError(f\"Cell not found: {cell_id}\")\\n    return cell.source'",
    "crumbs": [
      "wordslab-notebooks-lib.notebook"
    ]
  },
  {
    "objectID": "notebook.html#explore-the-notebook-variables-functions-and-objects",
    "href": "notebook.html#explore-the-notebook-variables-functions-and-objects",
    "title": "wordslab-notebooks-lib.notebook",
    "section": "Explore the notebook variables, functions and objects",
    "text": "Explore the notebook variables, functions and objects\n\n\nWordslabNotebook.show_variables_and_functions\n\ndef show_variables_and_functions(\n    \n):\n\nDisplay the variables and functions defined by the user so far in the notebook.\n\nnotebook.show_variables_and_functions()\n\nVariables\n\n\n\nName\nValue\n\n\nCode\nfunctools.partial(&lt;function ft at 0x7b36600a9580&gt;, 'code', void_=False)\n\n\nSource\nfunctools.partial(&lt;function ft at 0x7b36600a9580&gt;, 'source', void_=True)\n\n\nshell\n&lt;ipykernel.zmqshell.ZMQInteractiveShell object at 0x7b3670078680&gt;\n\n\nuser_items\nNone\n\n\nvariables\n{'Code': \"functools.partial(&lt;function ft at 0x7b36600a9580&gt;, 'code', void_=False)\", 'Source': \"functools.partial(&lt;function ft at 0x7b36600a9580&gt;, 'source', void_=True)\", 'shell': '&lt;ipykernel.zmqshell.…\n\n\nfunctions\n{'_safe_str': '(obj, max_str_len=200)'}\n\n\nget_variables_values\nNone\n\n\nget_tools_schemas_and_functions\nNone\n\n\nnotebook\n&lt;__main__.WordslabNotebook object at 0x7b3640f956a0&gt;\n\n\nadd_cell\nNone\n\n\nupdate_cell\nNone\n\n\ndelete_cell\nNone\n\n\nrun_cell\nNone\n\n\nread_cell\nNone\n\n\nshow_variables_and_functions\nNone\n\n\n\nFunctions\n\n\n\nName\nSignature\n\n\n_safe_str\n(obj, max_str_len=200)\n\n\n_get_schema\n(ns: dict, t)\n\n\nexample_sum\n(a: int, b: int = 1) -&gt; int\n\n\n_find_frame_dict\n(var: str)\n\n\nfind_var\n(var: str)\n\n\n\n\n\n\n\n\nWordslabNotebook.show_object_members\n\ndef show_object_members(\n    obj\n):\n\nDisplay the attributes and methods of a given python object\n\nnotebook.show_object_members(notebook)\n\nObject of type: WordslabNotebookJupyterlab notebook introspection and metaprogramming.Attributes\n\n\n\nName\nType\nValue\nDoc\n\n\ncell_id\nstr\nb602a192-b7e7-4976-a26d-4fa1854879b4\nUnique ID of the current notebook cell, useful to locate the current cell in the full notebook content\n\n\nchat_model\nstr\nqwen3:30b\n\n\n\nchat_thinking\nbool\nTrue\n\n\n\ncontent\nNotebookNode\n{'metadata': {'kernelspec': {'display_name': 'wordslab-notebooks-lib', 'language': 'python', 'name': 'wordslab-notebooks-lib'}, 'language_info': {'codemirror_mode': {'name': 'ipython', 'version': 3}, …\nFull content of the notebook returned as a NotebookNode object from the nbformat library\n\n\njupyterlab_extension_installed\nbool\nTrue\n\n\n\njupyterlab_extension_version\nstr\n0.0.13\nwordslab-notebooks-lib version number injected by the Jupyterlab frontend extension\n\n\npath\nstr\nwordslab-notebooks-lib/nbs/04_notebook.ipynb\nRelative path of the notebook .ipynb file in the notebook workspace\n\n\n\nMethods\n\n\n\nName\nSignatue\nType\nDoc\n\n\nJupyterlabExtensionComm\n(target_name='wordslab_notebooks', timeout=2.0)\ninstance method\n\n\n\nadd_cell\n(content: str, placement: str = 'add_after', cell_id: str = None, cell_type: str = 'note', notebook_path: str = '')\ninstance method\nAdd a cell to the current notebook or any other opened notebook (`notebook_path`), at the start/end of the notebook or before/after any cell (`placement`and `cell_id`), with a `cell_type` (note|prompt…\n\n\ndelete_cell\n(cell_id: str = None, notebook_path: str = '')\ninstance method\n\"Update the cell identified by `cell_id`, in the current notebook or any other opened notebook (`notebook_path`). Returns the deleted cell id.\n\n\nread_cell\n(cell_id: str = None)\ninstance method\n\"Read the text content of the cell identified by `cell_id`, only in the current notebook. Returns the text content of the cell as a single multiline string.\n\n\nrun_cell\n(cell_id: str = None)\ninstance method\n\"Adds the cell identified by `cell_id` to the run queue, only in the current notebook (jupyterlab 'run-cell' command limitation). Returns the cell id. DOES NOT return the result of the execution: the …\n\n\nshow_object_members\n(obj)\ninstance method\nDisplay the attributes and methods of a given python object\n\n\nshow_variables_and_functions\n()\ninstance method\nDisplay the variables and functions defined by the user so far in the notebook.\n\n\nupdate_cell\n(cell_id: str = None, content: str = None, notebook_path: str = '')\ninstance method\nUpdate the cell identified by `cell_id`, in the current notebook or any other opened notebook (`notebook_path`), with a new `content`. Returns the updated cell id.",
    "crumbs": [
      "wordslab-notebooks-lib.notebook"
    ]
  },
  {
    "objectID": "notebook.html#get-variable-values-and-tool-schemas-referenced-in-prompt-cells",
    "href": "notebook.html#get-variable-values-and-tool-schemas-referenced-in-prompt-cells",
    "title": "wordslab-notebooks-lib.notebook",
    "section": "Get $variable values and &tool schemas referenced in prompt cells",
    "text": "Get $variable values and &tool schemas referenced in prompt cells\n\n\nWordslabNotebook.get_tools_schemas_and_functions\n\ndef get_tools_schemas_and_functions(\n    func_names:list\n):\n\nGet a json schema of functions which can be used as tools.\n\n\n\nWordslabNotebook.get_variables_values\n\ndef get_variables_values(\n    var_names:list\n):\n\nGet a safe and serializable representation of variables values.\n\nnotebook.get_variables_values(var_names=[\"variables\", \"functions\"])\n\n{'variables': {'Code': \"functools.partial(&lt;function ft&gt;, 'code', void_=False)\",\n  'Source': \"functools.partial(&lt;function ft&gt;, 'source', void_=True)\",\n  'shell': '&lt;ipykernel.zmqshell.ZMQInteractiveShell object&gt;',\n  'user_items': 'None'},\n 'functions': {'_safe_str': '(obj, max_str_len=200)'}}\n\n\n\nnotebook.get_tools_schemas_and_functions([\"example_sum\"])\n\n{'example_sum': ({'type': 'function',\n   'function': {'name': 'example_sum',\n    'description': 'Adds a + b.\\n\\nReturns:\\n- type: integer',\n    'parameters': {'type': 'object',\n     'properties': {'a': {'type': 'integer',\n       'description': 'First thing to sum'},\n      'b': {'type': 'integer',\n       'description': 'Second thing to sum',\n       'default': 1}},\n     'required': ['a']}}},\n  &lt;function __main__.example_sum(a: int, b: int = 1) -&gt; int&gt;)}",
    "crumbs": [
      "wordslab-notebooks-lib.notebook"
    ]
  },
  {
    "objectID": "notebook.html#collect-the-notebook-context-for-llm-calls",
    "href": "notebook.html#collect-the-notebook-context-for-llm-calls",
    "title": "wordslab-notebooks-lib.notebook",
    "section": "Collect the notebook context for LLM calls",
    "text": "Collect the notebook context for LLM calls\nThe notebook cells format is documented here:\nhttps://nbformat.readthedocs.io/en/latest/format_description.html\nCode cell outputs are a list, where each item has an output_type. The main types are:\n\nstream — stdout/stderr text (e.g., from print())\n\nHas name (stdout/stderr) and text fields\n\nexecute_result — the return value of the last expression\n\nHas data dict with MIME types like text/plain, text/html, image/png\n\ndisplay_data — from display() calls\n\nSame data dict structure as execute_result\n\nerror — exceptions\n\nHas ename, evalue, and traceback fields\nThe tricky part is that execute_result and display_data can contain multiple representations of the same data (e.g., a pandas DataFrame might have both text/plain and text/html versions).\n\nHere is an example of “note” cell\n\n{'id': 'eb560f48-42a2-4573-bf12-b3edb40bff20',\n 'cell_type': 'markdown',\n 'source': 'Code cell outputs in nbformat are a list, where each item has an output_type. The main types are:\\n\\n- stream — stdout/stderr text (e.g., from print())\\n\\nHas name (stdout/stderr) and text fields\\n\\n- execute_result — the return value of the last expression\\n\\nHas data dict with MIME types like text/plain, text/html, image/png\\n\\n- display_data — from display() calls\\n\\nSame data dict structure as execute_result\\n\\n- error — exceptions\\n\\nHas ename, evalue, and traceback fields\\n\\nThe tricky part is that execute_result and display_data can contain multiple representations of the same data (e.g., a pandas DataFrame might have both text/plain and text/html versions).',\n 'metadata': {}}\n\nHere is an example of “prompt” cell\n\n{'id': '3d5a241d-890c-46db-acf5-d92886f9a77d',\n 'cell_type': 'code',\n 'source': '# This is an example of prompt\\nprint(\"and this is an example of answer\")',\n 'metadata': {'trusted': True,\n  'wordslab_cell_type': 'prompt',\n  'execution': {'iopub.status.busy': '2025-12-29T15:47:42.347364Z',\n   'iopub.execute_input': '2025-12-29T15:47:42.347549Z',\n   'iopub.status.idle': '2025-12-29T15:47:42.350815Z',\n   'shell.execute_reply.started': '2025-12-29T15:47:42.347534Z',\n   'shell.execute_reply': '2025-12-29T15:47:42.349884Z'}},\n 'outputs': [{'name': 'stdout',\n   'output_type': 'stream',\n   'text': 'and this is an example of answer\\n'}],\n 'execution_count': 248}\n\nHere is an example of code cell\n\nThis code\nimport sys\nfrom IPython.display import display, HTML, Markdown\n\n# stream (stdout)\nprint(\"This is stdout\")\n\n# stream (stderr)\nprint(\"This is stderr\", file=sys.stderr)\n\n# display_data (multiple formats)\ndisplay(HTML(\"&lt;b&gt;Bold HTML&lt;/b&gt;\"))\ndisplay(Markdown(\"**Bold Markdown**\"))\n\n# execute_result (last expression)\n{\"key\": \"value\", \"number\": 42}\nProduces these outputs\n[{'name': 'stdout', 'output_type': 'stream', 'text': 'This is stdout\\n'},\n {'name': 'stderr', 'output_type': 'stream', 'text': 'This is stderr\\n'},\n {'output_type': 'display_data',\n  'data': {'text/plain': '&lt;IPython.core.display.HTML object&gt;',\n   'text/html': '&lt;b&gt;Bold HTML&lt;/b&gt;'},\n  'metadata': {}},\n {'output_type': 'display_data',\n  'data': {'text/plain': '&lt;IPython.core.display.Markdown object&gt;',\n   'text/markdown': '**Bold Markdown**'},\n  'metadata': {}},\n {'execution_count': 223,\n  'output_type': 'execute_result',\n  'data': {'text/plain': \"{'key': 'value', 'number': 42}\"},\n  'metadata': {}},\n {'traceback': ['\\x1b[31m---------------------------------------------------------------------------\\x1b[39m',\n   '\\x1b[31mValueError\\x1b[39m                                Traceback (most recent call last)',\n   '\\x1b[36mCell\\x1b[39m\\x1b[36m \\x1b[39m\\x1b[32mIn[224]\\x1b[39m\\x1b[32m, line 1\\x1b[39m\\n\\x1b[32m----&gt; \\x1b[39m\\x1b[32m1\\x1b[39m \\x1b[38;5;28;01mraise\\x1b[39;00m \\x1b[38;5;167;01mValueError\\x1b[39;00m(\\x1b[33m\"\\x1b[39m\\x1b[33mExample error message\\x1b[39m\\x1b[33m\"\\x1b[39m)\\n',\n   '\\x1b[31mValueError\\x1b[39m: Example error message'],\n  'ename': 'ValueError',\n  'evalue': 'Example error message',\n  'output_type': 'error'}]\nIn this code cell\n{'id': 'a1d9fbe2-9a84-4d7d-9415-a2e4693ba7ac',\n 'cell_type': 'code',\n 'source': 'import sys\\nfrom IPython.display import display, HTML, Markdown\\n\\n# stream (stdout)\\nprint(\"This is stdout\")\\n\\n# stream (stderr)\\nprint(\"This is stderr\", file=sys.stderr)\\n\\n# display_data (multiple formats)\\ndisplay(HTML(\"&lt;b&gt;Bold HTML&lt;/b&gt;\"))\\ndisplay(Markdown(\"**Bold Markdown**\"))\\n\\n# execute_result (last expression)\\n{\"key\": \"value\", \"number\": 42}',\n 'metadata': {'trusted': True,\n  'execution': {'iopub.status.busy': '2025-12-29T15:07:25.670149Z',\n   'iopub.execute_input': '2025-12-29T15:07:25.670496Z',\n   'iopub.status.idle': '2025-12-29T15:07:25.678474Z',\n   'shell.execute_reply.started': '2025-12-29T15:07:25.670471Z',\n   'shell.execute_reply': '2025-12-29T15:07:25.677678Z'}},\n 'outputs': [...],\n 'execution_count': 223}\nThe following methods are inspired by the library toolslm from Answer.ai, but are adapted to our specific goal with prompt cells.\nTest\n[to_xml(_cell_output_to_xml(o)) for o in example_output]\nResult\n['&lt;out type=\"stdout\"&gt;This is stdout\\n&lt;/out&gt;',\n '&lt;out type=\"stderr\"&gt;This is stderr\\n&lt;/out&gt;',\n '&lt;out type=\"html\"&gt;&lt;b&gt;Bold HTML&lt;/b&gt;&lt;/out&gt;',\n '&lt;out type=\"markdown\"&gt;**Bold Markdown**&lt;/out&gt;',\n '&lt;out type=\"text\"&gt;{\\'key\\': \\'value\\', \\'number\\': 42}&lt;/out&gt;',\n '&lt;out type=\"error\"&gt;ValueError: Example error message&lt;/out&gt;']\nTest\nto_xml(_cell_to_xml(note_cell))\nResult\n'&lt;note&gt;Code cell outputs in nbformat are a list, where each item has an output_type. The main types are:\\n\\n- stream — stdout/stderr text (e.g., from print())\\n\\nHas name (stdout/stderr) and text fields\\n\\n- execute_result — the return value of the last expression\\n\\nHas data dict with MIME types like text/plain, text/html, image/png\\n\\n- display_data — from display() calls\\n\\nSame data dict structure as execute_result\\n\\n- error — exceptions\\n\\nHas ename, evalue, and traceback fields\\n\\nThe tricky part is that execute_result and display_data can contain multiple representations of the same data (e.g., a pandas DataFrame might have both text/plain and text/html versions).&lt;/note&gt;'\nTest\nto_xml(_cell_to_xml(prompt_cell))\nResult\n'&lt;prompt&gt;&lt;user&gt;# This is an example of prompt\\nprint(\"and this is an example of answer\")&lt;/user&gt;&lt;assistant&gt;&lt;out type=\"stdout\"&gt;and this is an example of answer\\n&lt;/out&gt;&lt;/assistant&gt;&lt;/prompt&gt;'\nTest\nto_xml(_cells_to_notebook_xml([note_cell, prompt_cell, code_cell]))\nResult\n'&lt;note&gt;Code cell outputs in nbformat are a list, where each item has an output_type. The main types are:\\n\\n- stream — stdout/stderr text (e.g., from print())\\n\\nHas name (stdout/stderr) and text fields\\n\\n- execute_result — the return value of the last expression\\n\\nHas data dict with MIME types like text/plain, text/html, image/png\\n\\n- display_data — from display() calls\\n\\nSame data dict structure as execute_result\\n\\n- error — exceptions\\n\\nHas ename, evalue, and traceback fields\\n\\nThe tricky part is that execute_result and display_data can contain multiple representations of the same data (e.g., a pandas DataFrame might have both text/plain and text/html versions).&lt;/note&gt;&lt;prompt&gt;&lt;user&gt;# This is an example of prompt\\nprint(\"and this is an example of answer\")&lt;/user&gt;&lt;assistant&gt;&lt;out type=\"stdout\"&gt;and this is an example of answer\\n&lt;/out&gt;&lt;/assistant&gt;&lt;/prompt&gt;&lt;code&gt;&lt;source&gt;import sys\\nfrom IPython.display import display, HTML, Markdown\\n\\n# stream (stdout)\\nprint(\"This is stdout\")\\n\\n# stream (stderr)\\nprint(\"This is stderr\", file=sys.stderr)\\n\\n# display_data (multiple formats)\\ndisplay(HTML(\"&lt;b&gt;Bold HTML&lt;/b&gt;\"))\\ndisplay(Markdown(\"**Bold Markdown**\"))\\n\\n# execute_result (last expression)\\n{\"key\": \"value\", \"number\": 42}&lt;outputs&gt;&lt;out type=\"stdout\"&gt;This is stdout\\n&lt;/out&gt;&lt;out type=\"stderr\"&gt;This is stderr\\n&lt;/out&gt;&lt;out type=\"html\"&gt;&lt;b&gt;Bold HTML&lt;/b&gt;&lt;/out&gt;&lt;out type=\"markdown\"&gt;**Bold Markdown**&lt;/out&gt;&lt;out type=\"text\"&gt;{\\'key\\': \\'value\\', \\'number\\': 42}&lt;/out&gt;&lt;/outputs&gt;&lt;/code&gt;'\n\n\nWordslabNotebook.get_context_for_llm\n\ndef get_context_for_llm(\n    \n):\n\n\n# Estimated number of tokens for this notebook\nint(len(notebook.get_context_for_llm())/3)\n\n16843",
    "crumbs": [
      "wordslab-notebooks-lib.notebook"
    ]
  },
  {
    "objectID": "notebook.html#notebook.chat---the-notebook-assistant",
    "href": "notebook.html#notebook.chat---the-notebook-assistant",
    "title": "wordslab-notebooks-lib.notebook",
    "section": "notebook.chat - the notebook assistant",
    "text": "notebook.chat - the notebook assistant\n\nPrompt template\nSyntax to reference tools and variables\n\n\nTools and variables\nDefine test tools and variables\n\ndef add(a: int,  # The first number\n        b: int   # The second number\n       ) -&gt; int: # The sum of the two numbers\n  \"\"\"Add two numbers\"\"\"\n  return a + b\n\n\ndef multiply(a: int,  # The first number \n             b: int   # The second number\n            ) -&gt; int: # The product of the two numbers\n  \"\"\"Multiply two numbers\"\"\"\n  return a * b\n\ncat_name = \"My cat is named Jerry\"\ndog_name = \"My dog is named Rex\"\n\nMention them so they are available to the AI assistant:\n\nyou can use functions &add, &multiply as tools\nyou can use variables $cat_name, $dog_nameas variables\n\n\ncontext = notebook.get_context_for_llm()\n\n\nfuncs_names = FUNC_RE.findall(context)\nvars_names = VAR_RE.findall(context)\n\nprint(funcs_names)\nprint(vars_names)\n\n['myfunc', 'add', 'multiply']\n['myvar', 'cat_name', 'dog_name']\n\n\n\ntools_schemas_and_functions = notebook.get_tools_schemas_and_functions(funcs_names)\ntools_schemas_and_functions\n\n{'add': ({'type': 'function',\n   'function': {'name': 'add',\n    'description': 'Add two numbers\\n\\nReturns:\\n- type: integer',\n    'parameters': {'type': 'object',\n     'properties': {'a': {'type': 'integer',\n       'description': 'The first number'},\n      'b': {'type': 'integer', 'description': 'The second number'}},\n     'required': ['a', 'b']}}},\n  &lt;function __main__.add(a: int, b: int) -&gt; int&gt;),\n 'multiply': ({'type': 'function',\n   'function': {'name': 'multiply',\n    'description': 'Multiply two numbers\\n\\nReturns:\\n- type: integer',\n    'parameters': {'type': 'object',\n     'properties': {'a': {'type': 'integer',\n       'description': 'The first number'},\n      'b': {'type': 'integer', 'description': 'The second number'}},\n     'required': ['a', 'b']}}},\n  &lt;function __main__.multiply(a: int, b: int) -&gt; int&gt;)}\n\n\n\ntools = Tools([t[1] for t in tools_schemas_and_functions.values()])\ntools.get_functions()\n\n[&lt;function __main__.add(a: int, b: int) -&gt; int&gt;,\n &lt;function __main__.multiply(a: int, b: int) -&gt; int&gt;]\n\n\n\nvar_values = notebook.get_variables_values(vars_names)\nvar_values\n\n{'cat_name': 'My cat is named Jerry', 'dog_name': 'My dog is named Rex'}\n\n\n\nreferenced_variables = \"\\n\".join(L([Var(value, name=name) for name,value in var_values.items()]).map(to_xml))\nreferenced_variables\n\n'&lt;var name=\"cat_name\"&gt;My cat is named Jerry&lt;/var&gt;\\n&lt;var name=\"dog_name\"&gt;My dog is named Rex&lt;/var&gt;'\n\n\n\n\n\nWordslabNotebook.set_ollama_chat_model\n\ndef set_ollama_chat_model(\n    model:str, think:Union=None,\n    context_size:int=32768, # This is the default value for the ollama server in wordslab-notebooks\n    web_search:bool=False, # The ollama API key is necessary to activate web search\n    base_url:str='http://localhost:11434',\n    api_key:Optional=None, # If not provided, the optional key will be pulled from WordslabEnv\n):\n\n\n\n\nWordslabNotebook.set_openrouter_chat_model\n\ndef set_openrouter_chat_model(\n    model:str, think:Union=None,\n    context_size:Optional=None, # For OpenRouter this parameter is ignored, we inherit the remote model config\n    web_search:bool=True, # Web search is activated by default four cloud models in openrouter\n    base_url:str='https://openrouter.ai/api/v1',\n    api_key:Optional=None, # If not provided, the mandatory key will be pulled from WordslabEnv\n):\n\n\n\n\nWordslabNotebook.chat\n\ndef chat(\n    user_instruction:str\n):\n\n\nimport importlib\nimport wordslab_notebooks_lib.chat\nimportlib.reload(wordslab_notebooks_lib.chat)\nfrom wordslab_notebooks_lib.chat import OllamaModelClient, OpenRouterModelClient, Tools\n\n\nnotebook = WordslabNotebook()\n\n\nnotebook.chat(\"Using only the provided tools to make no mistake, what is (11545468+78782431)*418742?\")\n\n\n[Thinking] … thought in 1285 words\n\n\n[Tool call] … add returned 90327899\n\n\n[Tool call] … multiply returned 37824085083058\n\n\n[Thinking] … thought in 191 words\n\n37824085083058\n\n\n\ndef get_weather(\n    city: str # A city name\n) -&gt; str: # A sentence describing the weather\n    \"A service predicting the weather city by city\"\n    return f\"The weather is nice in {city} today\"\n\nYou can use the service &get_weather.\n\nnotebook.chat(\"What will the weather be like tomorrow in Paris?\")\n\n\n[Thinking] … thought in 309 words\n\n\n[Tool call] … get_weather returned The weather is nice in Paris today\n\n\n[Thinking] … thought in 199 words\n\nThe weather is nice in Paris today.\n\n\n\nnotebook.chat(\"What's the name of my dog ?\")\n\n\n[Thinking] … thought in 272 words\n\nRex\n\n\nHere is the check that the frontend extension will do before executing notebook.chat:\n\n(\"notebook\" in globals()) and (\"WordslabNotebook\" in str(type(notebook)))\n\nTrue",
    "crumbs": [
      "wordslab-notebooks-lib.notebook"
    ]
  },
  {
    "objectID": "notebook.html#develop-a-jupyterlab-frontend-extension",
    "href": "notebook.html#develop-a-jupyterlab-frontend-extension",
    "title": "wordslab-notebooks-lib.notebook",
    "section": "Develop a Jupyterlab frontend extension",
    "text": "Develop a Jupyterlab frontend extension\n\nUnderstand Jupyterlab kernels and frontend extensions\nJupyter kernels technical implementation details\nhttps://chatgpt.com/share/692bea08-4510-8004-b9ab-c02feeb97c08\nJupyterlab extension development tutorial\nhttps://jupyterlab.readthedocs.io/en/latest/extension/extension_tutorial.html\n\n\nIntialize the components of a frontend extension\nThe source code of the Jupyterlab frontend extension can be found in the following files:\nTypescript source code, dependencies, and compilation config:\n\nsrc/index.ts\npackage.json\ntsconfig.json\n.yarnrc.yml\n\nExtension manifest and Javascript compiled code\n\nwordslab_notebooks_lib/labextension\n\npackage.json\nstatic/remoteEntry.97d57e417eaf8ebadeb6.js\n\n\nThis is how the extension files are included in the python package:\n\nMANIFEST.in\n\ninclude install.json\ninclude package.json\nrecursive-include wordslab_notebooks_lib/labextension *\n\ngraft wordslab_notebooks_lib/labextension\ngraft src\nThis is how the extension files are installed in Jupyterlab extensions directory when the python package is installed:\n\npyproject.toml\n\n[tool.setuptools]\ninclude-package-data = true \n\n[tool.setuptools.data-files]\n\"share/jupyter/labextensions/wordslab-notebooks-lib\" = [\n  \"wordslab_notebooks_lib/labextension/package.json\",\n  \"install.json\"\n]\n\"share/jupyter/labextensions/wordslab-notebooks-lib/static\" = [\n  \"wordslab_notebooks_lib/labextension/static/*\"\n]\nThis how the command jupyter labextension develop finds the directory where the extension files live:\n\nwordslab_notebooks_lib\\__init__.py\n\ndef _jupyter_labextension_paths():\n    return [{\n        \"src\": \"labextension\",\n        \"dest\": \"wordslab-notebooks-lib\"\n    }]\nThis is how the python package is identified as a Jupyterlab extension in pypi:\n\npyproject.toml\n\nclassifiers = [ \"Framework :: Jupyter :: JupyterLab :: Extensions :: Prebuilt\" ]\n\n\nInstall the Jupyterlab frontend extension in development mode\nOpen a Terminal\ncd $WORDSLAB_WORKSPACE/wordslab-notebooks-lib\nsource $JUPYTERLAB_ENV/.venv/bin/activate\n\n# Install Javascript dependencies\njlpm install\n\n# Build TypeScript extension\njlpm build\n\n# Register the extension with JupyterLab during development\n# jupyter labextension develop . --overwrite\nrm $JUPYTERLAB_ENV/.venv/share/jupyter/labextensions/wordslab-notebooks-lib\nln -s $WORDSLAB_WORKSPACE/wordslab-notebooks-lib/wordslab_notebooks_lib/labextension/ $JUPYTERLAB_ENV/.venv/share/jupyter/labextensions/wordslab-notebooks-lib\n\n# Verify extension is found\njupyter labextension list\n\n\nTest the Jupyterlab frontend extension\nAfter installing the extension in development mode once, you can iterate fast: - update the code in src/index.ts - build the extension with jlpm build\ncd $WORDSLAB_WORKSPACE/wordslab-notebooks-lib\nsource $JUPYTERLAB_ENV/.venv/bin/activate\n\n# Build TypeScript extension\njlpm build\n\nrefresh the Jupyterlab single page app in your browser\ntest the updated extension\n\nNo need to reinstall the extension or to restart Jupyterlab itself, just refresh your browser page.",
    "crumbs": [
      "wordslab-notebooks-lib.notebook"
    ]
  },
  {
    "objectID": "notebook.html#explore-the-notebook-format",
    "href": "notebook.html#explore-the-notebook-format",
    "title": "wordslab-notebooks-lib.notebook",
    "section": "Explore the notebook format",
    "text": "Explore the notebook format\nhttps://nbformat.readthedocs.io/en/latest/format_description.html\n\nnb = nbformat.from_dict(__notebook_content)\n\ncode_language = nb.metadata.language_info.name\nprint(\"&gt; \" + code_language + \" notebook\")\n\nfor cell in nb.cells:\n    if cell.id == __cell_id: break\n        \n    is_markdown = cell.cell_type == \"markdown\"\n    is_code = cell.cell_type == \"code\"\n    is_raw = cell.cell_type == \"raw\"\n\n    print(\"---------------------\")\n    print(\"cell\", cell.id, cell.cell_type)\n    print(\"---------------------\")\n    if is_markdown:\n        print(cell.source[:100])\n    elif is_code:\n        print(f\"```{code_language}\\n\" + cell.source[:100] + \"\\n```\")\n    elif is_raw:\n        print(cell.source[:100])\n    if is_code and cell.execution_count&gt;0 and len(cell.outputs)&gt;0:\n        print(\"---------------------\")\n        print(\"cell outputs\", cell.id, cell.execution_count)\n        print(\"---------------------\")\n        for output in cell.outputs:\n            if output.output_type == \"stream\":\n                print(f\"&lt;{output.name}&gt;\")\n                print(output.text[:100])\n                print(f\"&lt;/{output.name}&gt;\")\n            elif output.output_type == \"display_data\":\n                print(\"&lt;display&gt;\")\n                if \"data\" in output:\n                    print(\"  &lt;data&gt;\")\n                    repr(output.data)\n                    print(\"  &lt;/data&gt;\")\n                if \"metadata\" in output and len(output.metadata)&gt;0:\n                    print(\"  &lt;metadata&gt;\")\n                    repr(output.metadata)\n                    print(\"  &lt;/metadata&gt;\")\n                print(\"&lt;/display&gt;\")\n            elif output.output_type == \"execute_result\":\n                print(\"&lt;result&gt;\")\n                if \"data\" in output:\n                    print(\"  &lt;data&gt;\")\n                    print(output.data)\n                    print(\"  &lt;/data&gt;\")\n                if \"metadata\" in output and len(output.metadata)&gt;0:\n                    print(\"  &lt;metadata&gt;\")\n                    print(output.metadata)\n                    print(\"  &lt;/metadata&gt;\")\n                print(\"&lt;/result&gt;\")\n            elif output.output_type == \"error\":\n                print(\"&lt;error&gt;\")\n                print(output.ename)\n                print(output.evalue)\n                for frame in output.traceback:\n                    print(frame)\n                print(\"&lt;/error&gt;\")\n        print(\"---------------------\")\n\n&gt; python notebook\n---------------------\ncell 9d8a6aa0-8f58-4860-bcc1-2bfbdcb438b6 markdown\n---------------------\n# wordslab-notebooks-lib.jupyterlab\n\n&gt; Access wordslab-notebooks Jupyterlab extension version, curre\n---------------------\ncell 68f3493d-c252-4eb4-844b-abbd68ed3a70 markdown\n---------------------\n## Work together with AI in a Jupyterlab notebook - the Solveit method\n\nA Jupyter notebook is a conv\n---------------------\ncell 0ff6fbdc-4a54-4e29-acbb-07529df8cfdd markdown\n---------------------\n## Install the Jupyterlab extension - wordslab-notebooks-lib\n\nIf you want to use \"prompt\" cells, you\n---------------------\ncell 65cd4cf8-d77b-4026-b428-bbd9550ea971 markdown\n---------------------\n## Communicate with the Jupyterlab extension\n---------------------\ncell ece4d545-8f78-4232-82fb-e837ea0185e4 code\n---------------------\n```python\n#| export\nimport nbformat\n```\n---------------------\ncell af2f3f45-f0da-4d37-9e39-b37d19ba5650 code\n---------------------\n```python\nclass JupyterlabNotebook:\n    def __init__(self):\n        if not \"__wordslab_extension_version\" in g\n```\n---------------------\ncell 16180d26-5d5b-4c01-93d6-60c910f257bf code\n---------------------\n```python\nnotebook = JupyterlabNotebook()\n```\n---------------------\ncell 13af8845-c7f8-440b-a80f-097c7cf1a541 code\n---------------------\n```python\nnotebook.jupyterlab_extension_version\n```\n---------------------\ncell outputs 13af8845-c7f8-440b-a80f-097c7cf1a541 39\n---------------------\n&lt;result&gt;\n  &lt;data&gt;\n{'text/plain': \"'0.0.11'\"}\n  &lt;/data&gt;\n&lt;/result&gt;\n---------------------\n---------------------\ncell 2906d67c-0764-4816-92c6-4063f17621aa code\n---------------------\n```python\nnotebook.path\n```\n---------------------\ncell outputs 2906d67c-0764-4816-92c6-4063f17621aa 40\n---------------------\n&lt;result&gt;\n  &lt;data&gt;\n{'text/plain': \"'wordslab-notebooks-lib/nbs/02_jupyterlab.ipynb'\"}\n  &lt;/data&gt;\n&lt;/result&gt;\n---------------------\n---------------------\ncell 45a764c9-e0ab-415c-947c-bdb12d26a2dd code\n---------------------\n```python\nnotebook.content.metadata\n```\n---------------------\ncell outputs 45a764c9-e0ab-415c-947c-bdb12d26a2dd 41\n---------------------\n&lt;result&gt;\n  &lt;data&gt;\n{'text/plain': \"{'kernelspec': {'display_name': 'wordslab-notebooks-lib',\\n  'language': 'python',\\n  'name': 'wordslab-notebooks-lib'},\\n 'language_info': {'codemirror_mode': {'name': 'ipython', 'version': 3},\\n  'file_extension': '.py',\\n  'mimetype': 'text/x-python',\\n  'name': 'python',\\n  'nbconvert_exporter': 'python',\\n  'pygments_lexer': 'ipython3',\\n  'version': '3.12.12'}}\"}\n  &lt;/data&gt;\n&lt;/result&gt;\n---------------------\n---------------------\ncell d16ad869-d651-40bd-af2c-623d82b4edf0 code\n---------------------\n```python\nnotebook.cell_id\n```\n---------------------\ncell outputs d16ad869-d651-40bd-af2c-623d82b4edf0 42\n---------------------\n&lt;result&gt;\n  &lt;data&gt;\n{'text/plain': \"'d16ad869-d651-40bd-af2c-623d82b4edf0'\"}\n  &lt;/data&gt;\n&lt;/result&gt;\n---------------------\n---------------------\ncell 59347d6e-d1f7-4d23-b041-143e42887f6d code\n---------------------\n```python\nnotebook.cell_id\n```\n---------------------\ncell outputs 59347d6e-d1f7-4d23-b041-143e42887f6d 43\n---------------------\n&lt;result&gt;\n  &lt;data&gt;\n{'text/plain': \"'59347d6e-d1f7-4d23-b041-143e42887f6d'\"}\n  &lt;/data&gt;\n&lt;/result&gt;\n---------------------\n---------------------\ncell 9843c2c4-ac54-46d6-9725-0e957e944e3a markdown\n---------------------\n## Develop a Jupyterlab frontend extension\n---------------------\ncell 4178ac20-4612-4c8d-8d48-0fd2d2605aa9 markdown\n---------------------\n### Understand Jupyterlab kernels and frontend extensions\n\nJupyter kernels technical implementation \n---------------------\ncell da7ecd61-80f6-4a00-a795-6866d62b32bb markdown\n---------------------\n### Intialize the components of a frontend extension\n\nThe source code of the Jupyterlab frontend ext\n---------------------\ncell 7a4146ce-96e0-4c13-9296-67374c833560 markdown\n---------------------\n### Install the Jupyterlab frontend extension in development mode\n\nOpen a Terminal\n\n```bash\ncd $WORD\n---------------------\ncell 6a891df5-84b9-4579-a1ec-18fd7a13ebc2 markdown\n---------------------\n### Test the Jupyterlab frontend extension \n\nAfter installing the extension in development mode once\n---------------------\ncell 61a8b5d6-7821-4c76-acde-9080fb8ad95b markdown\n---------------------\n## Explore the notebook format\n---------------------\ncell bfe108e4-6b27-403d-b5d7-fde736c1f01c markdown\n---------------------\nhttps://nbformat.readthedocs.io/en/latest/format_description.html",
    "crumbs": [
      "wordslab-notebooks-lib.notebook"
    ]
  },
  {
    "objectID": "tools.html",
    "href": "tools.html",
    "title": "wordslab-notebooks-lib.tools",
    "section": "",
    "text": "The following tools code is inspired from Mistral Vibe 2.0 (Apache 2.0 licence) :\nhttps://github.com/mistralai/mistral-vibe/tree/main/vibe/core/tools/builtins",
    "crumbs": [
      "wordslab-notebooks-lib.tools"
    ]
  },
  {
    "objectID": "tools.html#mistral-vibe-bultin-tools",
    "href": "tools.html#mistral-vibe-bultin-tools",
    "title": "wordslab-notebooks-lib.tools",
    "section": "",
    "text": "The following tools code is inspired from Mistral Vibe 2.0 (Apache 2.0 licence) :\nhttps://github.com/mistralai/mistral-vibe/tree/main/vibe/core/tools/builtins",
    "crumbs": [
      "wordslab-notebooks-lib.tools"
    ]
  },
  {
    "objectID": "chat.html",
    "href": "chat.html",
    "title": "wordslab-notebooks-lib.chat",
    "section": "",
    "text": "def refresh_notebook_display(\n    hide_thinking:bool=True, hide_tool_calls:bool=True\n):\n\n\n\n\n\n\ndef ChatTurns(\n    refresh_display:Callable=None\n):\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\n\ndef ChatTurn(\n    refresh_display:Callable=None\n):\n\nInitialize self. See help(type(self)) for accurate signature.\n\nturns = ChatTurns(refresh_notebook_display(hide_thinking=False, hide_tool_calls=False))\n\nturn = turns.new_turn()\nturn.append_thinking(\"I think a lot longer.\\nIn sentences.\\n\\nWith line breaks.\")\ntime.sleep(0.5)\nturn.append_content(\"I need to call 2 tools.\")\ntime.sleep(0.5)\nturn.append_tool_call(\"myfunc\", {\"param1\": \"value1\", \"param2\": \"value2\"})\nturn.start_tool_call(\"myfunc\")\ntime.sleep(0.5)\nturn.end_tool_call(\"myfunc\", 17.43)\nturn1 = turn\n\nturn = turns.new_turn()\nturn.append_thinking(\"Ok, I got the first result, now call the second tool.\")\ntime.sleep(0.5)\nturn.append_tool_call(\"myfunc2\", {})\nturn.start_tool_call(\"myfunc2\")\ntime.sleep(0.5)\nturn.end_tool_call(\"myfunc2\", \"The weather is nice today but clouds an wind are coming for tommorow and the rest of the week will be awful\")\n\nturn = turns.new_turn()\nturn.append_thinking(\"Ok, I got the second result, now I can answer the question.\")\ntime.sleep(0.5)\nturn.append_content(\"This is the incredible result.\")\n\n\n[Thinking]\n\n\nI think a lot longer. In sentences.\nWith line breaks.\n\nI need to call 2 tools.\n\n[Tool call] - model wants to call myfunc with parameters {'param1': 'value1', 'param2': 'value2'} - agent called myfunc at 22:02:46 - myfunc returned 17.43 in 0.503 sec\n\n\n[Thinking]\n\n\nOk, I got the first result, now call the second tool.\n\n\n[Tool call] - model wants to call myfunc2 with parameters {} - agent called myfunc2 at 22:02:47 - myfunc2 returned The weather is nice today but clouds an wind are coming for tommorow and the rest of the week wil... in 0.503 sec\n\n\n[Thinking]\n\n\nOk, I got the second result, now I can answer the question.\n\nThis is the incredible result.\n\n\n\nturn1.thinking, turn1.content\n\n('I think a lot longer.\\nIn sentences.\\n\\nWith line breaks.',\n 'I need to call 2 tools.')\n\n\n\nMarkdown(turn1.to_markdown())\n\n\n[Thinking] … thought in 10 words\n\nI need to call 2 tools.\n\n[Tool call] … myfunc returned 17.43\n\n\n\n\nMarkdown(turn1.to_markdown(hide_thinking=False, hide_tool_calls=False))\n\n\n[Thinking]\n\n\nI think a lot longer. In sentences.\nWith line breaks.\n\nI need to call 2 tools.\n\n[Tool call] - model wants to call myfunc with parameters {'param1': 'value1', 'param2': 'value2'} - agent called myfunc at 22:02:46 - myfunc returned 17.43 in 0.503 sec",
    "crumbs": [
      "wordslab-notebooks-lib.chat"
    ]
  },
  {
    "objectID": "chat.html#observable-conversation-turns",
    "href": "chat.html#observable-conversation-turns",
    "title": "wordslab-notebooks-lib.chat",
    "section": "",
    "text": "def refresh_notebook_display(\n    hide_thinking:bool=True, hide_tool_calls:bool=True\n):\n\n\n\n\n\n\ndef ChatTurns(\n    refresh_display:Callable=None\n):\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\n\ndef ChatTurn(\n    refresh_display:Callable=None\n):\n\nInitialize self. See help(type(self)) for accurate signature.\n\nturns = ChatTurns(refresh_notebook_display(hide_thinking=False, hide_tool_calls=False))\n\nturn = turns.new_turn()\nturn.append_thinking(\"I think a lot longer.\\nIn sentences.\\n\\nWith line breaks.\")\ntime.sleep(0.5)\nturn.append_content(\"I need to call 2 tools.\")\ntime.sleep(0.5)\nturn.append_tool_call(\"myfunc\", {\"param1\": \"value1\", \"param2\": \"value2\"})\nturn.start_tool_call(\"myfunc\")\ntime.sleep(0.5)\nturn.end_tool_call(\"myfunc\", 17.43)\nturn1 = turn\n\nturn = turns.new_turn()\nturn.append_thinking(\"Ok, I got the first result, now call the second tool.\")\ntime.sleep(0.5)\nturn.append_tool_call(\"myfunc2\", {})\nturn.start_tool_call(\"myfunc2\")\ntime.sleep(0.5)\nturn.end_tool_call(\"myfunc2\", \"The weather is nice today but clouds an wind are coming for tommorow and the rest of the week will be awful\")\n\nturn = turns.new_turn()\nturn.append_thinking(\"Ok, I got the second result, now I can answer the question.\")\ntime.sleep(0.5)\nturn.append_content(\"This is the incredible result.\")\n\n\n[Thinking]\n\n\nI think a lot longer. In sentences.\nWith line breaks.\n\nI need to call 2 tools.\n\n[Tool call] - model wants to call myfunc with parameters {'param1': 'value1', 'param2': 'value2'} - agent called myfunc at 22:02:46 - myfunc returned 17.43 in 0.503 sec\n\n\n[Thinking]\n\n\nOk, I got the first result, now call the second tool.\n\n\n[Tool call] - model wants to call myfunc2 with parameters {} - agent called myfunc2 at 22:02:47 - myfunc2 returned The weather is nice today but clouds an wind are coming for tommorow and the rest of the week wil... in 0.503 sec\n\n\n[Thinking]\n\n\nOk, I got the second result, now I can answer the question.\n\nThis is the incredible result.\n\n\n\nturn1.thinking, turn1.content\n\n('I think a lot longer.\\nIn sentences.\\n\\nWith line breaks.',\n 'I need to call 2 tools.')\n\n\n\nMarkdown(turn1.to_markdown())\n\n\n[Thinking] … thought in 10 words\n\nI need to call 2 tools.\n\n[Tool call] … myfunc returned 17.43\n\n\n\n\nMarkdown(turn1.to_markdown(hide_thinking=False, hide_tool_calls=False))\n\n\n[Thinking]\n\n\nI think a lot longer. In sentences.\nWith line breaks.\n\nI need to call 2 tools.\n\n[Tool call] - model wants to call myfunc with parameters {'param1': 'value1', 'param2': 'value2'} - agent called myfunc at 22:02:46 - myfunc returned 17.43 in 0.503 sec",
    "crumbs": [
      "wordslab-notebooks-lib.chat"
    ]
  },
  {
    "objectID": "chat.html#native-tool-calling",
    "href": "chat.html#native-tool-calling",
    "title": "wordslab-notebooks-lib.chat",
    "section": "Native tool calling",
    "text": "Native tool calling\nUse python functions as tools callable by Large Language Models.\nThe python functions must be fully documented: - type annotations are mandatory on all parameters and on the return type - a docstring after the function definition is mandatory, it should explain the return value - a descriptive comment after each parameter is also mandatory - the expected format is: one parameter by line, a traditional python comment at the end of the line\n\ndef add(a: int,  # The first number\n        b: int   # The second number\n       ) -&gt; int: # The sum of the two numbers\n  \"\"\"Add two numbers\"\"\"\n  return a + b\n\n\ndef multiply(a: int,  # The first number \n             b: int   # The second number\n            ) -&gt; int: # The product of the two numbers\n  \"\"\"Multiply two numbers\"\"\"\n  return a * b\n\nTool description format for ollama API\nHere is the code used to process the tools parameter:\nfor unprocessed_tool in tools or []:\n    yield convert_function_to_tool(unprocessed_tool) if callable(unprocessed_tool) else Tool.model_validate(unprocessed_tool)\nSo we can pass either a list of pyhton functions or a list of dictionaries conforming to a specific tool schema.\nHere are the expectations for the python functions documentation:\ndef convert_function_to_tool(func: Callable) -&gt; Tool:\n \n  -&gt; def _parse_docstring(doc_string: Union[str, None]) -&gt; dict[str, str]:\n  ...\n  for line in doc_string.splitlines():\n    ...\n    if lowered_line.startswith('args:'):\n      key = 'args'\n    elif lowered_line.startswith(('returns:', 'yields:', 'raises:')):\n      key = '_'\n  ...\n  for line in parsed_docstring['args'].splitlines():\n    ...\n    if ':' in line:\n      # Split the line on either:\n      # 1. A parenthetical expression like (integer) - captured in group 1\n      # 2. A colon :\n      # Followed by optional whitespace. Only split on first occurrence.\n      ...\nThis is much less robust and readable than what toolslm.funccall.get_schema does, so we will preprocess the list of python functions ourselves.\nNow let’s see what tool description schema is expected by ollama.\npydantic Tool.model_validate() accepts: - dict - Pydantic model instances - Objects with attributes (ORM-style, if configured)\nHere is the ollama schema:\nclass Tool(SubscriptableBaseModel):\n  type: Optional[str] = 'function'\n\n  class Function(SubscriptableBaseModel):\n    name: Optional[str] = None\n    description: Optional[str] = None\n\n    class Parameters(SubscriptableBaseModel):\n      model_config = ConfigDict(populate_by_name=True)\n      type: Optional[Literal['object']] = 'object'\n      defs: Optional[Any] = Field(None, alias='$defs')\n      items: Optional[Any] = None\n      required: Optional[Sequence[str]] = None\nSo this is the schema expected by the openai completions API, as we will see below.\nTool description formats for the openai API\nThe legacy openai completions API: client.chat.completions.create(...) expects tools to be described in a json format that uses the “wrapped function” schema:\ntool = {\n  type: \"function\",\n  function: {\n    name,\n    description,\n    parameters\n  }\n}\nThis is the canonical format for Chat Completions and is what OpenAI examples historically used.\nThis format does not work for the new API: client.responses.create(...)\nThe Responses API uses a flattened tool schema.\n{\n  \"type\": \"function\",\n  \"name\": \"...\",\n  \"description\": \"..\",\n  \"parameters\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"a\": {\"type\": \"integer\"},\n      \"b\": {\"type\": \"integer\"}\n    },\n    \"required\": [\"a\", \"b\"]\n  }\n}\nIf you pass your wrapped version (function: {…}) to responses.create, you’ll get a schema validation error.\n\nChat Completions treats tools as message-level actions → nested function\nResponses API treats tools as first-class model capabilities → flattened schema\nThe Responses API also supports non-function tools (web search, file search, computer use), which drove the redesign\n\nIf you want maximum forward compatibility: - Use the flattened format - Even when working with Chat Completions, it’s easy to convert\n\n\nget_tools_schemas_and_functions\n\ndef get_tools_schemas_and_functions(\n    funcs:Sequence, responsesAPIFormat:bool=False\n):\n\nGet a dictionary of json schemas and callable functions which can be used for native tool calling.\n\nget_tools_schemas_and_functions([add, multiply])\n\n{'add': ({'type': 'function',\n   'function': {'name': 'add',\n    'description': 'Add two numbers\\n\\nReturns:\\n- type: integer',\n    'parameters': {'type': 'object',\n     'properties': {'a': {'type': 'integer',\n       'description': 'The first number'},\n      'b': {'type': 'integer', 'description': 'The second number'}},\n     'required': ['a', 'b']}}},\n  &lt;function __main__.add(a: int, b: int) -&gt; int&gt;),\n 'multiply': ({'type': 'function',\n   'function': {'name': 'multiply',\n    'description': 'Multiply two numbers\\n\\nReturns:\\n- type: integer',\n    'parameters': {'type': 'object',\n     'properties': {'a': {'type': 'integer',\n       'description': 'The first number'},\n      'b': {'type': 'integer', 'description': 'The second number'}},\n     'required': ['a', 'b']}}},\n  &lt;function __main__.multiply(a: int, b: int) -&gt; int&gt;)}\n\n\n\nget_tools_schemas_and_functions([add, multiply], responsesAPIFormat=True)\n\n{'add': ({'type': 'function',\n   'name': 'add',\n   'description': 'Add two numbers\\n\\nReturns:\\n- type: integer',\n   'parameters': {'type': 'object',\n    'properties': {'a': {'type': 'integer', 'description': 'The first number'},\n     'b': {'type': 'integer', 'description': 'The second number'}},\n    'required': ['a', 'b']}},\n  &lt;function __main__.add(a: int, b: int) -&gt; int&gt;),\n 'multiply': ({'type': 'function',\n   'name': 'multiply',\n   'description': 'Multiply two numbers\\n\\nReturns:\\n- type: integer',\n   'parameters': {'type': 'object',\n    'properties': {'a': {'type': 'integer', 'description': 'The first number'},\n     'b': {'type': 'integer', 'description': 'The second number'}},\n    'required': ['a', 'b']}},\n  &lt;function __main__.multiply(a: int, b: int) -&gt; int&gt;)}\n\n\n\n\n\nTools\n\ndef Tools(\n    python_functions:Sequence, responsesAPIFormat:bool=False\n):\n\n“Execute tools implemented as python functions with Large Language Models. The python functions must be fully documented: - type annotations are mandatory on all parameters and on the return type - a docstring after the function definition is mandatory - a descriptive comment after each parameter and the return type is also mandatory - the expected format is: one parameter by line, a traditional python comment at the end of the line\n\n\n\nToolExecutionError\n\ndef ToolExecutionError(\n    args:VAR_POSITIONAL, kwargs:VAR_KEYWORD\n):\n\nRaised when a tool cannot be executed safely.\n\ntools = Tools([add,multiply])\n\n\ntools.has_tool(\"add\"), tools.has_tool(\"toto\")\n\n(True, False)\n\n\n\ntools.get_schemas()\n\n[{'type': 'function',\n  'function': {'name': 'add',\n   'description': 'Add two numbers\\n\\nReturns:\\n- type: integer',\n   'parameters': {'type': 'object',\n    'properties': {'a': {'type': 'integer', 'description': 'The first number'},\n     'b': {'type': 'integer', 'description': 'The second number'}},\n    'required': ['a', 'b']}}},\n {'type': 'function',\n  'function': {'name': 'multiply',\n   'description': 'Multiply two numbers\\n\\nReturns:\\n- type: integer',\n   'parameters': {'type': 'object',\n    'properties': {'a': {'type': 'integer', 'description': 'The first number'},\n     'b': {'type': 'integer', 'description': 'The second number'}},\n    'required': ['a', 'b']}}}]\n\n\n\ntools.get_schema(\"add\")\n\n{'type': 'function',\n 'function': {'name': 'add',\n  'description': 'Add two numbers\\n\\nReturns:\\n- type: integer',\n  'parameters': {'type': 'object',\n   'properties': {'a': {'type': 'integer', 'description': 'The first number'},\n    'b': {'type': 'integer', 'description': 'The second number'}},\n   'required': ['a', 'b']}}}\n\n\n\ntools.get_functions()\n\n[&lt;function __main__.add(a: int, b: int) -&gt; int&gt;,\n &lt;function __main__.multiply(a: int, b: int) -&gt; int&gt;]\n\n\n\ntools.get_function(\"add\")\n\n&lt;function __main__.add(a: int, b: int) -&gt; int&gt;\n\n\n\ntools.call(\"add\", {\"a\": 1, \"b\": 2})\n\n3",
    "crumbs": [
      "wordslab-notebooks-lib.chat"
    ]
  },
  {
    "objectID": "chat.html#images",
    "href": "chat.html#images",
    "title": "wordslab-notebooks-lib.chat",
    "section": "Images",
    "text": "Images\n\n\nImages\n\ndef Images(\n    image:Union=None\n):\n\nInitialize self. See help(type(self)) for accurate signature.\n\nimages = Images(\"puppy.jpg\")\nfor image in images.get_base64_data():\n    print(image[:50])\n\n/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQ\n\n\n\nimages = Images(\"https://i.pinimg.com/736x/3c/fa/a2/3cfaa27aeff09adff6c2e6fbc5fd0dfa.jpg\")\nfor image in images.get_base64_data():\n    print(image[:50])\n\n/9j/4AAQSkZJRgABAQEASABIAAD/2wBDAAYEBQYFBAYGBQYHBw\n\n\n\nfor image,is_url in images.get_base64_data_or_url():\n    print(image[:50], is_url)\n\nhttps://i.pinimg.com/736x/3c/fa/a2/3cfaa27aeff09ad True\n\n\n\nimage_bytes = Path(\"puppy.jpg\").read_bytes()\nimages = Images(image_bytes)\nfor image in images.get_base64_data():\n    print(image[:50])\n\n/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQ\n\n\n\nimages.add_web_url(\"https://i.pinimg.com/736x/3c/fa/a2/3cfaa27aeff09adff6c2e6fbc5fd0dfa.jpg\")\nfor image,is_url in images.get_base64_data_or_url():\n    print(image[:50], is_url)\n\n/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQ False\nhttps://i.pinimg.com/736x/3c/fa/a2/3cfaa27aeff09ad True",
    "crumbs": [
      "wordslab-notebooks-lib.chat"
    ]
  },
  {
    "objectID": "chat.html#model-client",
    "href": "chat.html#model-client",
    "title": "wordslab-notebooks-lib.chat",
    "section": "Model client",
    "text": "Model client\n\n\nModelClient\n\ndef ModelClient(\n    model:str, context_size:Optional=None, base_url:Optional=None, api_key:Optional=None\n):\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\n\nollama model client\n\n\n\nOllamaModelClient\n\ndef OllamaModelClient(\n    model:str, context_size:int=32768, # This is the default value for the ollama server in wordslab-notebooks\n    base_url:str='http://localhost:11434',\n    api_key:Optional=None, # If not provided, the optional key will be pulled from WordslabEnv\n):\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nmodel = env.default_model_code\nmodel\n\n'qwen3:30b'\n\n\n\noclient = OllamaModelClient(model, context_size=65000)\n\nollama: loading model qwen3:30b with context size 65000 ... ok\n\n\n\nprompt = \"In one sentence: why is the sky blue?\"\noclient(user_prompt=prompt, think=True, max_new_tokens=1000, seed=42, temperature=2)\n\n\n[Thinking] … thought in 205 words\n\nSunlight scatters in Earth’s atmosphere, with shorter blue wavelengths scattering more effectively than other colors, causing the sky to appear blue.\n\n\n\nsystem = \"Talk like a pirate\"\nprompt = \"In one sentence: why is the sky blue?\"\noclient(system_prompt=system, user_prompt=prompt)\n\n\n[Thinking] … thought in 264 words\n\nArrr! The sun’s light scatters in the air, and the short blue waves bounce all about, making the sky shine like a doubloon!\n\n\n\nprompt = \"In one sentence: why is the sky blue?\"\nassistant = \"Once upon a time \"\noclient(user_prompt=prompt, assistant_prefill=assistant)\n\nOnce upon a time 200 years ago, a scientist named Rayleigh discovered that the sky is blue because of a phenomenon called Rayleigh scattering, where the atmosphere scatters shorter blue wavelengths of sunlight more than longer red wavelengths, making the sky appear blue during the day.\n\n\n\nprompt = \"Using only the provided tools to make no mistake, what is (11545468+78782431)*418742?\"\ntools = Tools([add, multiply])\noclient(user_prompt=prompt, tools=tools, think=True)\n\n\n[Thinking] … thought in 2925 words\n\n\n[Tool call] … add returned 90327899\n\n\n[Tool call] … multiply returned 37824085083058\n\n\n[Thinking] … thought in 116 words\n\nThe result of \\((11545468 + 78782431) \\times 418742\\) is 37824085083058.\n\n\n\nclass Book:\n    def __init__(self, title: str, pages: int):\n        self.title = title\n        self.pages = pages\n\n    def __repr__(self):\n        return f\"Book Title : {self.title}\\nNumber of Pages : {self.pages}\"\n\nbook = Book(\"War and Peace\", 950)\nbook\n\nBook Title : War and Peace\nNumber of Pages : 950\n\n\n\ndef find_page(book: Book, # The book to find the halfway point of\n              percent: int, # Percent of a book to read to, e.g. halfway == 50, \n) -&gt; int:\n    \"The page number corresponding to `percent` completion of a book\"\n    return round(book.pages * (percent / 100.0))\n\nfind_page(book, 50)\n\n475\n\n\n\nprompt = \"Using only the provided tools to make no mistake, how many pages do I have to read to get halfway through my 950 page copy of War and Peace\"\ntools = Tools([find_page])\noclient(user_prompt=prompt, tools=tools, think=True)\n\n\n[Thinking] … thought in 285 words\n\n\n[Tool call] … find_page returned 475\n\n\n[Thinking] … thought in 94 words\n\nTo reach the halfway point of your 950-page copy of War and Peace, you need to read 475 pages.\n\n\nImages\n\nmodel = \"devstral-small-2:24b\"\n\n\noclient = OllamaModelClient(model)\n\nollama: loading model devstral-small-2:24b with context size 32768 ... ok\n\n\n\nprompt = \"Describe this picture in a structured way\"\nimages = Images(\"puppy.jpg\")\noclient(user_prompt=prompt, user_images=images)\n\nHere is a structured description of the image:\n\nSubject:\n\nA young Golden Retriever puppy.\n\nAppearance:\n\nCoat: Light golden fur, slightly wavy and fluffy.\nEyes: Dark, round, and expressive.\nEars: Floppy and medium-sized.\nMouth: Open slightly, showing a playful expression.\nBody: Compact and sturdy, typical of a puppy.\n\nAction:\n\nThe puppy appears to be in motion, possibly running or playing.\nFront paws are lifted off the ground, suggesting movement.\n\nBackground:\n\nOutdoor setting with a grassy field.\nBlurred background, indicating focus on the puppy.\n\nLighting and Mood:\n\nNatural daylight, likely during the day.\nWarm and cheerful atmosphere, enhanced by the puppy’s joyful expression.\n\nAdditional Details:\n\nA leash is visible around the puppy’s neck, suggesting it might be on a walk or playtime.\nThe grass is slightly tall and appears well-maintained.\n\n\nThis structured description captures the key elements of the image in a clear and organized manner.\n\n\n\nprompt = \"Describe both images in a short paragraph\"\nimages.add_web_url(\"https://i.pinimg.com/736x/3c/fa/a2/3cfaa27aeff09adff6c2e6fbc5fd0dfa.jpg\")\noclient(user_prompt=prompt, user_images=images)\n\nThe first image shows a golden retriever puppy standing in a grassy field, looking directly at the camera with a happy expression. The puppy has a fluffy coat and appears to be in motion, possibly running or playing. The second image features the same golden retriever puppy sitting in a grassy field during sunset, with the sun visible in the background. The puppy is looking up and to the side, also with a joyful expression, and its fur is illuminated by the warm, golden light of the setting sun. Both images capture the playful and cheerful nature of the puppy in different settings.\n\n\nStructured outputs\n\nclass Pet(BaseModel):\n    model_config = ConfigDict(extra=\"forbid\")    \n    name: str\n    animal: str\n    age: int\n    color: str | None\n\nclass PetList(BaseModel):\n    model_config = ConfigDict(extra=\"forbid\")\n    pets: list[Pet]\n\n\nprompt = \"I have two cats named Luna and Loki, Luna is 2 years old an yellow, Loki is 2 years older and the same color as the sky\"\noclient(user_prompt=prompt, output_model=PetList)\n\noclient.response\n\n{ “pets”: [ { “name”: “Luna”, “animal”: “cat”, “age”: 2, “color”: “yellow” }, { “name”: “Loki”, “animal”: “cat”, “age”: 4, “color”: “blue” } ] }\n\n\nPetList(pets=[Pet(name='Luna', animal='cat', age=2, color='yellow'), Pet(name='Loki', animal='cat', age=4, color='blue')])\n\n\nWeb search\n\nmodel = env.default_model_code\noclient = OllamaModelClient(model, context_size=65000)\n\nollama: loading model qwen3:30b with context size 65000 ... ok\n\n\n\nprompt = \"what are the features in the latest github relase of ollama\"\noclient(user_prompt=prompt, think=True, web_search=True)\n\n\n[Thinking] … thought in 357 words\n\n\n[Tool call] … web_search returned results=[WebSearchResult(content='Releases ·oll...\n\n\n[Thinking] … thought in 585 words\n\nThe latest GitHub release of Ollama is v0.14.0-rc2 (pre-release), which includes the following key features:\n\nNew Features:\n\nExperimental CLI:\n\nollama run --experimental now includes an agent loop and the bash tool for interactive workflows.\n\nAnthropic API Compatibility:\n\nSupport for the /v1/messages API endpoint (compatible with Anthropic models).\n\nModel Version Requirements:\n\nNew REQUIRES command in Modelfile to declare required Ollama version for a model.\n\nVRAM Improvements:\n\nFixes for integer underflow on low VRAM systems during memory estimation.\n\nMore accurate VRAM measurements for AMD iGPUs.\n\nApp Enhancements:\n\nOllama’s app now highlights Swift source code.\n\nImproved error handling for embeddings returning NaN or -Inf.\n\nLinux Install Optimization:\n\nLinux install bundles now use zst compression (smaller downloads).\n\nExperimental Image Generation:\n\nNew support for image generation models via MLX (experimental).\n\n\n\n\n\nNotes:\n\nStable Release: The latest stable release is v0.13.5 (Dec 18, 2025), which added support for bert architecture models, DeepSeek-V3.1 tool parsing, and Google’s FunctionGemma model.\nPre-release: v0.14.0-rc2 is the most recent release candidate, but it is not yet finalized (marked as “Pre-release” on GitHub).\n\nFor the most up-to-date features, check the Ollama GitHub Releases page.\n\n\n\n\nprompt = \"read https://docs.ollama.com/capabilities/web-search and summarize in one sentence what tools i can use to implement a search agent with ollama\"\noclient(user_prompt=prompt, think=True, web_search=True)\n\n\n[Thinking] … thought in 424 words\n\n\n[Tool call] … web_fetch returned title='Web search - Ollama' content='Web search...\n\n\n[Thinking] … thought in 224 words\n\nOllama provides the web_search and web_fetch APIs as core tools to implement a search agent, enabling model-based web queries and page content retrieval for accurate, up-to-date information.\n\n\n\n\nopenrouter chat client\n\nenv = WordslabEnv()\n\n\nclient = OpenAI(base_url=\"https://openrouter.ai/api/v1\", api_key=env.cloud_openrouter_api_key)\n\n\nmodel = \"google/gemini-3-flash-preview\"\nmessages = [{'role': 'user', 'content': 'What is the smallest number palindrome greater than 130?'}]\nstream = client.chat.completions.create(model=model, messages=messages, stream=True, extra_body={\"reasoning\": {\"enabled\": True}})\nfor chunk in stream:\n    delta = chunk.choices[0].delta\n    print(delta)\n\nChoiceDelta(content='', function_call=None, refusal=None, role='assistant', tool_calls=None, reasoning=\"**Identifying a Solution**\\n\\nI've homed in on the core challenge: pinpointing the smallest palindrome exceeding 130. The constraints are clear, and I'm strategizing how to efficiently generate and validate candidate numbers. I am starting by looking at the numbers directly after 130.\\n\\n\\n\", reasoning_details=[{'index': 0, 'type': 'reasoning.text', 'text': \"**Identifying a Solution**\\n\\nI've homed in on the core challenge: pinpointing the smallest palindrome exceeding 130. The constraints are clear, and I'm strategizing how to efficiently generate and validate candidate numbers. I am starting by looking at the numbers directly after 130.\\n\\n\\n\", 'format': 'google-gemini-v1'}], annotations=[])\nChoiceDelta(content='', function_call=None, refusal=None, role='assistant', tool_calls=None, reasoning=\"**Determining the Answer**\\n\\nI've directly confirmed that 131 fulfills all criteria. No need to look further: it's the smallest palindrome that's larger than 130. I have confirmed that this number satisfies the relevant constraints, and I consider this the final result.\\n\\n\\n\", reasoning_details=[{'index': 0, 'type': 'reasoning.text', 'text': \"**Determining the Answer**\\n\\nI've directly confirmed that 131 fulfills all criteria. No need to look further: it's the smallest palindrome that's larger than 130. I have confirmed that this number satisfies the relevant constraints, and I consider this the final result.\\n\\n\\n\", 'format': 'google-gemini-v1'}], annotations=[])\nChoiceDelta(content='The smallest palindrome number greater than 130 is **131**.\\n\\nA palindrome is a number that reads the same forwards and backwards. Since 131 reads as \"1-3-1\" in both directions and is the very next integer after 130 that follows this rule, it is the', function_call=None, refusal=None, role='assistant', tool_calls=None, reasoning=None, reasoning_details=[], annotations=[])\nChoiceDelta(content=' correct answer.', function_call=None, refusal=None, role='assistant', tool_calls=None, reasoning=None, reasoning_details=[{'index': 0, 'type': 'reasoning.encrypted', 'data': 'CiIBjz1rXx1wDxrqK4iEiYLEfo4BC0TbRcSKhvVO48q1Ge9HCmgBjz1rX93sCp42z//JmKxDWvv3kYw3fkxVXSsbAkJ9/OsR5GxzvX3NRxq1GmWU6PzNix6QQ9aFfZmZqEwc3u/anTQiQwCLH+N77Rh7vLxAvr0VCSVLJ83rqljqFi93fMyjr0pjX9OE5gpZAY89a1/PpcHwJxd6EjqwLvSFlebK6nVnQDn90G86P3/tRXy2sZxYogVEh44KsQZ4r+0B/2ClLCNJf+EvIBCEs6zXHTMLx6AJIDU6SumITgIppnPdYWwoUuoKeAGPPWtfeJ7lICOOm3dtb2YzhnJiSGPI5m96K1G/2lwo04wSze+2vbS0FGKQnAYkOR21tRqDV+JGOKon/MdNhKTfjVd+TNPUfcQCNQh+dmlVajIJRyql4sYeYSg6Wz+AA0A0dNv6GWIKaXmKCBS087b+FUT1ZMBuSgpgAY89a1+ZR6BSJsZTg5YUGg11YzeDIsaqhs+FvHy/1XB4uPp3SW+mqIfysxta4e4y7oeOuTNButgOKnDeaJfPG8VSjSh0lT0R1dHEifHAIFbiF2IKQJXW07ns6L1E9syYCn8Bjz1rXwn3gb5Q8D+qPic5yZTziFidiTxpKH3uhjyYqTrZR8hRGnmVVk05a8E/5J81UkxOevZ5yAxLiFCdimXI5yr7LriL4bDiHajdSXVxdOiKHzb6Pqx3MiLySYUt1ToD3XUAIZQBmfDVw+Qd6SQdagvsi86NKMv793xrUMlECroBAY89a18BugGYHx3fQYckccMCOS91fxOFH7hBj24O746sJhbrBMLmQSPk0du421Zmd8Lx1Ns21/SwpHCfnQ3AEA9BZ9XfahR51tE96d9DjXO7kMsgGDPoLsDApRTnRenQbPJnpXqoQYZbsnE/H1B6Tb4wcE5xJpcBKLRvXhg+c14T5NYpFEcE3dHaNUH6xXj68ZLBl2Q5FtzqOd6tyxSLRhQJfa00yDFnpn+Lbo0zLSRqbP+ovAWIq79BCpMBAY89a1/ENqGNTYpM7VJzpDZus8SoUClqlwaOwrOOHYfz6NaB3CP3coADgPnltU0hZrGfE0w4xHdM85pJaT/2HHv9GQH2o2R4nAGwT3KInxBeAVo0TtE2M9aN78wDhveiyqPUsHO1uybcRorF3uUwEIAT8NX93ykz3FwcVZvO/aYtE9A7mVN4qYgXfjvIFwIBjFJs', 'format': 'google-gemini-v1'}], annotations=[])\nChoiceDelta(content='', function_call=None, refusal=None, role='assistant', tool_calls=None)\n\n\n\nmessages = [{'role': 'user', 'content': \"Using only the provided tools to make no mistake, what is (11545468+78782431)*418742?\"}]\ntools = Tools([add, multiply])\nstream = client.chat.completions.create(model=model, messages=messages, tools = tools.get_schemas(), stream=True, extra_body={\"reasoning\": {\"enabled\": True}})\nfor chunk in stream:\n    delta = chunk.choices[0].delta\n    print(delta)\n\nChoiceDelta(content='', function_call=None, refusal=None, role='assistant', tool_calls=None, reasoning=\"**Calculating the Total Sum**\\n\\nI'm currently focused on the first step: summing the initial numbers. I've successfully employed the `add` function, and the intermediate result is now readily available. It's a significant figure, and I'm ready to proceed to the next stage after a brief review.\\n\\n\\n\", reasoning_details=[{'index': 0, 'type': 'reasoning.text', 'text': \"**Calculating the Total Sum**\\n\\nI'm currently focused on the first step: summing the initial numbers. I've successfully employed the `add` function, and the intermediate result is now readily available. It's a significant figure, and I'm ready to proceed to the next stage after a brief review.\\n\\n\\n\", 'format': 'google-gemini-v1'}], annotations=[])\nChoiceDelta(content='', function_call=None, refusal=None, role='assistant', tool_calls=None, reasoning=\"**Initiating Multiplication Operations**\\n\\nI've got the total sum from the previous stage, a substantial number. Now, the plan is to multiply this sum by 418,742. I'm preparing to invoke the `multiply` function, and I'm ready to observe the final result soon.\\n\\n\\n\", reasoning_details=[{'index': 0, 'type': 'reasoning.text', 'text': \"**Initiating Multiplication Operations**\\n\\nI've got the total sum from the previous stage, a substantial number. Now, the plan is to multiply this sum by 418,742. I'm preparing to invoke the `multiply` function, and I'm ready to observe the final result soon.\\n\\n\\n\", 'format': 'google-gemini-v1'}], annotations=[])\nChoiceDelta(content=None, function_call=None, refusal=None, role='assistant', tool_calls=[ChoiceDeltaToolCall(index=0, id='tool_add_bxj7qiqYR0YykpEmfzoC', function=ChoiceDeltaToolCallFunction(arguments='{\"b\":78782431,\"a\":11545468}', name='add'), type='function')], reasoning=None, reasoning_details=[{'index': 0, 'id': 'tool_add_bxj7qiqYR0YykpEmfzoC', 'type': 'reasoning.encrypted', 'data': 'CiQBjz1rX1bExybTM6VtK0kASX6FTxfBBwyi932MRRrEuYtvkdsKUQGPPWtfr55kYc/lU0ZhVkHjN6wZ74W1LVMPXkSF3tFEoDwQtfzpuzYnV7xzD1qz5CnzNvZm5i9eY0qQKJhzhXAC3FTmjZ6D2OdODRvDX7uUGwo0AY89a1/zpxr22ksclwFiLTfaAWYr57nJPyB5mHc8h6PIE3uu5ggyUE4VsZoaqVxv2q03jQp6AY89a187jC4w08IR8/fDOrJwQEOLQO5yNEUlQphL8lckJvAltj2ULGXY06WBV8yi1n3YGUZEAgLU4ihc/o8+/nVuk+OnKvicbju9XY82ZeP35D8yFVPEFMyHifVptjjPry8rCpr4N84UipqVfc0rmtFPBbE6+6TTFIUKaQGPPWtf6XkgaitG1kvzHAeW+I55A2dWbhyG/8Z15RjCkArCtKMEEf1r9v7G0ke7kNuIW8jhHF5H/wm/20m69zrsCp1obV+jbZR8T6lb3Km4EXJOldkm/U1lsHZRI8Lp2mkzreV1QbkqHwo2AY89a1/PJ5AClPojS2Y4jbD3oN8tblRCmUNTysi69fLd/4tqapYHz4CLAo+LMhFvzLN9N4qZ', 'format': 'google-gemini-v1'}], annotations=[])\nChoiceDelta(content='', function_call=None, refusal=None, role='assistant', tool_calls=None)\n\n\nOpenrouter API reference\nNote: as of January 2026 - the OpenAI-compatible Responses API (Beta) is in beta stage and may have breaking changes. Use with caution in production environments.\n=&gt; we will use the competions API for now\nhttps://openrouter.ai/docs/api/reference/overview\nREQUEST SCHEMA\n// Definitions of subtypes are below\ntype Request = {\n  // Either \"messages\" or \"prompt\" is required\n  messages?: Message[];\n  prompt?: string;\n  // If \"model\" is unspecified, uses the user's default\n  model?: string; // See \"Supported Models\" section\n  // Allows to force the model to produce specific output format.\n  // See models page and note on this docs page for which models support it.\n  response_format?: { type: 'json_object' };\n  stop?: string | string[];\n  stream?: boolean; // Enable streaming\n  // See LLM Parameters (openrouter.ai/docs/api/reference/parameters)\n  max_tokens?: number; // Range: [1, context_length)\n  temperature?: number; // Range: [0, 2]\n  // Tool calling\n  // Will be passed down as-is for providers implementing OpenAI's interface.\n  // For providers with custom interfaces, we transform and map the properties.\n  // Otherwise, we transform the tools into a YAML template. The model responds with an assistant message.\n  // See models supporting tool calling: openrouter.ai/models?supported_parameters=tools\n  tools?: Tool[];\n  tool_choice?: ToolChoice;\n  // Advanced optional parameters\n  seed?: number; // Integer only\n  top_p?: number; // Range: (0, 1]\n  top_k?: number; // Range: [1, Infinity) Not available for OpenAI models\n  frequency_penalty?: number; // Range: [-2, 2]\n  presence_penalty?: number; // Range: [-2, 2]\n  repetition_penalty?: number; // Range: (0, 2]\n  logit_bias?: { [key: number]: number };\n  top_logprobs: number; // Integer only\n  min_p?: number; // Range: [0, 1]\n  top_a?: number; // Range: [0, 1]\n  // Reduce latency by providing the model with a predicted output\n  // https://platform.openai.com/docs/guides/latency-optimization#use-predicted-outputs\n  prediction?: { type: 'content'; content: string };\n  // OpenRouter-only parameters\n  // See \"Prompt Transforms\" section: openrouter.ai/docs/guides/features/message-transforms\n  transforms?: string[];\n  // See \"Model Routing\" section: openrouter.ai/docs/guides/features/model-routing\n  models?: string[];\n  route?: 'fallback';\n  // See \"Provider Routing\" section: openrouter.ai/docs/guides/routing/provider-selection\n  provider?: ProviderPreferences;\n  user?: string; // A stable identifier for your end-users. Used to help detect and prevent abuse.\n  \n  // Debug options (streaming only)\n  debug?: {\n    echo_upstream_body?: boolean; // If true, returns the transformed request body sent to the provider\n  };\n};\n\n// Subtypes:\ntype TextContent = {\n  type: 'text';\n  text: string;\n};\ntype ImageContentPart = {\n  type: 'image_url';\n  image_url: {\n    url: string; // URL or base64 encoded image data\n    detail?: string; // Optional, defaults to \"auto\"\n  };\n};\ntype ContentPart = TextContent | ImageContentPart;\ntype Message =\n  | {\n      role: 'user' | 'assistant' | 'system';\n      // ContentParts are only for the \"user\" role:\n      content: string | ContentPart[];\n      // If \"name\" is included, it will be prepended like this\n      // for non-OpenAI models: `{name}: {content}`\n      name?: string;\n    }\n  | {\n      role: 'tool';\n      content: string;\n      tool_call_id: string;\n      name?: string;\n    };\ntype FunctionDescription = {\n  description?: string;\n  name: string;\n  parameters: object; // JSON Schema object\n};\ntype Tool = {\n  type: 'function';\n  function: FunctionDescription;\n};\ntype ToolChoice =\n  | 'none'\n  | 'auto'\n  | {\n      type: 'function';\n      function: {\n        name: string;\n      };\nRESPONSE SCHEMA\n// Definitions of subtypes are below\ntype Response = {\n  id: string;\n  // Depending on whether you set \"stream\" to \"true\" and\n  // whether you passed in \"messages\" or a \"prompt\", you\n  // will get a different output shape\n  choices: (NonStreamingChoice | StreamingChoice | NonChatChoice)[];\n  created: number; // Unix timestamp\n  model: string;\n  object: 'chat.completion' | 'chat.completion.chunk';\n  system_fingerprint?: string; // Only present if the provider supports it\n  // Usage data is always returned for non-streaming.\n  // When streaming, you will get one usage object at\n  // the end accompanied by an empty choices array.\n  usage?: ResponseUsage;\n};\n// If the provider returns usage, we pass it down\n// as-is. Otherwise, we count using the GPT-4 tokenizer.\ntype ResponseUsage = {\n  /** Including images and tools if any */\n  prompt_tokens: number;\n  /** The tokens generated */\n  completion_tokens: number;\n  /** Sum of the above two fields */\n  total_tokens: number;\n};\n\n\n// Subtypes:\ntype NonChatChoice = {\n  finish_reason: string | null;\n  text: string;\n  error?: ErrorResponse;\n};\ntype NonStreamingChoice = {\n  finish_reason: string | null;\n  native_finish_reason: string | null;\n  message: {\n    content: string | null;\n    role: string;\n    tool_calls?: ToolCall[];\n  };\n  error?: ErrorResponse;\n};\ntype StreamingChoice = {\n  finish_reason: string | null;\n  native_finish_reason: string | null;\n  delta: {\n    content: string | null;\n    role?: string;\n    tool_calls?: ToolCall[];\n  };\n  error?: ErrorResponse;\n};\ntype ErrorResponse = {\n  code: number; // See \"Error Handling\" section\n  message: string;\n  metadata?: Record&lt;string, unknown&gt;; // Contains additional error information such as provider details, the raw error message, etc.\n};\ntype ToolCall = {\n  id: string;\n  type: 'function';\n  function: FunctionCall;\n};\n\n\n\nOpenRouterModelClient\n\ndef OpenRouterModelClient(\n    model:str,\n    context_size:Optional=None, # For OpenRouter this parameter is ignored, we inherit the remote model config\n    base_url:str='https://openrouter.ai/api/v1',\n    api_key:Optional=None, # If not provided, the mandatory key will be pulled from WordslabEnv\n):\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nmodel = \"anthropic/claude-sonnet-4.5\"\norclient = OpenRouterModelClient(model)\n\nopenrouter: testing model anthropic/claude-sonnet-4.5 ... ok\n\n\n\nprompt = 'Why is the sky blue?'\norclient(user_prompt=prompt)\n\nThe sky is blue due to a phenomenon called Rayleigh scattering.\nHere’s how it works:\n\nSunlight contains all colors - White sunlight is actually made up of all the colors of the rainbow (different wavelengths of light).\nLight interacts with the atmosphere - When sunlight enters Earth’s atmosphere, it collides with gas molecules (mainly nitrogen and oxygen).\nBlue light scatters more - Shorter wavelengths (blue and violet) scatter much more easily than longer wavelengths (red and orange). Blue light gets scattered in all directions throughout the sky.\nWhy not violet? - Violet scatters even more than blue, but our eyes are more sensitive to blue, and some violet light is absorbed by the upper atmosphere, so we perceive the sky as blue.\n\nAt sunset/sunrise, the sky turns red/orange because sunlight travels through more atmosphere to reach your eyes, scattering away most of the blue light and leaving the longer red and orange wavelengths visible.\n\n\n\nprompt = 'Why is the sky blue?'\norclient(system_prompt=\"Talk like a pirate\", user_prompt=prompt)\n\nArrr, ye be askin’ a fine question there, matey!\nThe sky be blue because of a bit o’ science called “Rayleigh scatterin’,” savvy? When the sun’s light comes sailin’ through our atmosphere, it be carryin’ all the colors o’ the rainbow mixed together, aye.\nNow here be the trick - the tiny molecules in the air be like little scallywags that scatter the light in all directions. But the blue light, bein’ shorter and choppier like waves in a storm, gets scattered MORE than the other colors. Red and orange light? Those long wavelengths sail right on through like a ship with the wind at its back!\nSo when ye look up at the heavens, yer eyes be seein’ all that scattered blue light bouncin’ around the sky from every direction. That be why the whole sky looks blue instead of just where the sun be!\nAt sunset though, arrr, the light travels through more atmosphere - like a longer voyage across the seas - and all that blue gets scattered away completely, leavin’ only the reds and oranges to paint the sky. Beautiful as a Caribbean sunset, it is!\nTips tricorn hat\nAny more questions for this old sea dog? ⚓\n\n\n\nprompt = 'Why is the sky blue?'\norclient(user_prompt=prompt, assistant_prefill=\"Once upon a time \")\n\nOnce upon a time in the kingdom of Light, photons embarked on a journey from the Sun to Earth. As they traveled through the atmosphere, they encountered tiny molecules of nitrogen and oxygen.\nThese molecules were much smaller than the wavelengths of visible light, which caused something magical called Rayleigh scattering. This type of scattering has a special property: it scatters shorter wavelengths (blue and violet light) much more effectively than longer wavelengths (red and orange light) — specifically, shorter wavelengths scatter about 10 times more!\nHere’s what happens:\n🔵 Blue light gets scattered in all directions by air molecules 🟣 Violet light scatters even more, but our eyes are less sensitive to it 🔴 Red/orange light passes through with less scattering\nSo when you look up at the sky (away from the Sun), you’re seeing all that scattered blue light coming from every direction. This creates the beautiful blue canopy above us.\nFun fact: At sunrise and sunset, sunlight travels through more atmosphere to reach your eyes. Most of the blue light gets scattered away before it reaches you, leaving the warm reds and oranges — creating those stunning twilight colors! 🌅\n\n\n\nprompt = 'What is the smallest number palindrome greater than 130?'\norclient(user_prompt=prompt, think=1024, max_new_tokens=2000, seed=42, temperature=0.7)\n\n\n[Thinking] … thought in 92 words\n\nLooking at numbers greater than 130:\n\n131: reads as 1-3-1 → This is the same forwards and backwards ✓\n\n131 is the smallest palindrome greater than 130.\n\n\n\nprompt = \"Using only the provided tools to make no mistake, what is (11545468+78782431)*418742?\"\ntools = Tools([add, multiply])\norclient(user_prompt=prompt, tools=tools, think=2014)\n\n\n[Thinking] … thought in 35 words\n\nI’ll solve this step by step using the provided tools.\nFirst, let me add 11545468 and 78782431:\n\n[Tool call] … add returned 90327899\n\nNow let me multiply the result by 418742:\n\n[Tool call] … multiply returned 37824085083058\n\nThe answer is 37,824,085,083,058.\n\n\nImages\n\nmodel = \"openai/gpt-5.2\"\norclient = OpenRouterModelClient(model)\n\nopenrouter: testing model openai/gpt-5.2 ... ok\n\n\n\nprompt = \"Describe this picture in a structured way\"\nimages = Images(\"puppy.jpg\")\norclient(user_prompt=prompt, user_images=images)\n\nStructured Description of the Image\n\n1) Overview\n\nScene type: Outdoor animal portrait/action shot\n\nMain subject: A light-colored puppy (appears to be a golden retriever-type) running toward the camera\n\nSetting: Open grassy field with a softly blurred background\n\n\n\n2) Subject Details\n\nAnimal: Young dog/puppy\n\nFur color & texture: Cream to pale golden, fluffy coat\n\nFace: Dark eyes, black nose, mouth open with tongue visible (panting/happy expression)\n\nEars: Floppy, slightly darker golden tone than the face\n\nAccessories: Thin collar/strap visible around the neck area\n\n\n\n3) Action & Pose\n\nMotion: Running forward toward the viewer\n\nBody position: Front paw lifted mid-step; posture suggests energetic movement\n\nExpression/emotion: Playful, cheerful, excited (open mouth “smile,” tongue out)\n\n\n\n4) Environment / Background\n\nForeground: Short grass with a few small stems/seedheads visible\n\nBackground: Strong blur (shallow depth of field), suggesting distance and open space; colors imply a natural landscape (field/meadow)\n\n\n\n5) Composition & Camera Perspective\n\nFraming: Subject centered and dominant in the frame\n\nAngle: Low, near ground level, emphasizing the puppy’s approach\n\nFocus: Sharp focus on the puppy; background is heavily defocused (bokeh), drawing attention to the subject\n\n\n\n6) Lighting & Color Palette\n\nLighting: Soft natural daylight, evenly illuminating the puppy\n\nDominant colors: Cream/golden (puppy), green/yellow-brown (grass and background), muted neutral sky tones\n\nOverall tone: Warm and gentle, with a natural outdoor feel\n\n\n\n7) Mood / Impression\n\nMood: Joyful, lively, friendly\n\nImplied context: A puppy playing or running freely in a field\n\n\n\n\n\nprompt = \"Describe both images in a short paragraph\"\nimages.add_web_url(\"https://i.pinimg.com/736x/3c/fa/a2/3cfaa27aeff09adff6c2e6fbc5fd0dfa.jpg\")\norclient(user_prompt=prompt, user_images=images)\n\nBoth images feature a fluffy golden retriever puppy outdoors in a grassy field. In the first image, the puppy is running toward the camera with its mouth open and tongue out, looking playful and energetic against a softly blurred green background. In the second image, the puppy is sitting calmly in warm golden-hour light, gazing upward with a relaxed expression while the sun hangs low in an orange sky behind it.\n\n\nStructured outputs\n\nprompt = \"I have two cats named Luna and Loki, Luna is 2 years old an yellow, Loki is 2 years older and the same color as the sky\"\norclient(user_prompt=prompt, output_model=PetList)\norclient.response\n\n\n[Thinking] … thought in 71 words\n\n{“pets”:[{“name”:“Luna”,“animal”:“cat”,“age”:2,“color”:“yellow”},{“name”:“Loki”,“animal”:“cat”,“age”:4,“color”:“blue”}]}\n\n\nPetList(pets=[Pet(name='Luna', animal='cat', age=2, color='yellow'), Pet(name='Loki', animal='cat', age=4, color='blue')])\n\n\nWeb search\n\nprompt = \"what are the features in the latest github relase of ollama\"\norclient(user_prompt=prompt, web_search=True)\n\n\n[Thinking] … thought in 473 words\n\nAs of January 10, 2026, the newest item on Ollama’s GitHub “Releases” page is v0.14.0-rc2 (pre-release). (github.com)\n\nv0.14.0-rc2 (pre-release) — main new features / changes\n\nNew experimental CLI mode: ollama run --experimental adds a new Ollama CLI with an agent loop and a built-in bash tool. (github.com)\n\nAnthropic API compatibility: support for the /v1/messages API. (github.com)\n\nModelfile version gating: new REQUIRES command to declare the minimum Ollama version a model needs. (github.com)\n\nLow-VRAM safety fix: avoids an integer underflow during memory estimation on low-VRAM systems (for older models). (github.com)\n\nBetter AMD iGPU reporting: more accurate VRAM measurements. (github.com)\n\nApp improvement: highlights Swift source code. (github.com)\n\nEmbedding robustness: returns an error if embeddings contain NaN or -Inf. (github.com)\n\nLinux installer packaging: bundles now use zst compression. (github.com)\n\nExperimental image generation: adds experimental support for image generation models, powered by MLX. (github.com)\n\n\n\nIf you meant the latest stable (non-pre-release) release: v0.13.5\n\nNew model: Google’s FunctionGemma (Gemma 3 270M tuned for function calling). (github.com)\n\nEngine support: bert-architecture models now run on Ollama’s engine. (github.com)\n\nDeepSeek-V3.1 improvements: built-in renderer and tool parsing. (github.com)\n\nTool rendering fix: nested tool properties render correctly. (github.com)\n\nIf you tell me whether you’re on macOS/Windows/Linux (and whether you want stable only), I can highlight which of these changes you’ll actually notice day-to-day.",
    "crumbs": [
      "wordslab-notebooks-lib.chat"
    ]
  },
  {
    "objectID": "chat.html#structured-description-of-the-image",
    "href": "chat.html#structured-description-of-the-image",
    "title": "wordslab-notebooks-lib.chat",
    "section": "Structured Description of the Image",
    "text": "Structured Description of the Image\n\n1) Overview\n\nScene type: Outdoor animal portrait/action shot\n\nMain subject: A light-colored puppy (appears to be a golden retriever-type) running toward the camera\n\nSetting: Open grassy field with a softly blurred background\n\n\n\n2) Subject Details\n\nAnimal: Young dog/puppy\n\nFur color & texture: Cream to pale golden, fluffy coat\n\nFace: Dark eyes, black nose, mouth open with tongue visible (panting/happy expression)\n\nEars: Floppy, slightly darker golden tone than the face\n\nAccessories: Thin collar/strap visible around the neck area\n\n\n\n3) Action & Pose\n\nMotion: Running forward toward the viewer\n\nBody position: Front paw lifted mid-step; posture suggests energetic movement\n\nExpression/emotion: Playful, cheerful, excited (open mouth “smile,” tongue out)\n\n\n\n4) Environment / Background\n\nForeground: Short grass with a few small stems/seedheads visible\n\nBackground: Strong blur (shallow depth of field), suggesting distance and open space; colors imply a natural landscape (field/meadow)\n\n\n\n5) Composition & Camera Perspective\n\nFraming: Subject centered and dominant in the frame\n\nAngle: Low, near ground level, emphasizing the puppy’s approach\n\nFocus: Sharp focus on the puppy; background is heavily defocused (bokeh), drawing attention to the subject\n\n\n\n6) Lighting & Color Palette\n\nLighting: Soft natural daylight, evenly illuminating the puppy\n\nDominant colors: Cream/golden (puppy), green/yellow-brown (grass and background), muted neutral sky tones\n\nOverall tone: Warm and gentle, with a natural outdoor feel\n\n\n\n7) Mood / Impression\n\nMood: Joyful, lively, friendly\n\nImplied context: A puppy playing or running freely in a field",
    "crumbs": [
      "wordslab-notebooks-lib.chat"
    ]
  },
  {
    "objectID": "chat.html#v0.14.0-rc2-pre-release-main-new-features-changes",
    "href": "chat.html#v0.14.0-rc2-pre-release-main-new-features-changes",
    "title": "wordslab-notebooks-lib.chat",
    "section": "v0.14.0-rc2 (pre-release) — main new features / changes",
    "text": "v0.14.0-rc2 (pre-release) — main new features / changes\n\nNew experimental CLI mode: ollama run --experimental adds a new Ollama CLI with an agent loop and a built-in bash tool. (github.com)\n\nAnthropic API compatibility: support for the /v1/messages API. (github.com)\n\nModelfile version gating: new REQUIRES command to declare the minimum Ollama version a model needs. (github.com)\n\nLow-VRAM safety fix: avoids an integer underflow during memory estimation on low-VRAM systems (for older models). (github.com)\n\nBetter AMD iGPU reporting: more accurate VRAM measurements. (github.com)\n\nApp improvement: highlights Swift source code. (github.com)\n\nEmbedding robustness: returns an error if embeddings contain NaN or -Inf. (github.com)\n\nLinux installer packaging: bundles now use zst compression. (github.com)\n\nExperimental image generation: adds experimental support for image generation models, powered by MLX. (github.com)",
    "crumbs": [
      "wordslab-notebooks-lib.chat"
    ]
  },
  {
    "objectID": "chat.html#if-you-meant-the-latest-stable-non-pre-release-release-v0.13.5",
    "href": "chat.html#if-you-meant-the-latest-stable-non-pre-release-release-v0.13.5",
    "title": "wordslab-notebooks-lib.chat",
    "section": "If you meant the latest stable (non-pre-release) release: v0.13.5",
    "text": "If you meant the latest stable (non-pre-release) release: v0.13.5\n\nNew model: Google’s FunctionGemma (Gemma 3 270M tuned for function calling). (github.com)\n\nEngine support: bert-architecture models now run on Ollama’s engine. (github.com)\n\nDeepSeek-V3.1 improvements: built-in renderer and tool parsing. (github.com)\n\nTool rendering fix: nested tool properties render correctly. (github.com)\n\nIf you tell me whether you’re on macOS/Windows/Linux (and whether you want stable only), I can highlight which of these changes you’ll actually notice day-to-day.",
    "crumbs": [
      "wordslab-notebooks-lib.chat"
    ]
  },
  {
    "objectID": "chat.html#models-providers",
    "href": "chat.html#models-providers",
    "title": "wordslab-notebooks-lib.chat",
    "section": "Models providers",
    "text": "Models providers\n\nDesign concepts\n\nUser centric workflow\n\nidentify your self-hosted inference or inference as a service options\nunderstand your task type, properties, privacy needs and scale\nfind the best model for your task, given your constraints\nprepare and start your self hosted inference or connect to your inference as a service provider\nmonitor your resource usage and cost\n\n\n\nSelf-hosted inference or inference as a service\nModel families - architecture name - parameter size - training type: base / instruct / thinking - version: relase date - quantization\nModel constraints - model capabilities - modalities in/out - context length - instruction - thinking - tools - model usage - prompt template and special tokens - languages supported - recommended use cases - prompting guidelines - model license - use case restrictions - commercial usage restrictions - outputs usage restrictions - model transparency\nSelf-hosted inference constraints - model requirements - size on disk -&gt; download time / load time in vram - size in vram -&gt; max context length / num parallel sequence - tensor flops -&gt; input tokens/sec - memory bandwidth -&gt; output tokens/sec - inference machine constraints - download speed - disk size and speed - GPU vram, memory bandwidth, tensor flops - rented machine constraints - GPU availability - price when you use per GPU - price when you don’t use per GB (storage)\nInference as a service constraints - router constraints - … same as provider constraints below … - provider constraints - terms of service - privacy options - inference quotas - service availability - per model provider constraints - model capabilities exposed - input/output tokens cost - input/output tokens/sec\n\n\n\nList, download and load models\n\nExplore ollama API\nGet ollama version\n\nRequest\ncurl http://localhost:11434/api/version\nResponse\n{\n  \"version\": \"0.5.1\"\n}\n\nList remote models\nAs of december 2025, there is no API to get the ollama catalog of models, web scraping is the only solution.\n\nimport httpx\nimport re\nfrom html import unescape\n\ndef updated_to_months(updated):\n    \"\"\"\n    Convert strings like:\n      \"1 year ago\", \"2 years ago\",\n      \"1 month ago\", \"3 weeks ago\",\n      \"7 days ago\", \"yesterday\",\n      \"4 hours ago\"\n    into integer months.\n    \"\"\"\n    if not updated:\n        return None\n\n    updated = updated.lower().strip()\n\n    # handle 'yesterday' explicitly\n    if updated == \"yesterday\":\n        return 0\n\n    # years → months\n    m = re.match(r'(\\d+)\\s+year', updated)\n    if m:\n        years = int(m.group(1))\n        return years * 12\n\n    # months\n    m = re.match(r'(\\d+)\\s+month', updated)\n    if m:\n        return int(m.group(1))\n\n    # weeks\n    m = re.match(r'(\\d+)\\s+week', updated)\n    if m:\n        weeks = int(m.group(1))\n        return max(0, weeks // 4)\n\n    # days\n    m = re.match(r'(\\d+)\\s+day', updated)\n    if m:\n        return 0\n\n    # hours / minutes / seconds → treat as &lt; 1 month\n    if any(unit in updated for unit in [\"hour\", \"minute\", \"second\"]):\n        return 0\n\n    return None\n\ndef pulls_to_int(pulls_str):\n    \"\"\"\n    Convert a pulls string like:\n        '5M', '655.8K', '49K', '73.7M', '957.4K', '27.7M'\n    into an integer.\n    \"\"\"\n    if not pulls_str:\n        return None\n\n    pulls_str = pulls_str.strip().upper()\n\n    match = re.match(r'([\\d,.]+)\\s*([KM]?)', pulls_str)\n    if not match:\n        return None\n\n    number, suffix = match.groups()\n    # Remove commas and convert to float\n    number = float(number.replace(',', ''))\n\n    if suffix == 'M':\n        number *= 1_000_000\n    elif suffix == 'K':\n        number *= 1_000\n\n    return int(number)\n\ndef parse_model_list_regex(html):\n    models = []\n\n    # --- Extract each &lt;li x-test-model&gt;...&lt;/li&gt; block ---\n    li_blocks = re.findall(\n        r'&lt;li[^&gt;]*x-test-model[^&gt;]*&gt;(.*?)&lt;/li&gt;',\n        html,\n        flags=re.DOTALL\n    )\n\n    for block in li_blocks:\n\n        # name from &lt;a href=\"/library/...\"&gt;\n        name = None\n        m = re.search(r'href=\"/library/([^\"]+)\"', block)\n        if m:\n            name = m.group(1)\n\n        # description &lt;p class=\"max-w-lg ...\"&gt;...&lt;/p&gt;\n        description = \"\"\n        m = re.search(\n            r'&lt;p[^&gt;]*text-neutral-800[^&gt;]*&gt;(.*?)&lt;/p&gt;',\n            block,\n            flags=re.DOTALL\n        )\n        if m:\n            description = re.sub(r'&lt;.*?&gt;', '', m.group(1)).strip()\n            description = unescape(description)\n\n        # capabilities (x-test-capability)\n        capabilities = re.findall(\n            r'&lt;span[^&gt;]*x-test-capability[^&gt;]*&gt;(.*?)&lt;/span&gt;',\n            block,\n            flags=re.DOTALL\n        )\n        capabilities = [c.strip() for c in capabilities]\n\n        # check for the special 'cloud' span \n        cloud = False\n        if re.search(\n            r'&lt;span[^&gt;]*&gt;cloud&lt;/span&gt;',\n            block,\n            flags=re.DOTALL\n        ):\n            cloud = True\n\n        # sizes (x-test-size)\n        sizes = re.findall(\n            r'&lt;span[^&gt;]*x-test-size[^&gt;]*&gt;(.*?)&lt;/span&gt;',\n            block,\n            flags=re.DOTALL\n        )\n        sizes = [s.strip() for s in sizes]\n\n        # pulls &lt;span x-test-pull-count&gt;5M&lt;/span&gt;\n        pulls = None\n        m = re.search(\n            r'&lt;span[^&gt;]*x-test-pull-count[^&gt;]*&gt;(.*?)&lt;/span&gt;',\n            block\n        )\n        if m:\n            pulls = m.group(1).strip()\n\n        # tag count &lt;span x-test-tag-count&gt;5&lt;/span&gt;\n        tag_count = None\n        m = re.search(\n            r'&lt;span[^&gt;]*x-test-tag-count[^&gt;]*&gt;(.*?)&lt;/span&gt;',\n            block\n        )\n        if m:\n            tag_count = m.group(1).strip()\n\n        # updated text &lt;span x-test-updated&gt;...&lt;/span&gt;\n        updated = None\n        m = re.search(\n            r'&lt;span[^&gt;]*x-test-updated[^&gt;]*&gt;(.*?)&lt;/span&gt;',\n            block\n        )\n        if m:\n            updated = m.group(1).strip()\n\n        models.append({\n            \"name\": name,\n            \"description\": description,\n            \"capabilities\": capabilities,\n            \"cloud\": cloud,\n            \"sizes\": sizes,\n            \"pulls\": pulls_to_int(pulls),\n            \"tag_count\": int(tag_count),\n            \"updated_months\": updated_to_months(updated),\n            \"url\": f\"https://ollama.com/library/{name}\" if name else None\n        })\n\n    return models   \n\ndef list_models(contains=None):\n    \"\"\"\n    Extract model names and properties from https://ollama.com/library\n    Optionally filter by substring.\n    \"\"\"\n\n    html = httpx.get(\"https://ollama.com/library\").text\n    models = parse_model_list_regex(html)\n\n    if contains:\n        models = [\n            m for m in models\n            if contains.lower() in m[\"name\"].lower()\n        ]\n        models = sorted(models, key=lambda m:m[\"name\"])\n\n    return models\n\ndef list_recent_models_from_family(familyfilter):\n    return [f\"{m['name']} {m['capabilities'] if len(m['capabilities'])&gt;0 else ''} {m['sizes'] if len(m['sizes'])&gt;0 else ''}{' [cloud]' if m['cloud'] else ''}\" for m in list_models(familyfilter) if m[\"updated_months\"] is not None and m[\"updated_months\"]&lt;12]\n\ndef list_tags(model):\n    \"\"\"\n    Extract valid quantized tags only, without HTML noise,\n    and apply the same exclusions as original greps.\n    \"\"\"\n    html = httpx.get(f\"https://ollama.com/library/{model}/tags\").text\n\n    # Capture ONLY the tag part after model:..., e.g. 3b-instruct-q4_K_M\n    raw_tags = re.findall(\n        rf'{re.escape(model)}:([A-Za-z0-9._-]*q[A-Za-z0-9._-]*)',\n        html\n    )\n\n    # Re-add full prefix model:&lt;tag&gt;\n    tags = [f\"{model}:{t}\" for t in raw_tags]\n\n    # Exclude text|base|fp|q4_[01]|q5_[01]\n    tags = [\n        t for t in tags\n        if not re.search(r'(text|base|fp|q[45]_[01])', t)\n    ]\n\n    # Deduplicate\n    return set(tags)\n\n\nlist_models()[:5]\n\n[{'name': 'gpt-oss',\n  'description': 'OpenAI’s open-weight models designed for powerful reasoning, agentic tasks, and versatile developer use cases.',\n  'capabilities': ['tools', 'thinking'],\n  'cloud': True,\n  'sizes': ['20b', '120b'],\n  'pulls': 5000000,\n  'tag_count': 5,\n  'updated_months': 1,\n  'url': 'https://ollama.com/library/gpt-oss'},\n {'name': 'qwen3-vl',\n  'description': 'The most powerful vision-language model in the Qwen model family to date.',\n  'capabilities': ['vision', 'tools'],\n  'cloud': True,\n  'sizes': ['2b', '4b', '8b', '30b', '32b', '235b'],\n  'pulls': 656300,\n  'tag_count': 59,\n  'updated_months': 1,\n  'url': 'https://ollama.com/library/qwen3-vl'},\n {'name': 'ministral-3',\n  'description': 'The Ministral 3 family is designed for edge deployment, capable of running on a wide range of hardware.',\n  'capabilities': ['vision', 'tools'],\n  'cloud': True,\n  'sizes': ['3b', '8b', '14b'],\n  'pulls': 49100,\n  'tag_count': 16,\n  'updated_months': 0,\n  'url': 'https://ollama.com/library/ministral-3'},\n {'name': 'deepseek-r1',\n  'description': 'DeepSeek-R1 is a family of open reasoning models with performance approaching that of leading models, such as O3 and Gemini 2.5 Pro.',\n  'capabilities': ['tools', 'thinking'],\n  'cloud': False,\n  'sizes': ['1.5b', '7b', '8b', '14b', '32b', '70b', '671b'],\n  'pulls': 73700000,\n  'tag_count': 35,\n  'updated_months': 5,\n  'url': 'https://ollama.com/library/deepseek-r1'},\n {'name': 'qwen3-coder',\n  'description': \"Alibaba's performant long context models for agentic and coding tasks.\",\n  'capabilities': ['tools'],\n  'cloud': True,\n  'sizes': ['30b', '480b'],\n  'pulls': 958100,\n  'tag_count': 10,\n  'updated_months': 2,\n  'url': 'https://ollama.com/library/qwen3-coder'}]\n\n\n\nlist_recent_models_from_family(\"qwen\")\n\n[\"qwen2.5-coder ['tools'] ['0.5b', '1.5b', '3b', '7b', '14b', '32b']\",\n \"qwen2.5vl ['vision'] ['3b', '7b', '32b', '72b']\",\n \"qwen3 ['tools', 'thinking'] ['0.6b', '1.7b', '4b', '8b', '14b', '30b', '32b', '235b']\",\n \"qwen3-coder ['tools'] ['30b', '480b'] [cloud]\",\n \"qwen3-embedding ['embedding'] ['0.6b', '4b', '8b']\",\n \"qwen3-vl ['vision', 'tools'] ['2b', '4b', '8b', '30b', '32b', '235b'] [cloud]\"]\n\n\n\nlist_recent_models_from_family(\"gemma\")\n\n[\"embeddinggemma ['embedding'] ['300m']\",\n \"gemma3 ['vision'] ['270m', '1b', '4b', '12b', '27b'] [cloud]\",\n \"gemma3n  ['e2b', 'e4b']\"]\n\n\n\nlist_recent_models_from_family(\"stral\")\n\n[\"devstral ['tools'] ['24b']\",\n \"magistral ['tools', 'thinking'] ['24b']\",\n \"ministral-3 ['vision', 'tools'] ['3b', '8b', '14b'] [cloud]\",\n \"mistral ['tools'] ['7b']\",\n 'mistral-large-3   [cloud]',\n \"mistral-nemo ['tools'] ['12b']\",\n \"mistral-small ['tools'] ['22b', '24b']\",\n \"mistral-small3.1 ['vision', 'tools'] ['24b']\",\n \"mistral-small3.2 ['vision', 'tools'] ['24b']\"]\n\n\n\nlist_recent_models_from_family(\"gpt\")\n\n[\"gpt-oss ['tools', 'thinking'] ['20b', '120b'] [cloud]\",\n \"gpt-oss-safeguard ['tools', 'thinking'] ['20b', '120b']\"]\n\n\n\nlist_recent_models_from_family(\"deepseek\")\n\n[\"deepseek-ocr ['vision'] ['3b']\",\n \"deepseek-r1 ['tools', 'thinking'] ['1.5b', '7b', '8b', '14b', '32b', '70b', '671b']\",\n \"deepseek-v3  ['671b']\",\n \"deepseek-v3.1 ['tools', 'thinking'] ['671b'] [cloud]\"]\n\n\n\nlist_recent_models_from_family(\"glm\")\n\n['glm-4.6   [cloud]']\n\n\n\nlist_recent_models_from_family(\"granite\")\n\n[\"granite-embedding ['embedding'] ['30m', '278m']\",\n \"granite3.1-dense ['tools'] ['2b', '8b']\",\n \"granite3.1-moe ['tools'] ['1b', '3b']\",\n \"granite3.2 ['tools'] ['2b', '8b']\",\n \"granite3.2-vision ['vision', 'tools'] ['2b']\",\n \"granite3.3 ['tools'] ['2b', '8b']\",\n \"granite4 ['tools'] ['350m', '1b', '3b']\"]\n\n\n\nlist_recent_models_from_family(\"llama\")\n\n[\"llama3.2-vision ['vision'] ['11b', '90b']\",\n \"llama4 ['vision', 'tools'] ['16x17b', '128x17b']\"]\n\n\n\nlist_recent_models_from_family(\"phi\")\n\n[\"dolphin-mixtral  ['8x7b', '8x22b']\",\n \"dolphin3  ['8b']\",\n \"phi4  ['14b']\",\n \"phi4-mini ['tools'] ['3.8b']\",\n \"phi4-mini-reasoning  ['3.8b']\",\n \"phi4-reasoning  ['14b']\"]\n\n\n\nlist_recent_models_from_family(\"hermes\")\n\n[\"hermes3 ['tools'] ['3b', '8b', '70b', '405b']\",\n \"nous-hermes2-mixtral  ['8x7b']\"]\n\n\n\nlist_recent_models_from_family(\"olmo\")\n\n[\"olmo2  ['7b', '13b']\"]\n\n\n\nlist_recent_models_from_family(\"embed\")\n\n[\"embeddinggemma ['embedding'] ['300m']\",\n \"granite-embedding ['embedding'] ['30m', '278m']\",\n \"qwen3-embedding ['embedding'] ['0.6b', '4b', '8b']\"]\n\n\n\nlist_tags(\"ministral-3\")\n\n{'ministral-3:14b-instruct-2512-q4_K_M',\n 'ministral-3:14b-instruct-2512-q8_0',\n 'ministral-3:3b-instruct-2512-q4_K_M',\n 'ministral-3:3b-instruct-2512-q8_0',\n 'ministral-3:8b-instruct-2512-q4_K_M',\n 'ministral-3:8b-instruct-2512-q8_0'}\n\n\n\nlist_tags(\"mistral-small3.2\")\n\n{'mistral-small3.2:24b-instruct-2506-q4_K_M',\n 'mistral-small3.2:24b-instruct-2506-q8_0'}\n\n\n\nlist_tags(\"qwen3-vl\")\n\n{'qwen3-vl:235b-a22b-instruct-q4_K_M',\n 'qwen3-vl:235b-a22b-instruct-q8_0',\n 'qwen3-vl:235b-a22b-thinking-q4_K_M',\n 'qwen3-vl:235b-a22b-thinking-q8_0',\n 'qwen3-vl:2b-instruct-q4_K_M',\n 'qwen3-vl:2b-instruct-q8_0',\n 'qwen3-vl:2b-thinking-q4_K_M',\n 'qwen3-vl:2b-thinking-q8_0',\n 'qwen3-vl:30b-a3b-instruct-q4_K_M',\n 'qwen3-vl:30b-a3b-instruct-q8_0',\n 'qwen3-vl:30b-a3b-thinking-q4_K_M',\n 'qwen3-vl:30b-a3b-thinking-q8_0',\n 'qwen3-vl:32b-instruct-q4_K_M',\n 'qwen3-vl:32b-instruct-q8_0',\n 'qwen3-vl:32b-thinking-q4_K_M',\n 'qwen3-vl:32b-thinking-q8_0',\n 'qwen3-vl:4b-instruct-q4_K_M',\n 'qwen3-vl:4b-instruct-q8_0',\n 'qwen3-vl:4b-thinking-q4_K_M',\n 'qwen3-vl:4b-thinking-q8_0',\n 'qwen3-vl:8b-instruct-q4_K_M',\n 'qwen3-vl:8b-instruct-q8_0',\n 'qwen3-vl:8b-thinking-q4_K_M',\n 'qwen3-vl:8b-thinking-q8_0'}\n\n\nhttps://github.com/ollama/ollama/blob/main/docs/api.md#list-local-models\nollama.list().models -&gt; list(ollama._types.ListResponse.Model)\nollama._types.ListResponse.Model\n- model: str 'qwen3:4b'\n- modified_at: datetime.datetime datetime(2025, 11, 22, 18, 53, 11)\n- digest: str '359d7dd4bcdab3d86b87d73ac27966f4dbb9f5efdfcc75d34a8764a09474fae7'\n- size: pydantic.types.ByteSize 2497293931\n- details: ollama._types.ModelDetails\n  - parent_model: str ''\n  - format: str 'gguf'\n  - family: str 'qwen3'\n  - families: Sequence[str] ['qwen3']\n  - parameter_size: str '4.0B'\n  - quantization_level: str 'Q4_K_M'\n\nollama.list().models[0]\n\nModel(model='qwen3:4b', modified_at=datetime.datetime(2025, 11, 22, 18, 53, 11, 586211, tzinfo=TzInfo(3600)), digest='359d7dd4bcdab3d86b87d73ac27966f4dbb9f5efdfcc75d34a8764a09474fae7', size=2497293931, details=ModelDetails(parent_model='', format='gguf', family='qwen3', families=['qwen3'], parameter_size='4.0B', quantization_level='Q4_K_M'))\n\n\nhttps://github.com/ollama/ollama/blob/main/docs/api.md#show-model-information\nollama._types.ShowResponse\n- modified_at: datetime.datetime datetime.datetime(2025, 11, 22, 18, 53, 11)\n- template: str '{{- $lastUserIdx := -1 -}}...\\n{{- end }}'\n- modelfile: str '...'\n- license: str '...'\n- details: ollama._types.ModelDetails -&gt; see above\n- model_info: Mapping[str, Any]\n  -'general.architecture': 'qwen3'\n  -'general.basename': 'Qwen3' \n  -'general.file_type': 15\n  -'general.finetune': 'Thinking' \n  -'general.license': 'apache-2.0'\n  -'general.license.link': 'https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507/blob/main/LICENSE'\n  -'general.parameter_count': 4022468096\n  -'general.quantization_version': 2, \n  -'general.size_label': '4B'\n  -'general.tags': None\n  -'general.type': 'model'\n  -'general.version': '2507'\n  -'qwen3.attention.head_count': 32\n  -'qwen3.attention.head_count_kv': 8\n  -'qwen3.attention.key_length': 128\n  -'qwen3.attention.layer_norm_rms_epsilon': 1e-06\n  -'qwen3.attention.value_length': 128\n  -'qwen3.block_count': 36\n  -'qwen3.context_length': 262144\n  -'qwen3.embedding_length': 2560\n  -'qwen3.feed_forward_length': 9728\n  -'qwen3.rope.freq_base': 5000000\n  -'tokenizer.ggml.add_bos_token': False\n  -'tokenizer.ggml.bos_token_id': 151643\n  -'tokenizer.ggml.eos_token_id': 151645\n  -'tokenizer.ggml.merges': None\n  -'tokenizer.ggml.model': 'gpt2'\n  -'tokenizer.ggml.padding_token_id': 151643\n  -'tokenizer.ggml.pre': 'qwen2'\n  -'tokenizer.ggml.token_type': None\n  -'tokenizer.ggml.tokens': None\n- parameters: str 'top_p 0.95\\n repeat_penalty 1\\n stop \"&lt;|im_start|&gt;\"\\n stop \"&lt;|im_end|&gt;\"\\n temperature 0.6\\ n top_k 20'\n- capabilities: List[str] ['completion', 'tools', 'thinking']\n\nollama.show('gemma3:4b').capabilities, ollama.show('gemma3:4b').modelinfo\n\n(['completion', 'vision'],\n {'gemma3.attention.head_count': 8,\n  'gemma3.attention.head_count_kv': 4,\n  'gemma3.attention.key_length': 256,\n  'gemma3.attention.sliding_window': 1024,\n  'gemma3.attention.value_length': 256,\n  'gemma3.block_count': 34,\n  'gemma3.context_length': 131072,\n  'gemma3.embedding_length': 2560,\n  'gemma3.feed_forward_length': 10240,\n  'gemma3.mm.tokens_per_image': 256,\n  'gemma3.vision.attention.head_count': 16,\n  'gemma3.vision.attention.layer_norm_epsilon': 1e-06,\n  'gemma3.vision.block_count': 27,\n  'gemma3.vision.embedding_length': 1152,\n  'gemma3.vision.feed_forward_length': 4304,\n  'gemma3.vision.image_size': 896,\n  'gemma3.vision.num_channels': 3,\n  'gemma3.vision.patch_size': 14,\n  'general.architecture': 'gemma3',\n  'general.file_type': 15,\n  'general.parameter_count': 4299915632,\n  'general.quantization_version': 2,\n  'tokenizer.ggml.add_bos_token': True,\n  'tokenizer.ggml.add_eos_token': False,\n  'tokenizer.ggml.add_padding_token': False,\n  'tokenizer.ggml.add_unknown_token': False,\n  'tokenizer.ggml.bos_token_id': 2,\n  'tokenizer.ggml.eos_token_id': 1,\n  'tokenizer.ggml.merges': None,\n  'tokenizer.ggml.model': 'llama',\n  'tokenizer.ggml.padding_token_id': 0,\n  'tokenizer.ggml.pre': 'default',\n  'tokenizer.ggml.scores': None,\n  'tokenizer.ggml.token_type': None,\n  'tokenizer.ggml.tokens': None,\n  'tokenizer.ggml.unknown_token_id': 3})\n\n\n\nollama.pull??\n\n\nSignature: ollama.pull(model: str, *, insecure: bool = False, stream: bool = False) -&gt; Union[ollama._types.ProgressResponse, collections.abc.Iterator[ollama._types.ProgressResponse]]\nSource:   \n  def pull(\n    self,\n    model: str,\n    *,\n    insecure: bool = False,\n    stream: bool = False,\n  ) -&gt; Union[ProgressResponse, Iterator[ProgressResponse]]:\n    \"\"\"\n    Raises `ResponseError` if the request could not be fulfilled.\n    Returns `ProgressResponse` if `stream` is `False`, otherwise returns a `ProgressResponse` generator.\n    \"\"\"\n    return self._request(\n      ProgressResponse,\n      'POST',\n      '/api/pull',\n      json=PullRequest(\n        model=model,\n        insecure=insecure,\n        stream=stream,\n      ).model_dump(exclude_none=True),\n      stream=stream,\n    )\nFile:      /home/workspace/wordslab-notebooks-lib/.venv/lib/python3.12/site-packages/ollama/_client.py\nType:      method\n\n\n\n\nollama.delete??\n\n\nSignature: ollama.delete(model: str) -&gt; ollama._types.StatusResponse\nDocstring: &lt;no docstring&gt;\nSource:   \n  def delete(self, model: str) -&gt; StatusResponse:\n    r = self._request_raw(\n      'DELETE',\n      '/api/delete',\n      json=DeleteRequest(\n        model=model,\n      ).model_dump(exclude_none=True),\n    )\n    return StatusResponse(\n      status='success' if r.status_code == 200 else 'error',\n    )\nFile:      /home/workspace/wordslab-notebooks-lib/.venv/lib/python3.12/site-packages/ollama/_client.py\nType:      method\n\n\n\nStreaming responses\nCertain endpoints stream responses as JSON objects. Streaming can be disabled by providing {“stream”: false} for these endpoints.\nStructured outputs\nStructured outputs are supported by providing a JSON schema in the format parameter. The model will generate a response that matches the schema. See the structured outputs example below.\nJSON mode\nEnable JSON mode by setting the format parameter to json. This will structure the response as a valid JSON object.\nhttps://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-completion\nParameters - model: (required) the model name - prompt: the prompt to generate a response for - suffix: the text after the model response - images: (optional) a list of base64-encoded images (for multimodal models such as llava) - think: (for thinking models) should the model think before responding?\nAdvanced parameters (optional): - format: the format to return a response in. Format can be json or a JSON schema - options: additional model parameters listed in the documentation for the Modelfile such as temperature - system: system message to (overrides what is defined in the Modelfile) - template: the prompt template to use (overrides what is defined in the Modelfile) - stream: if false the response will be returned as a single response object, rather than a stream of objects - raw: if true no formatting will be applied to the prompt. You may choose to use the raw parameter if you are specifying a full templated prompt in your request to the API - keep_alive: controls how long the model will stay loaded into memory following the request (default: 5m)\nResponse\nA stream of JSON objects is returned:\n{ “model”: “llama3.2”, “created_at”: “2023-08-04T08:52:19.385406455-07:00”, “response”: “The”, “done”: false }\nThe final response in the stream also includes additional data about the generation: - total_duration: time spent generating the response - load_duration: time spent in nanoseconds loading the model - prompt_eval_count: number of tokens in the prompt - prompt_eval_duration: time spent in nanoseconds evaluating the prompt - eval_count: number of tokens in the response - eval_duration: time in nanoseconds spent generating the response - response: empty if the response was streamed, if not streamed, this will contain the full response\nA response can be received in one reply when streaming is off.\nTo calculate how fast the response is generated in tokens per second (token/s), divide eval_count / eval_duration * 10^9.\nImages\nTo submit images to multimodal models, provide a list of base64-encoded images:\n\n“images”: [“iVBORw0KGgoAAAANSUhEUgAAAG0AAABmCAYAAADBPx+VAAAACXBI…”]\n\n\nollama.generate(model='gemma3', prompt='Why is the sky blue?')\n\n\nollama.chat(model='gemma3', messages=[{'role': 'user', 'content': 'Why is the sky blue?'}])\n\n\nollama.embed(model='gemma3', input='The sky is blue because of rayleigh scattering')\n\n\nollama.embed(model='gemma3', input=['The sky is blue because of rayleigh scattering', 'Grass is green because of chlorophyll'])\n\n\nollama.ps()\n\nProcessResponse(models=[])\n\n\n\nollama.web_search??\n\n\nSignature: ollama.web_search(query: str, max_results: int = 3) -&gt; ollama._types.WebSearchResponse\nSource:   \n  def web_search(self, query: str, max_results: int = 3) -&gt; WebSearchResponse:\n    \"\"\"\n    Performs a web search\n    Args:\n      query: The query to search for\n      max_results: The maximum number of results to return (default: 3)\n    Returns:\n      WebSearchResponse with the search results\n    Raises:\n      ValueError: If OLLAMA_API_KEY environment variable is not set\n    \"\"\"\n    if not self._client.headers.get('authorization', '').startswith('Bearer '):\n      raise ValueError('Authorization header with Bearer token is required for web search')\n    return self._request(\n      WebSearchResponse,\n      'POST',\n      'https://ollama.com/api/web_search',\n      json=WebSearchRequest(\n        query=query,\n        max_results=max_results,\n      ).model_dump(exclude_none=True),\n    )\nFile:      /home/workspace/wordslab-notebooks-lib/.venv/lib/python3.12/site-packages/ollama/_client.py\nType:      method\n\n\n\n\nollama.web_fetch??\n\n\nSignature: ollama.web_fetch(url: str) -&gt; ollama._types.WebFetchResponse\nSource:   \n  def web_fetch(self, url: str) -&gt; WebFetchResponse:\n    \"\"\"\n    Fetches the content of a web page for the provided URL.\n    Args:\n      url: The URL to fetch\n    Returns:\n      WebFetchResponse with the fetched result\n    \"\"\"\n    if not self._client.headers.get('authorization', '').startswith('Bearer '):\n      raise ValueError('Authorization header with Bearer token is required for web fetch')\n    return self._request(\n      WebFetchResponse,\n      'POST',\n      'https://ollama.com/api/web_fetch',\n      json=WebFetchRequest(\n        url=url,\n      ).model_dump(exclude_none=True),\n    )\nFile:      /home/workspace/wordslab-notebooks-lib/.venv/lib/python3.12/site-packages/ollama/_client.py\nType:      method",
    "crumbs": [
      "wordslab-notebooks-lib.chat"
    ]
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "wordslab-notebooks-lib.core",
    "section": "",
    "text": "The WordslabNotebooks class is the entry point to access all wordslab-notebooks resources.\n\n\n\n\ndef Wordslab(\n    \n):\n\nInitialize self. See help(type(self)) for accurate signature.\n\nwordslab = Wordslab()\n\n\nwordslab.python_lib_version\n\n'0.0.11'\n\n\n\nwordslab.env.version\n\n'2025-12'",
    "crumbs": [
      "wordslab-notebooks-lib.core"
    ]
  },
  {
    "objectID": "core.html#wordslab-notebooks-python-sdk",
    "href": "core.html#wordslab-notebooks-python-sdk",
    "title": "wordslab-notebooks-lib.core",
    "section": "",
    "text": "The WordslabNotebooks class is the entry point to access all wordslab-notebooks resources.\n\n\n\n\ndef Wordslab(\n    \n):\n\nInitialize self. See help(type(self)) for accurate signature.\n\nwordslab = Wordslab()\n\n\nwordslab.python_lib_version\n\n'0.0.11'\n\n\n\nwordslab.env.version\n\n'2025-12'",
    "crumbs": [
      "wordslab-notebooks-lib.core"
    ]
  },
  {
    "objectID": "core.html#develop-a-python-library-with-nbdev",
    "href": "core.html#develop-a-python-library-with-nbdev",
    "title": "wordslab-notebooks-lib.core",
    "section": "Develop a Python library with nbdev",
    "text": "Develop a Python library with nbdev\n\nInstall the python client library in development mode\ncd $WORDSLAB_WORKSPACE/wordslab-notebooks-lib\nsource .venv/bin/activate\n\n# Install nbdev and twine\n# Install the wordslab-notebooks-lib python library in editable mode\nuv sync --dev\n\n# Install the current package in editable mode\nuv pip install -e .\n\n\nGenerate the python library from the source notebooks\ncd $WORDSLAB_WORKSPACE/wordslab-notebooks-lib\nsource .venv/bin/activate\n\n# Export notebooks to Python modules\nnbdev_export\n\n# Clean the notebooks before commit in git\nnbdev_clean\n\n\nTest the python client library\nAfter installing the client library in development mode once, you can iterate fast: - create a notebook using the kernel “wordslab-notebooks-lib” - restart the kernel if needed - import wordslab_notebooks_lib - use the functions defined in the library\n\n\nPublish the extension to pypi when ready\nCreate a file called ~/.pypirc with your token details. It should have these contents:\n[pypi]\nusername = __token__\npassword = your_pypi_token\nThen execute the following commands:\ncd $WORDSLAB_WORKSPACE/wordslab-notebooks-lib\nsource .venv/bin/activate\n\n# Bump the version number\nnbdev_bump_version\n\n# Publish to PyPI\nnbdev_pypi",
    "crumbs": [
      "wordslab-notebooks-lib.core"
    ]
  },
  {
    "objectID": "tools_study_anthropic.html",
    "href": "tools_study_anthropic.html",
    "title": "Anthropic tools study",
    "section": "",
    "text": "from bs4 import BeautifulSoup\nfrom cloudscraper import create_scraper\nfrom html2text import HTML2Text\nfrom pathlib import Path\nfrom urllib.parse import urljoin, urlparse\nimport re\nfrom textwrap import dedent",
    "crumbs": [
      "Anthropic tools study"
    ]
  },
  {
    "objectID": "tools_study_anthropic.html#text-editor-tool",
    "href": "tools_study_anthropic.html#text-editor-tool",
    "title": "Anthropic tools study",
    "section": "Text editor tool",
    "text": "Text editor tool\nThe model can use an the text editor tool to view and modify text files, helping you debug, fix, and improve your code or other text documents. This allows the model to directly interact with your files, providing hands-on assistance rather than just suggesting changes.\nWhen to use the text editor tool\nSome examples of when to use the text editor tool are: - Code debugging: Have the model identify and fix bugs in your code, from syntax errors to logic issues. - Code refactoring: Let the model improve your code structure, readability, and performance through targeted edits. - Documentation generation: Ask the model to add docstrings, comments, or README files to your codebase. - Test creation: Have the model create unit tests for your code based on its understanding of the implementation.\nUse the text editor tool\nProvide the text editor tool (named str_replace_based_edit_tool) to the model.\nYou can optionally specify a max_characters parameter to control truncation when viewing large files.\nThe text editor tool can be used in the following way:\n1 Provide the model with the text editor tool and a user prompt\n\nInclude the text editor tool in your API call\nProvide a user prompt that may require examining or modifying files, such as “Can you fix the syntax error in my code?”\n\n2 The model uses the tool to examine files or directories\n\nThe model assesses what it needs to look at and uses the view command to examine file contents or list directory contents\nThe model response will contain a tool use request with the view command\n\n3 Execute the view command and return results\n\nExtract the file or directory path from the model’s tool use request\nRead the file’s contents or list the directory contents\nIf a max_characters parameter was specified in the tool configuration, truncate the file contents to that length\nReturn the results to the model by continuing the conversation with a new user message containing a tool result\n\n4 The model uses the tool to modify files\n\nAfter examining the file or directory, the model may use a command such as str_replace to make changes or insert to add text at a specific line number.\nIf the model uses the str_replace command, it constructs a properly formatted tool use request with the old text and new text to replace it with\n\n5 Execute the edit and return results\n\nExtract the file path, old text, and new text from the model’s tool use request\nPerform the text replacement in the file\nReturn the results to the model\n\n6 The model provides its analysis and explanation\n\nAfter examining and possibly editing the files, the model provides a complete explanation of what it found and what changes it made\n\nText editor tool commands\nThe text editor tool supports several commands for viewing and modifying files:\nview\nThe view command allows the model to examine the contents of a file or list the contents of a directory. It can read the entire file or a specific range of lines.\nParameters: - command: Must be “view” - path: The path to the file or directory to view - view_range (optional): An array of two integers specifying the start and end line numbers to view. Line numbers are 1-indexed, and -1 for the end line means read to the end of the file. This parameter only applies when viewing files, not directories.\nExample view commands\n// Example for viewing a file\n{\n  \"input\": {\n    \"command\": \"view\",\n    \"path\": \"primes.py\"\n  }\n}\n\n// Example for viewing a directory\n{\n  \"input\": {\n    \"command\": \"view\",\n    \"path\": \"src/\"\n  }\n}\nstr_replace\nThe str_replace command allows the model to replace a specific string in a file with a new string. This is used for making precise edits.\nParameters: - command: Must be “str_replace” - path: The path to the file to modify - old_str: The text to replace (must match exactly, including whitespace and indentation) - new_str: The new text to insert in place of the old text\nExample str_replace command\n{\n  \"input\": {\n    \"command\": \"str_replace\",\n    \"path\": \"primes.py\",\n    \"old_str\": \"for num in range(2, limit + 1)\",\n    \"new_str\": \"for num in range(2, limit + 1):\"\n  }\n}\ncreate\nThe create command allows the model to create a new file with specified content.\nParameters: - command: Must be “create” - path: The path where the new file should be created - file_text: The content to write to the new file\nExample create command\n{\n  \"input\": {\n    \"command\": \"create\",\n    \"path\": \"test_primes.py\",\n    \"file_text\": \"import unittest\\nimport primes\\n\\nclass TestPrimes(unittest.TestCase):\\n    def test_is_prime(self):\\n        self.assertTrue(primes.is_prime(2))\\n        self.assertTrue(primes.is_prime(3))\\n        self.assertFalse(primes.is_prime(4))\\n\\nif __name__ == '__main__':\\n    unittest.main()\"\n  }\n}\ninsert\nThe insert command allows the model to insert text at a specific location in a file.\nParameters: - command: Must be “insert” - path: The path to the file to modify - insert_line: The line number after which to insert the text (0 for beginning of file) - new_str: The text to insert\nExample insert command\n{\n  \"input\": {\n    \"command\": \"insert\",\n    \"path\": \"primes.py\",\n    \"insert_line\": 0,\n    \"new_str\": \"\\\"\\\"\\\"Module for working with prime numbers.\\n\\nThis module provides functions to check if a number is prime\\nand to generate a list of prime numbers up to a given limit.\\n\\\"\\\"\\\"\\n\"\n  }\n}\nImplement the text editor tool\n1 Initialize your editor implementation\nCreate helper functions to handle file operations like reading, writing, and modifying files. Consider implementing backup functionality to recover from mistakes.\n2 Handle editor tool calls\nCreate a function that processes tool calls from the model based on the command type:\n3 Implement security measures\nAdd validation and security checks: - Validate file paths to prevent directory traversal - Create backups before making changes - Handle errors gracefully - Implement permissions checks\nWhen implementing the text editor tool, keep in mind: - Security: The tool has access to your local filesystem, so implement proper security measures. - Backup: Always create backups before allowing edits to important files. - Validation: Validate all inputs to prevent unintended changes. - Unique matching: Make sure replacements match exactly one location to avoid unintended edits.\nHandle errors\nFile not found\nIf the model tries to view or modify a file that doesn’t exist, return an appropriate error message in the tool_result: “Error: File not found”\nMultiple matches for replacement\nIf the str_replace command matches multiple locations in the file, return an appropriate error message: “Error: Found 3 matches for replacement text. Please provide more context to make a unique match.”\nNo matches for replacement\nIf the str_replace command doesn’t match any text in the file, return an appropriate error message: “Error: No match found for replacement. Please check your text and try again.”\nPermission errors\nIf there are permission issues with creating, reading, or modifying files, return an appropriate error message: “Error: Permission denied. Cannot write to file.”\nImplementation best practices\nProvide clear context\nWhen asking the model to fix or modify code, be specific about what files need to be examined or what issues need to be addressed. Clear context helps the model identify the right files and make appropriate changes.\n\nLess helpful prompt: “Can you fix my code?”\nBetter prompt: “There’s a syntax error in my primes.py file that prevents it from running. Can you fix it?”\n\nBe explicit about file paths\nSpecify file paths clearly when needed, especially if you’re working with multiple files or files in different directories. - Less helpful prompt: “Review my helper file” - Better prompt: “Can you check my utils/helpers.py file for any performance issues?”\nCreate backups before editing\nImplement a backup system in your application that creates copies of files before allowing the model to edit them, especially for important or production code.\nHandle unique text replacement carefully\nThe str_replace command requires an exact match for the text to be replaced. Your application should ensure that there is exactly one match for the old text or provide appropriate error messages.\nif count == 0:\n    return \"Error: No match found\"\nelif count &gt; 1:\n    return f\"Error: Found {count} matches\"\nelse:\n    ...\n    return \"Successfully replaced text\"\nVerify changes\nAfter the model makes changes to a file, verify the changes by running tests or checking that the code still works as expected.\n\nAnswerai Text editor tools implementation\nImplementation from https://github.com/AnswerDotAI/claudette\nImplements functions for Anthropic’s Text Editor Tool API, allowing a model to view and edit files.\n\ndef view(path:str,  # The path to the file or directory to view\n         view_range:tuple[int,int]=None, # Optional array of two integers specifying the start and end line numbers to view. Line numbers are 1-indexed, and -1 for the end line means read to the end of the file. This parameter only applies when viewing files, not directories.\n         nums:bool=False # Optionally prefix all lines of the file with a line number\n        ) -&gt; str:\n    'Examine the contents of a file or list the contents of a directory. It can read the entire file or a specific range of lines. With or without line numbers.'\n    try:\n        p = Path(path).expanduser().resolve()\n        if not p.exists(): return f'Error: File not found: {p}'\n        if p.is_dir():\n            res = [str(f) for f in p.glob('**/*') \n                   if not any(part.startswith('.') for part in f.relative_to(p).parts)]\n            return f'Directory contents of {p}:\\n' + '\\n'.join(res)\n        \n        lines = p.read_text().splitlines()\n        s,e = 1,len(lines)\n        if view_range:\n            s,e = view_range\n            if not (1 &lt;= s &lt;= len(lines)): return f'Error: Invalid start line {s}'\n            if e != -1 and not (s &lt;= e &lt;= len(lines)): return f'Error: Invalid end line {e}'\n            lines = lines[s-1:None if e==-1 else e]\n            \n        return '\\n'.join([f'{i+s-1:6d} │ {l}' for i,l in enumerate(lines,1)] if nums else lines)\n    except Exception as e: return f'Error viewing file: {str(e)}'\n\n\nprint(view('styles.css', (1,10), nums=True))\n\n     1 │ .cell {\n     2 │   margin-bottom: 1rem;\n     3 │ }\n     4 │ \n     5 │ .cell &gt; .sourceCode {\n     6 │   margin-bottom: 0;\n     7 │ }\n     8 │ \n     9 │ .cell-output &gt; pre {\n    10 │   margin-bottom: 0;\n\n\n\ndef create(path: str, # The path where the new file should be created\n           file_text: str, # The text content to write to the new file\n           overwrite:bool=False # Allows overwriting an existing file\n          ) -&gt; str:\n    'Creates a new file with the given text content at the specified path'\n    try:\n        p = Path(path)\n        if p.exists():\n            if not overwrite: return f'Error: File already exists: {p}'\n        p.parent.mkdir(parents=True, exist_ok=True)\n        p.write_text(file_text)\n        return f'Created file {p} containing:\\n{file_text}'\n    except Exception as e: return f'Error creating file: {str(e)}'\n\n\nprint(create('test.txt', 'Hello, world!'))\nprint(view('test.txt', nums=True))\n\nCreated file test.txt containing:\nHello, world!\n     1 │ Hello, world!\n\n\n\ndef insert(path: str,  # The path to the file to modify\n           insert_line: int, # The line number after which to insert the text (0 for beginning of file)\n           new_str: str # The text to insert\n          ) -&gt; str: \n    'Insert text at a specific line number in a file.'\n    try:\n        p = Path(path)\n        if not p.exists(): return f'Error: File not found: {p}'\n            \n        content = p.read_text().splitlines()\n        if not (0 &lt;= insert_line &lt;= len(content)): return f'Error: Invalid line number {insert_line}'\n            \n        content.insert(insert_line, new_str)\n        new_content = '\\n'.join(content)\n        p.write_text(new_content)\n        return f'Inserted text at line {insert_line} in {p}.\\nNew contents:\\n{new_content}'\n    except Exception as e: return f'Error inserting text: {str(e)}'\n\n\ninsert('test.txt', 0, 'Let\\'s add a new line')\nprint(view('test.txt', nums=True))\n\n     1 │ Let's add a new line\n     2 │ Hello, world!\n\n\n\ndef str_replace(path: str, # The path to the file to modify\n                old_str: str, # The text to replace (must match exactly, including whitespace and indentation)\n                new_str: str # The new text to insert in place of the old text\n               ) -&gt; str:\n    'Replace a specific string in a file with a new string. This is used for making precise edits.'\n    try:\n        p = Path(path)\n        if not p.exists(): return f'Error: File not found: {p}'\n            \n        content = p.read_text()\n        count = content.count(old_str)\n        \n        if count == 0: return 'Error: Text not found in file'\n        if count &gt; 1: return f'Error: Multiple matches found ({count})'\n            \n        new_content = content.replace(old_str, new_str, 1)\n        p.write_text(new_content)\n        return f'Replaced text in {p}.\\nNew contents:\\n{new_content}'\n    except Exception as e: return f'Error replacing text: {str(e)}'\n\n\nstr_replace('test.txt', 'new line', '')\nprint(view('test.txt', nums=True))\n\n     1 │ Let's add a \n     2 │ Hello, world!",
    "crumbs": [
      "Anthropic tools study"
    ]
  },
  {
    "objectID": "tools_study_anthropic.html#bash-tool",
    "href": "tools_study_anthropic.html#bash-tool",
    "title": "Anthropic tools study",
    "section": "Bash tool",
    "text": "Bash tool\nThe bash tool enables the model to execute shell commands in a persistent bash session, allowing system operations, script execution, and command-line automation.\nOverview\nThe bash tool provides the model with: - Persistent bash session that maintains state - Ability to run any shell command - Access to environment variables and working directory - Command chaining and scripting capabilities\nUse cases\n\nDevelopment workflows: Run build commands, tests, and development tools\nSystem automation: Execute scripts, manage files, automate tasks\nData processing: Process files, run analysis scripts, manage datasets\nEnvironment setup: Install packages, configure environments\n\nHow it works\nThe bash tool maintains a persistent session: 1. the model determines what command to run 2. you execute the command in a bash shell 3. reurn the output (stdout and stderr) to the model 4. session state persists between commands (environment variables, working directory)\nParameters\n\n\n\nParameter\nRequired\nDescription\n\n\n\n\ncommand\nYes*\nThe bash command to run\n\n\nrestart\nNo\nSet to true to restart the bash session\n\n\n\n*Required unless using restart\nExample: Multi-step automation\nThe model can chain commands to complete complex tasks:\n# User request\n\"Install the requests library and create a simple Python script that fetches a joke from an API, then run it.\"\n\n# Model tool calls:\n# 1. Install package\n{\"command\": \"pip install requests\"}\n\n# 2. Create script\n{\"command\": \"cat &gt; fetch_joke.py &lt;&lt; 'EOF'\\nimport requests\\nresponse = requests.get('https://official-joke-api.appspot.com/random_joke')\\njoke = response.json()\\nprint(f\\\"Setup: {joke['setup']}\\\")\\nprint(f\\\"Punchline: {joke['punchline']}\\\")\\nEOF\"}\n\n# 3. Run script\n{\"command\": \"python fetch_joke.py\"}\nThe session maintains state between commands, so files created in step 2 are available in step 3.\nHandle errors\nCommand execution timeout - If a command takes too long to execute: “Error: Command timed out after 30 seconds”\nCommand not found - If a command doesn’t exist: “bash: nonexistentcommand: command not found”\nPermission denied - If there are permission issues: “bash: /root/sensitive-file: Permission denied”\nImplementation best practices\nUse command timeouts: Implement timeouts to prevent hanging commands.\nMaintain session state: Keep the bash session persistent to maintain environment variables and working directory.\nHandle large outputs: Truncate very large outputs to prevent token limit issues.\nLog all commands: Keep an audit trail of executed commands.\nSanitize outputs: Remove sensitive information from command outputs.\nSecurity\nThe bash tool provides direct system access. Implement these essential safety measures: - Running in isolated environments (Docker/VM) - Implementing command filtering and allowlists - Setting resource limits (CPU, memory, disk) - Logging all executed commands\nKey recommendations - Use ulimit to set resource constraints - Filter dangerous commands (sudo, rm -rf, etc.) - Run with minimal user permissions - Monitor and log all command execution\nCommon patterns\nDevelopment workflows - Running tests: pytest && coverage report - Building projects: npm install && npm run build - Git operations: git status && git add . && git commit -m “message”\nFile operations - Processing data: wc -l .csv && ls -lh .csv - Searching files: find . -name “*.py” | xargs grep “pattern” - Creating backups: tar -czf backup.tar.gz ./data\nSystem tasks - Checking resources: df -h && free -m - Process management: ps aux | grep python - Environment setup: export PATH=PATH:/new/path && echo PATH\nLimitations\n\nNo interactive commands: Cannot handle vim, less, or password prompts\nNo GUI applications: Command-line only\nSession scope: Persists within conversation, lost between API calls\nOutput limits: Large outputs may be truncated\nNo streaming: Results returned after completion\n\nCombining with other tools\nThe bash tool is most powerful when combined with the text editor and other tools.",
    "crumbs": [
      "Anthropic tools study"
    ]
  },
  {
    "objectID": "tools_study_anthropic.html#code-execution-tool",
    "href": "tools_study_anthropic.html#code-execution-tool",
    "title": "Anthropic tools study",
    "section": "Code execution tool",
    "text": "Code execution tool\nCode execution tool = bash tool + text editor tool in a remote container.\nThe code execution tool defined by Anthropic is implemented as a remote execution container, and most of its features are designed around the interaction between a local and remote cloud environment.\nIn our wordslab-notebooks context, we want to execute everything locally, so we will only keep a few interesting parts of the Anthropic documentation.\nWhen this tool is provided, the model automatically gains access to two sub-tools: - bash_code_execution: Run shell commands - text_editor_code_execution: View, create, and edit files, including writing code\nHow code execution works\nWhen you add the code execution tool to your API request: 1. The model evaluates whether code execution would help answer your question 2. The tool automatically provides the model with the following capabilities: - Bash commands: Execute shell commands for system operations and package management - File operations: Create, view, and edit files directly, including writing code 3. The model can use any combination of these capabilities in a single request 4. All operations run in a secure sandbox environment 5. The tool provides results with any generated charts, calculations, or analysis\nContainers\nThe code execution tool runs in a secure, containerized environment designed specifically for code execution, with a higher focus on Python.\nRuntime environment - Python version: 3.11.12 - Operating system: Linux-based container - Architecture: x86_64 (AMD64)\nResource limits - Memory: 5GiB RAM - Disk space: 5GiB workspace storage - CPU: 1 CPU\nNetworking and security - Internet access: Completely disabled for security - External connections: No outbound network requests permitted - Sandbox isolation: Full isolation from host system and other containers - File access: Limited to workspace directory only - Workspace scoping: Like Files, containers are scoped to the workspace of the API key - Expiration: Containers expire 30 days after creation\nPre-installed libraries - Data Science: pandas, numpy, scipy, scikit-learn, statsmodels - Visualization: matplotlib, seaborn - File Processing: pyarrow, openpyxl, xlsxwriter, xlrd, pillow, python-pptx, python-docx, pypdf, pdfplumber, pypdfium2, pdf2image, pdfkit, tabula-py, reportlab[pycairo], Img2pdf - Math & Computing: sympy, mpmath - Utilities: tqdm, python-dateutil, pytz, joblib, unzip, unrar, 7zip, bc, rg (ripgrep), fd, sqlite\nContainer reuse - You can reuse an existing container across multiple API requests by providing the container ID from a previous response. - This allows you to maintain created files between requests.\nHow to use the tool\nExecute Bash commands\nAsk the model to check system information and install packages: “Check the Python version and list installed packages”\nCreate and edit files directly\nThe model can create, view, and edit files directly in the sandbox using the file manipulation capabilities: “Create a config.yaml file with database settings, then update the port from 5432 to 3306”\nUpload and analyze your own files\nTo analyze your own data files (CSV, Excel, images, etc.), upload them via the Files API and reference them in your request: “Analyze this CSV data”\n\"content\": [\n                {\"type\": \"text\", \"text\": \"Analyze this CSV data\"},\n                {\"type\": \"container_upload\", \"file_id\": \"file_abc123\"}\n            ]\nRetrieve generated files\nWhen the tool creates files during code execution, you can retrieve these files using the Files API: “Create a matplotlib visualization and save it as output.png” - Extract file IDs from the response - Download the created files\nCombine operations\nA complex workflow using all capabilities: - First, upload a file - Extract file_id - Then use it with code execution - “Analyze this CSV data: create a summary report, save visualizations, and create a README with the findings”\nResponse format\nThe code execution tool can return two types of results depending on the operation:\nBash command response\n    \"stdout\": \"total 24\\ndrwxr-xr-x 2 user user 4096 Jan 1 12:00 .\\ndrwxr-xr-x 3 user user 4096 Jan 1 11:00 ..\\n-rw-r--r-- 1 user user  220 Jan 1 12:00 data.csv\\n-rw-r--r-- 1 user user  180 Jan 1 12:00 config.json\",\n    \"stderr\": \"\",\n    \"return_code\": 0\nFile operation responses\n\nView file\n\n    \"file_type\": \"text\",\n    \"content\": \"{\\n  \\\"setting\\\": \\\"value\\\",\\n  \\\"debug\\\": true\\n}\",\n    \"numLines\": 4,\n    \"startLine\": 1,\n    \"totalLines\": 4\n\nCreate file\n\nis_file_update: whether file already existed\n\n  \"is_file_update\": false\nEdit file (str_replace)\n\nlines: diff format\n\n  \"oldStart\": 3,\n  \"oldLines\": 1,\n  \"newStart\": 3,\n  \"newLines\": 1,\n  \"lines\": [\"-  \\\"debug\\\": true\", \"+  \\\"debug\\\": false\"]\n\nErrors\nError codes by tool type:\n\n\n\n\n\n\n\n\nTool\nError Code\nDescription\n\n\n\n\nAll tools\nunavailable\nThe tool is temporarily unavailable\n\n\nAll tools\nexecution_time_exceeded\nExecution exceeded maximum time limit\n\n\nAll tools\ncontainer_expired\nContainer expired and is no longer available\n\n\nAll tools\ninvalid_tool_input\nInvalid parameters provided to the tool\n\n\nAll tools\ntoo_many_requests\nRate limit exceeded for tool usage\n\n\ntext_editor\nfile_not_found\nFile doesn’t exist (for view/edit operations)\n\n\ntext_editor\nstring_not_found\nThe old_str not found in file (for str_replace)\n\n\n\nProgrammatic tool calling\nThe code execution tool powers programmatic tool calling, which allows the model to write code that calls your custom tools programmatically within the execution container. This enables efficient multi-tool workflows, data filtering before reaching the model’s context, and complex conditional logic.\nEnable programmatic calling for your tools:\n    tools=[\n        {\n            \"name\": \"get_weather\",\n            \"description\": \"Get weather for a city\",\n            \"input_schema\": {...},\n            \"allowed_callers\": [\"code_execution_20250825\"]  # Enable programmatic calling\n        }\n    ]\nLearn more in the Programmatic tool calling documentation.\nUsing code execution with Agent Skills\nThe code execution tool enables the mdeol to use Agent Skills. Skills are modular capabilities consisting of instructions, scripts, and resources that extend the model’s functionality.\nLearn more in the Agent Skills documentation and Agent Skills API guide.",
    "crumbs": [
      "Anthropic tools study"
    ]
  },
  {
    "objectID": "tools_study_anthropic.html#code-interpreter",
    "href": "tools_study_anthropic.html#code-interpreter",
    "title": "Anthropic tools study",
    "section": "Code interpreter",
    "text": "Code interpreter\nFrom https://github.com/AnswerDotAI/claudette\nCode interpreter Here is an example of using toolloop to implement a simple code interpreter with additional tools.\nfrom toolslm.shell import get_shell from fastcore.meta import delegates import traceback\n@delegates() class CodeChat(Chat): imps = ‘os, warnings, time, json, re, math, collections, itertools, functools, dateutil, datetime, string, types, copy, pprint, enum, numbers, decimal, fractions, random, operator, typing, dataclasses’ def init(self, model: Optional[str] = None, ask:bool=True, **kwargs): super().__init__(model=model, **kwargs) self.ask = askm self.tools.append(self.run_cell) self.shell = get_shell() self.shell.run_cell(‘import’+self.imps)\nWe have one additional parameter to creating a CodeChat beyond what we pass to Chat, which is ask – if that’s True, we’ll prompt the user before running code.\n@patch def run_cell( self:CodeChat, code:str, # Code to execute in persistent IPython session )-&gt;str: “““Asks user for permission, and if provided, executes python code using persistent IPython session. Returns: Result of expression on last line (if exists); ‘#DECLINED#’ if user declines request to execute”“” confirm = f’Press Enter to execute, or enter “n” to skip?\\n{code}\\n’ if self.ask and input(confirm): return ‘#DECLINED#’ try: res = self.shell.run_cell(code) except Exception as e: return traceback.format_exc() return res.stdout if res.result is None else res.result",
    "crumbs": [
      "Anthropic tools study"
    ]
  },
  {
    "objectID": "tools_study_anthropic.html#web-fetch-tool",
    "href": "tools_study_anthropic.html#web-fetch-tool",
    "title": "Anthropic tools study",
    "section": "Web fetch tool",
    "text": "Web fetch tool\nThe web fetch tool allows the model to retrieve full content from specified web pages and PDF documents.\nSecurity Warning\nEnabling the web fetch tool in environments where the model processes untrusted input alongside sensitive data poses data exfiltration risks. We recommend only using this tool in trusted environments or when handling non-sensitive data.\nTo minimize exfiltration risks, the model should not be allowed to dynamically construct URLs. The system should only fetch URLs that have been explicitly provided by the user or that come from previous web search or web fetch results. However, there is still residual risk that should be carefully considered when using this tool.\nIf data exfiltration is a concern, consider: - Disabling the web fetch tool entirely - Using the max_uses parameter to limit the number of requests - Using the allowed_domains parameter to restrict to known safe domains\nHow web fetch works\nWhen you add the web fetch tool to your API request: - The model decides when to fetch content based on the prompt and available URLs. - The tools retrieves the full text content from the specified URL. - For PDFs, automatic text extraction is performed. - The model analyzes the fetched content and provides a response with optional citations. - The web fetch tool currently does not support web sites dynamically rendered via Javascript.\nParameters\nThe web fetch tool supports the following parameters: - max_uses = 10: Optional: Limit the number of fetches per request - allowed_domains = [“example.com”, “docs.example.com”]: Optional: Only fetch from these domains - blocked_domains = [“private.example.com”]: Optional: Never fetch from these domains - citations = {“enabled”: true}: Optional: Enable citations for fetched content - max_content_tokens=100000: Optional: Maximum content length in tokens\n\nAnswerai web fetch implementation\nThis tool implementation is inspired by the library ipykernel_helper from Answer.ai. As of december 2025, this library is not open source, but it is available to users in the solve.it.com environment and is a dependency of other Apache 2.0 libraries, so I think it is OK to use it as an inspiration.\n\n\n\nread_url\n\ndef read_url(\n    url:str, as_md:bool=True, extract_section:bool=True, selector:str=None, math_mode:str=None\n):\n\nThis functions extracts a web page information for LLM ingestion 1. Downloads a web page 2. Parses HTML 3. Optionally extracts a specific section (fragment or CSS selector) 4. Converts MathML → LaTeX 5. Optionally converts HTML → Markdown 6. Convert code sections to fenced markdown blocks 7. Makes image URLs absolute 8. Returns the processed text\n\n\n\nscrape_url\n\ndef scrape_url(\n    url\n):\n\nGet the html content of a web page using the cloudscraper library to bypass Cloudflare’s anti-bot page.\n\nurl2md = read_url(\"https://answerdotai.github.io/toolslm/\")\nurl2md\n\n'[ toolslm ](./index.html)\\n\\n__\\n\\n  1. [toolslm](./index.html)\\n\\n\\n\\n  * [ toolslm](./index.html)\\n\\n  * [ xml source](./xml.html)\\n\\n  * [ funccall source](./funccall.html)\\n\\n  * [ shell source](./shell.html)\\n\\n  * [ Download helpers](./download.html)\\n\\n  * [ Markdown Hierarchy Parser](./md_hier.html)\\n\\n\\n\\n\\n## On this page\\n\\n  * Install\\n  * How to use\\n    * Context creation\\n\\n\\n\\n  * [__Report an issue](https://github.com/AnswerDotAI/toolslm/issues/new)\\n\\n\\n\\n## Other Formats\\n\\n  * [ __CommonMark](index.html.md)\\n\\n\\n\\n# toolslm\\n\\nTools to make language models a bit easier to use \\n\\nThis is a work in progress…\\n\\n## Install\\n    \\n    \\n    pip install toolslm\\n\\n __\\n\\n## How to use\\n\\n### Context creation\\n\\ntoolslm has some helpers to make it easier to generate XML context from files, for instance [`folder2ctx`](https://AnswerDotAI.github.io/toolslm/xml.html#folder2ctx):\\n    \\n    \\n    print(folder2ctx(\\'samples\\', prefix=False, file_glob=\\'*.py\\'))\\n\\n__\\n    \\n    \\n    &lt;documents&gt;&lt;document index=\"1\"&gt;&lt;src&gt;\\n    samples/sample_core.py\\n    &lt;/src&gt;&lt;document-content&gt;\\n    import inspect\\n    empty = inspect.Parameter.empty\\n    models = \\'claude-3-opus-20240229\\',\\'claude-3-sonnet-20240229\\',\\'claude-3-haiku-20240307\\'\\n    &lt;/document-content&gt;&lt;/document&gt;&lt;/documents&gt;\\n\\nJSON doesn’t map as nicely to XML as the `ft` data structure from `fastcore.xml`, but for simple XML trees it can be convenient. The [`json_to_xml`](https://AnswerDotAI.github.io/toolslm/xml.html#json_to_xml) function handles that conversion:\\n    \\n    \\n    a = dict(surname=\\'Howard\\', firstnames=[\\'Jeremy\\',\\'Peter\\'],\\n             address=dict(state=\\'Queensland\\',country=\\'Australia\\'))\\n    print(json_to_xml(a, \\'person\\'))\\n\\n__\\n    \\n    \\n    &lt;person&gt;\\n      &lt;surname&gt;Howard&lt;/surname&gt;\\n      &lt;firstnames&gt;\\n        &lt;item&gt;Jeremy&lt;/item&gt;\\n        &lt;item&gt;Peter&lt;/item&gt;\\n      &lt;/firstnames&gt;\\n      &lt;address&gt;\\n        &lt;state&gt;Queensland&lt;/state&gt;\\n        &lt;country&gt;Australia&lt;/country&gt;\\n      &lt;/address&gt;\\n    &lt;/person&gt;\\n\\n  * [__Report an issue](https://github.com/AnswerDotAI/toolslm/issues/new)\\n\\n\\n'",
    "crumbs": [
      "Anthropic tools study"
    ]
  }
]